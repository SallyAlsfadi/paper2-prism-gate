rank_priority_only,issue_key,project,created_ts,updated_ts,issue_type,status,priority,priority_ordinal,text_len,text
1,AIRFLOW-56,AIRFLOW,1462536460000,1462871357000,Bug,Closed,Blocker,1,531,"Airflow's scheduler can ""lose"" queued tasks Tasks that get into queued status can get ""lost"" on the scheduler/executor. They remain in the DB with state ""queued"", but the scheduler won't pick them up as it will not look at the DB currently. The executor does not see those tasks, although the scheduler has NOT been restarted (ie. to see this issue it is not required to have ""-n x"" as a parameter to the scheduler). This happens *also* to tasks scheduled by the scheduler and not just with backfills created by the SubDagOperator."
2,AIRFLOW-106,AIRFLOW,1462985047000,1463126376000,Bug,Closed,Blocker,1,2999,"Task Retries, on_failure callback, and email_on_failure Not Honored if First Task in a DAG Fails Hello. I created the following workflow : {code} from airflow import DAG from airflow.operators import PythonOperator from datetime import datetime, timedelta from airflow.models import Variable from time import sleep default_args = { 'depends_on_past': False, 'start_date': datetime(2016, 5, 11, 15, 20), 'email': <my email> 'email_on_failure': True, 'email_on_retry': False, 'retries': 1, 'retry_delay': timedelta(minutes=2), 'end_date': datetime(2016, 5, 11, 16, 00), } PARENT_DAG_NAME = 'test' dag = DAG(PARENT_DAG_NAME, default_args=default_args, schedule_interval=timedelta(minutes=10)) def sleep1_function(**kwargs): sleep(90) return Variable.get('test_var') sleep1 = PythonOperator( task_id='sleep1', python_callable=sleep1_function, dag=dag) {code} I forgot to declare test_var so when this DAG launched it failed quickly. However no failure email was ever sent. Clearing the failed task to make it rerun doesn't trigger any email. Here is the logs : {code} [2016-05-11 15:53:31,784] {models.py:157} INFO - Filling up the DagBag from /var/lib/airflow/airflow/dags/test.py [2016-05-11 15:53:32,272] {models.py:157} INFO - Filling up the DagBag from /var/lib/airflow/airflow/dags/test.py [2016-05-11 15:53:32,313] {models.py:1216} INFO - -------------------------------------------------------------------------------- Starting attempt 1 of 2 -------------------------------------------------------------------------------- [2016-05-11 15:53:32,333] {models.py:1239} INFO - Executing <Task(PythonOperator): sleep1> on 2016-05-11 15:20:00 [2016-05-11 15:55:03,450] {models.py:1306} ERROR - Variable test_var does not exist Traceback (most recent call last): File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.0-py2.7.egg/airflow/models.py"", line 1265, in run result = task_copy.execute(context=context) File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.0-py2.7.egg/airflow/operators/python_operator.py"", line 66, in execute return_value = self.python_callable(*self.op_args, **self.op_kwargs) File ""/var/lib/airflow/airflow/dags/test.py"", line 31, in sleep1_function return Variable.get('test_var') File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.0-py2.7.egg/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.0-py2.7.egg/airflow/models.py"", line 3145, in get raise ValueError('Variable {} does not exist'.format(key)) ValueError: Variable test_var does not exist [2016-05-11 15:55:03,581] {models.py:1318} INFO - Marking task as UP_FOR_RETRY [2016-05-11 15:55:03,759] {models.py:1347} ERROR - Variable test_var does not exist {code} In the DAG Runs page, the workflow is set as failed. In hte taks instance page, it is set as up_for_retry but no new run is ever scheduled. I tried incrementing the retires parameter, but nothing different happens, Airflow never retries after the first run. dud"
3,AIRFLOW-434,AIRFLOW,1471385417000,1475022298000,Bug,Resolved,Blocker,1,2555,"max_dag_run_reached blocks dag state change and new task scheduling Using the following DAG: ``` from airflow import DAG from airflow.operators.bash_operator import BashOperator from datetime import datetime, timedelta default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2016, 1, 1, 1, 0), 'email': ['xuanji@gmail.com'], 'email_on_failure': True, 'email_on_retry': False, 'retries': 3, 'retry_delay': timedelta(minutes=1), } dag = DAG('bash_bash_bash', default_args=default_args, schedule_interval=timedelta(seconds=10)) # t1, t2 and t3 are examples of tasks created by instatiating operators t1 = BashOperator( task_id='print_date', bash_command='date', dag=dag) t2 = BashOperator( task_id='sleep', bash_command='sleep 120', retries=3, dag=dag) templated_command = """""" {% for i in range(5) %} echo ""{{ ds }}"" echo ""{{ macros.ds_add(ds, 7)}}"" echo ""{{ params.my_param }}"" {% endfor %} """""" t3 = BashOperator( task_id='templated', bash_command=templated_command, params={'my_param': 'Parameter I passed in'}, dag=dag) t2.set_upstream(t1) t3.set_upstream(t1) ``` and an `airflow.cfg` that contains this: ``` min_file_process_interval = 1 ``` The state eventually becomes this: http://imgur.com/a/5bRTe The scheduler should be marking the 14 leftmost dagruns as success, but does not. the scheduler should also be scheduling tasks for the last two dagruns. A look at the logs explains the probable cause: ``` [2016-08-16 15:12:10,257] {jobs.py:1446} DagFileProcessor174 INFO - Processing file /Users/xuanji_li/airflow/dags/bash_bash_bash.py for tasks to queue [2016-08-16 15:12:10,258] {models.py:162} DagFileProcessor174 INFO - Filling up the DagBag from /Users/xuanji_li/airflow/dags/bash_bash_bash.py [2016-08-16 15:12:10,267] {jobs.py:1460} DagFileProcessor174 INFO - DAG(s) ['bash_bash_bash'] retrieved from /Users/xuanji_li/airflow/dags/bash_bash_bash.py [2016-08-16 15:12:10,289] {jobs.py:1062} DagFileProcessor174 INFO - Not processing DAG bash_bash_bash since its max runs has been reached [2016-08-16 15:12:10,290] {models.py:313} DagFileProcessor174 INFO - Finding 'running' jobs without a recent heartbeat [2016-08-16 15:12:10,290] {models.py:319} DagFileProcessor174 INFO - Failing jobs without heartbeat after 2016-08-16 15:09:55.290479 ``` It seems that processing of the dagrun is skipped completely because there are already 16 running dagruns. Binary search tracked down this commit as the one that introduced the bug. The logic added looks wrong to me. https://github.com/apache/incubator-airflow/pull/1716"
4,AIRFLOW-447,AIRFLOW,1471590020000,1472737415000,Bug,Resolved,Blocker,1,683,"Python 3 map object cannot be json-serialized - need lists instead On this line, `source_uris` are generated with `map`: https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/operators/gcs_to_bq.py#L124 In Python 2 `map` would return a list, however in Python 3 `map` returns a `map` object / generator. The Python 3 `map` object cannot be json-serialized downstream when storing corresponding job objects in the database: https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/hooks/bigquery_hook.py#L441 Constructing `source_uris` with a list comprehension instead of `map` resolves the issue for Python 3 and retains compatibility with Python 2."
5,AIRFLOW-607,AIRFLOW,1477931169000,1490736678000,Bug,Resolved,Blocker,1,851,"Cannot initdb on Oracle because of String(5000) fields in model Unfortunately Oracle cannot be used to host the Airflow application DB due to the fact that Oracle limits varchar data types to 4000 bytes. If you try to run the airflow initdb command to install the DB onto Oracle you will end up with the following error: sqlalchemy.exc.DatabaseError: (cx_Oracle.DatabaseError) ORA-00910: specified length too long for its datatype [SQL: '\nCREATE TABLE connection (\n\tid INTEGER NOT NULL, \n\tconn_id VARCHAR2(250 CHAR), \n\tconn_type VARCHAR2(500 CHAR), \n\thost VARCHAR2(500 CHAR), \n\tschema VARCHAR2(500 CHAR), \n\tlogin VARCHAR2(500 CHAR), \n\tpassword VARCHAR2(500 CHAR), \n\tport INTEGER, \n\textra VARCHAR2(5000 CHAR), \n\tPRIMARY KEY (id)\n)\n\n'] Request that we change String(5000) types in the Chart and Connection models to 4000. Thanks!"
6,AIRFLOW-660,AIRFLOW,1480515716000,1558123380000,Bug,Resolved,Blocker,1,2682,"Impossible to record second task failure {code} /var/log/airflow/airflow_scheduler_err.log.10: [SQL: 'INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s)'] [parameters: {'task_id': 'test_task', 'end_date': datetime.datetime(2016, 11, 30, 14, 38, 39, 197485), 'execution_date': datetime.datetime(2016, 11, 30, 0, 0), 'duration': 331.723087, 'start_date': datetime.datetime(2016, 11, 30, 14, 33, 7, 474398), 'dag_id': 'test_dag'}] /var/log/airflow/airflow_scheduler_err.log.10-Process DagFileProcessor314-Process: /var/log/airflow/airflow_scheduler_err.log.10-Traceback (most recent call last): /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap /var/log/airflow/airflow_scheduler_err.log.10- self.run() /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/multiprocessing/process.py"", line 114, in run /var/log/airflow/airflow_scheduler_err.log.10- self._target(*self._args, **self._kwargs) /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/site-packages/airflow/jobs.py"", line 318, in helper /var/log/airflow/airflow_scheduler_err.log.10- pickle_dags) /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/site-packages/airflow/utils/db.py"", line 56, in wrapper /var/log/airflow/airflow_scheduler_err.log.10- session.commit() /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 813, in commit /var/log/airflow/airflow_scheduler_err.log.10- self.transaction.commit() /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 390, in commit /var/log/airflow/airflow_scheduler_err.log.10- self._assert_active(prepared_ok=True) /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 214, in _assert_active /var/log/airflow/airflow_scheduler_err.log.10- % self._rollback_exception /var/log/airflow/airflow_scheduler_err.log.10:InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.IntegrityError) duplicate key value violates unique constraint ""task_fail_pkey"" /var/log/airflow/airflow_scheduler_err.log.10-DETAIL: Key (task_id, dag_id, execution_date)=(test_dag, test_task, 2016-11-30 00:00:00) already exists. {code}"
7,AIRFLOW-695,AIRFLOW,1481580870000,1542279432000,Bug,Closed,Blocker,1,910,"Retries do not execute because dagrun is in FAILED state Currently on the latest master commit (15ff540ecd5e60e7ce080177ea3ea227582a4672), running on the LocalExecutor, retries on tasks do not execute because the state of the corresponding dagrun changes to FAILED. The task instance then gets blocked because ""Task instance's dagrun was not in the 'running' state but in the state 'failed',"" the error message produced by the following lines: https://github.com/apache/incubator-airflow/blob/master/airflow/ti_deps/deps/dagrun_exists_dep.py#L48-L50 This error can be reproduced with the following simple DAG: {code:title=DAG.py|borderStyle=solid} dag = models.DAG(dag_id='test_retry_handling') task = BashOperator( task_id='test_retry_handling_op', bash_command='exit 1', retries=1, retry_delay=datetime.timedelta(minutes=1), dag=dag, owner='airflow', start_date=datetime.datetime(2016, 2, 1, 0, 0, 0)) {code}"
8,AIRFLOW-719,AIRFLOW,1482857456000,1541102707000,Sub-task,Closed,Blocker,1,251,"Skipped operations make DAG finish prematurely Since revision 2630361ca24737c28f458825b20ab11c9c996b17 the SKIPPED and SUCCESS are treated the same, this causes the DAG to finish prematurely when using a Branching operator where on branch is finished."
9,AIRFLOW-727,AIRFLOW,1483390477000,1490374009000,Bug,Resolved,Blocker,1,210,"try_number is not increased A dag that has retries enabled will retry indefinitely as try_number gets reset to 0 in LocalTaskJob as task_instance is not fully populated, but nevertheless saved to the databases."
10,AIRFLOW-738,AIRFLOW,1483735453000,1535980834000,Bug,Resolved,Blocker,1,5821,"XCom: Deadlock found when trying to get lock; try restarting transaction When using the following dag: {code} from datetime import datetime, timedelta import logging import pprint import random # The DAG object; we'll need this to instantiate a DAG from airflow import DAG # Operators; we need this to operate! from airflow.operators.python_operator import PythonOperator start_time = datetime.now().replace(minute=0, second=0, microsecond=0) start_time += timedelta(hours=-1) # timedelta(days=-2) default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': start_time, 'email': ['alex.papanic@gmail.com'], 'email_on_failure': True, 'email_on_retry': True, 'retries': 1, 'retry_delay': timedelta(minutes=1) # 'queue': 'bash_queue', # 'pool': 'backfill', # 'priority_weight': 10, # 'end_date': datetime(2016, 1, 1), } dag = DAG( 'xcom_test', default_args=default_args, schedule_interval='@once') def upload_activity_status(pgconn_id, **context): upstream_task_ids = context['task'].upstream_task_ids logging.info( ""Getting status from upstream task {}"".format(upstream_task_ids)) status = context['ti'].xcom_pull(task_ids=upstream_task_ids) logging.info(""Xcom pull results:\n{}"".format(pprint.pformat(status))) logging.info(""Upload to DB here"") upload_ativity_status = PythonOperator( task_id='upload_activity_status', python_callable=upload_activity_status, op_kwargs={'pgconn_id': 'postgres_conn'}, provide_context=True, dag=dag) def poll_data(params, execution_date, **context): logging.info(""Test polling function for {data_stream}"".format(**params)) status = random.random() < 0.5 output = dict( data_stream=params['data_stream'], timeperiod=execution_date + timedelta(hours=-1), status=status ) return output def poll_data_factory(data_stream, dag): return PythonOperator( task_id='poll_{}'.format(data_stream), python_callable=poll_data, params={u'data_stream': data_stream}, provide_context=True, dag=dag ) poll_streams = [] streams = ['stream' + str(i) for i in range(30)] for data_stream in streams: poll = poll_data_factory(data_stream, dag) poll_streams.append(poll) upload_ativity_status.set_upstream(poll) {code} The following error is thrown: {code} 2017-01-06 21:41:35,824] {jobs.py:1433} INFO - Heartbeating the scheduler Traceback (most recent call last): File ""/Users/bolke/Documents/dev/airflow_env/bin/airflow"", line 4, in <module> __import__('pkg_resources').run_script('airflow==1.7.2.dev0', 'airflow') File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 739, in run_script self.require(requires)[0].run_script(script_name, ns) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 1494, in run_script exec(code, namespace, namespace) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/airflow-1.7.2.dev0-py2.7.egg/EGG-INFO/scripts/airflow"", line 28, in <module> args.func(args) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/airflow-1.7.2.dev0-py2.7.egg/airflow/bin/cli.py"", line 380, in run pool=args.pool, File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/airflow-1.7.2.dev0-py2.7.egg/airflow/utils/db.py"", line 54, in wrapper result = func(*args, **kwargs) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/airflow-1.7.2.dev0-py2.7.egg/airflow/models.py"", line 1334, in run self.handle_failure(e, test_mode, context) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/airflow-1.7.2.dev0-py2.7.egg/airflow/models.py"", line 1407, in handle_failure session.merge(self) File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/session.py"", line 1815, in merge File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/session.py"", line 1861, in _merge File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 831, in get File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 864, in _get_impl File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/loading.py"", line 223, in load_on_ident File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 2756, in one File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 2726, in one_or_none File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 2797, in __iter__ File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 2818, in _execute_and_instances File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 2827, in _get_bind_args File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 2809, in _connection_from_session File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/session.py"", line 966, in connection File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/session.py"", line 971, in _connection_for_bind File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/session.py"", line 382, in _connection_for_bind File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/session.py"", line 276, in _assert_active sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (_mysql_exceptions.OperationalError) (1213, 'Deadlock found when trying to get lock; try restarting transaction') [SQL: u'INSERT INTO xcom (`key`, value, timestamp, execution_date, task_id, dag_id) VALUES (%s, %s, now(), %s, %s, %s)'] [parameters: (u'return_value', '\x80\x02}q\x00(U\x06statusq\x01\x89U\ntimeperiodq\x02cdatetime\ndatetime\nq\x03U\n\x07\xe1\x01\x06\x13\x00\x00\x00\x00\x00q\x04\x85q\x05Rq\x06U\x0bdata_streamq\x07U\x08stream26q\x08u.', datetime.datetime(2017, 1, 6, 20, 0), 'poll_stream26', 'xcom_test')] {code}"
11,AIRFLOW-747,AIRFLOW,1484155872000,1484688596000,Bug,Closed,Blocker,1,1001,"retry_delay not honored In Airflow 1.8 alpha 2, using LocalExecutor, DAGs do not seem to honor the retry_delay parameter, i.e. the retries happen immediately one after the other without waiting the specific retry_delay time. However, the number of retries is honored. I am testing with the following code: from airflow import DAG from airflow.operators.bash_operator import BashOperator from datetime import datetime, timedelta default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2016, 10, 5, 19), 'end_date': datetime(2016, 10, 6, 19), 'email': ['airflow@airflow.com'], 'email_on_failure': False, 'email_on_retry': False, 'retries': 10, 'retry_delay': timedelta(0, 500) } dag = DAG('test_retry_handling_job', default_args=default_args, schedule_interval='@once') task1 = BashOperator( task_id='test_retry_handling_op1', bash_command='exit 1', dag=dag) task2 = BashOperator( task_id='test_retry_handling_op2', bash_command='exit 1', dag=dag) task2.set_upstream(task1)"
12,AIRFLOW-778,AIRFLOW,1484943262000,1486061708000,Bug,Resolved,Blocker,1,267,Metastore Partition Sensor Broken MetastorePartitionSensor always throws an exception on initialization due to 72cc8b3006576153aa30d27643807b4ae5dfb593 . Looks like the tests for this are only run if an explicit flag is set which is how this got past CI. cc [~xuanji]
13,AIRFLOW-803,AIRFLOW,1485341396000,1489525140000,Bug,Resolved,Blocker,1,461,"Manual triggered dags are not running After cgroups+impersonation was added the task_instances for manually created dag_runs are not executed anymore. This is due to the fact the task_instance table is now joined against running dag_runs with a 'scheduled' run_id. This change is however not required, as task_instances will only be in 'scheduled' state when they are send to the executore. Tasks from dag_runs in failed state will not be scheduled by contract."
14,AIRFLOW-821,AIRFLOW,1485857944000,1487511153000,Bug,Resolved,Blocker,1,237,Scheduler dagbag importing not Py3 compatible Function {{update_import_errors}} in scheduler (https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L694) in not Py3 compatible (using {{iteritems}} instead of {{items}}).
15,AIRFLOW-844,AIRFLOW,1486421662000,1486500622000,Bug,Resolved,Blocker,1,159,"CgroupTaskRunner does not create cgroups correctly Right now os.mkdir is used to create cgroups which is incorrect, instead the cgroups library should be used."
16,AIRFLOW-846,AIRFLOW,1486467795000,1489524613000,Task,Closed,Blocker,1,536,"Release schedule, latest tag is too old To my understanding, there is no clear point about the release schedule of the project. The latest tag is 1.7.1.3 from June 2016, which is not well suited for production now days. For example, the latest available release is still affected by AIRFLOW-178 which means that we have to patch the sources on production to work with ZIP files. Could you please share your thoughts and position on the release planning of the project ? Would it be possible to get a newer tag sometimes soon ? Thank you"
17,AIRFLOW-847,AIRFLOW,1486475103000,1558131912000,Bug,Resolved,Blocker,1,220,Xcoms are not passed into SubDAG It's not possible to do a xcom_pull within a subdag None of the following seems to be working: * As templated var in SubDagoperator * As var in SubDagoperator * From within Subdag-factory
18,AIRFLOW-856,AIRFLOW,1486675763000,1486732738000,Bug,Resolved,Blocker,1,316,"Execution_date is always set to None in local_client API def trigger_dag(self, dag_id, run_id=None, conf=None, execution_date=None): dr = trigger_dag.trigger_dag(dag_id=dag_id, run_id=run_id, conf=conf, execution_date=None) return ""Created {}"".format(dr) is the code in local_client. This sets execution_date to None"
19,AIRFLOW-893,AIRFLOW,1487798258000,1487886741000,Bug,Resolved,Blocker,1,259,"Webservers crash when a DAG doesn't have a start date set This commit introduced a bug: [AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger Dag 7c94d81c390881643f94d5e3d7d6fb351a445b72 Where webservers will crash if a DAG does not have a start date set."
20,AIRFLOW-894,AIRFLOW,1487865446000,1516027157000,Bug,Closed,Blocker,1,1158,"Trigger Rules not functioning Code below fails to schedule the join task. This includes with trigger rules for all_done, and one_success. It seems to only occur when dynamically generating tasks. from airflow import DAG from airflow.operators import PythonOperator, BranchPythonOperator, DummyOperator from datetime import datetime, timedelta from datetime import datetime from slackclient import SlackClient default_args = { 'owner': 'analytics', 'depends_on_past': False, #'start_date': sixty_days_ago, 'start_date': datetime(2017, 2, 22), 'retries': 0 # 'retry_delay': timedelta(seconds=30), } dag = DAG( 'Valet_Data', default_args=default_args, schedule_interval='*/5 * * * *', dagrun_timeout=timedelta(seconds=60)) def valet_function(locdata, ds, **kwargs): if locdata == 'D': print(intentionalFail) join = DummyOperator( task_id='join', trigger_rule='all_done', dag=dag ) list = ['A','B','C','D','E','F','G','H','I','J','Z'] for l in list: task = PythonOperator( task_id='{0}_PANTS'.format(l), provide_context=True, python_callable=valet_function, op_kwargs={'locdata': l}, # on_failure_callback=on_failure, # on_success_callback=on_success, dag=dag, )"
21,AIRFLOW-897,AIRFLOW,1487885310000,1487975374000,Bug,Closed,Blocker,1,320,"Dagruns get marked as failed as soon as one root task fails In 1.7.1, all root tasks must have failed in order for a dagrun to fail, in 1.8.0 a single root task failure will fail a dagrun and prevent all other tasks in the DAG from running which is not logical behavior and a regression from 1.7.1. [~bolke][~criccomini]"
22,AIRFLOW-910,AIRFLOW,1488047085000,1569858390000,Sub-task,Resolved,Blocker,1,167,Parallelize dag runs in backfills Currently dag runs are executed sequentially while backfilling. This is a regression and slows down the processing off tasks. [~aoen]
23,AIRFLOW-921,AIRFLOW,1488223549000,1489380318000,Task,Resolved,Blocker,1,600,"1.8.0rc Issues These are the pending issues for the Airflow 1.8.0 release: Blockers: [~bolke] please merge into the next RC and then remove from the list the sub-tasks linked in this JIRA Other Issues: - High DB Load (~8x more than 1.7) - We are still investigating but it's probably not a blocker for the release - (Theories: Might need execution_date index on dag_run (based on slow process list) OR it might be this query which is long running SELECT union_ti.dag_id AS union_ti_dag_id, union_ti.state AS union_ti_state, count( *) AS count_1 FR)) - Front page loading time is a lot slower [~bolke]"
24,AIRFLOW-932,AIRFLOW,1488394470000,1489380308000,Sub-task,Resolved,Blocker,1,90,Backfills delete existing task instances and mark them as removed I'm still investigating.
25,AIRFLOW-934,AIRFLOW,1488425529000,1489519643000,Bug,Resolved,Blocker,1,2440,"airflow delayed the task to start we have a complex DAG which includes many tasks. but recently we found some tasks start delayed. for instance: start_task(which will start at 00:00) --> create_cluster(will finished at 00:11) --> wait_task(start at 00:16) , note: 1. wait_task only has one upstream that is create_cluster 2. the server that the airflow hosts has enough memory, and celeryd_concurrency is 20. below is the log of wait_task: [2017-03-02 00:16:39,602] {models.py:124} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:16:39,603] {models.py:197} INFO - Importing /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:16:39,613] {models.py:284} INFO - Loaded DAG <DAG: etl_prod> [2017-03-02 00:16:40,333] {models.py:124} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:16:40,333] {models.py:197} INFO - Importing /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:16:40,345] {models.py:284} INFO - Loaded DAG <DAG: etl_prod> [2017-03-02 00:16:40,373] {models.py:936} INFO - -------------------------------------------------------------------------------- New run starting @2017-03-02T00:16:40.369560 -------------------------------------------------------------------------------- [2017-03-02 00:16:40,402] {models.py:951} INFO - Queuing into pool None [2017-03-02 00:22:31,161] {models.py:124} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:22:31,162] {models.py:197} INFO - Importing /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:22:31,172] {models.py:284} INFO - Loaded DAG <DAG: etl_prod> [2017-03-02 00:22:31,863] {models.py:124} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:22:31,863] {models.py:197} INFO - Importing /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:22:31,874] {models.py:284} INFO - Loaded DAG <DAG: etl_prod> [2017-03-02 00:22:31,901] {models.py:936} INFO - -------------------------------------------------------------------------------- New run starting @2017-03-02T00:22:31.897547 -------------------------------------------------------------------------------- [2017-03-02 00:22:31,911] {models.py:974} INFO - Executing <Task(BashOperator): wait_mins> on 2017-03-01 00:00:00 [2017-03-02 00:22:31,922] {bash_operator.py:52} INFO - tmp dir root location: /tmp"
26,AIRFLOW-968,AIRFLOW,1489180243000,1556702180000,Bug,Closed,Blocker,1,298,"TravisCI builds in master are failing for Python 2.7 Last good build on master: https://travis-ci.org/apache/incubator-airflow/builds/204780659 First failing build on master: https://travis-ci.org/apache/incubator-airflow/builds/205138766 Python 3.4 builds seem fine, Python 2.7 builds are failing."
27,AIRFLOW-1000,AIRFLOW,1489779310000,1492445369000,Task,Closed,Blocker,1,478,"Rebrand to Apache Airflow instead of Airflow Apache requires branding in the form of ""Apache Airflow"" instead of just ""Airflow"". We should figure out a way to rebrand to ""Apache Airflow"" while having an upgrade path as smooth as possible for our users. This could mean that we first release side-by-side: airflow-1.8.1-apache.incubating apache-airflow-1.8.1-incubating It seems that pip does support some kind of packaging that helps with this (see for example Apache Libcloud)."
28,AIRFLOW-1001,AIRFLOW,1489782787000,1535984422000,Bug,Closed,Blocker,1,1990,"Landing Time shows ""unsupported operand type(s) for -: 'datetime.datetime' and 'NoneType'"" on example_subdag_operator Sample DAG example_subdag_operator shows below Oops page on ""Landing Times"" tab {noformat} ------------------------------------------------------------------------------- Traceback (most recent call last): File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask/app.py"", line 1988, in wsgi_app response = self.full_dispatch_request() File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask/app.py"", line 1641, in full_dispatch_request rv = self.handle_user_exception(e) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask/app.py"", line 1544, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask/app.py"", line 1639, in full_dispatch_request rv = self.dispatch_request() File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask/app.py"", line 1625, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask_admin/base.py"", line 69, in inner return self._run_view(f, *args, **kwargs) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask_admin/base.py"", line 368, in _run_view return fn(self, *args, **kwargs) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask_login.py"", line 755, in decorated_view return func(*args, **kwargs) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/www/utils.py"", line 125, in wrapper return f(*args, **kwargs) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/www/views.py"", line 1560, in landing_times secs = (ti.end_date - ts).total_seconds() TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'NoneType' {noformat} http://somehost:18111/admin/airflow/landing_times?root=&days=30&dag_id=example_subdag_operator"
29,AIRFLOW-1011,AIRFLOW,1489933675000,1558131584000,Bug,Resolved,Blocker,1,426,"Fix bug in BackfillJob._execute() for SubDAGs The attached test SubDAG is not executed when the parent DAG is triggered manually. Attached is a simple test DAG that exhibits the issue along with screenshots showing the UI differences between v1.8 and v1.7.1.3. Note that if the DAG is run via backfill from command line (e.g. ""airflow backfill Test_SubDAG -s 2017-03-18 -e 2017-03-18"") the task instances show up successfully."
30,AIRFLOW-1018,AIRFLOW,1490049741000,1580461132000,Bug,Resolved,Blocker,1,409,"Scheduler DAG processes can not log to stdout Each DAG has its own log file for the scheduler and we can specify the directory with child_process_log_directory param. Unfortunately we can not change device / by specifying /dev/stdout for example. That is very useful when we execute Airflow in a container. When we specify /dev/stdout it raises: ""OSError: [Errno 20] Not a directory: '/dev/stdout/2017-03-19'"""
31,AIRFLOW-1033,AIRFLOW,1490286084000,1491480235000,Bug,Resolved,Blocker,1,2377,"TypeError: can't compare datetime.datetime to NoneType in prev_dagrun_dep.py Dear, When starting a specific new dag we get the following error: [2017-03-23 16:51:16,354] {jobs.py:354} DagFileProcessor908 ERROR - Got an exception! Propagating... Traceback (most recent call last): File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper pickle_dags) File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1581, in process_file self._process_dags(dagbag, dags, ti_keys_to_schedule) File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1174, in _process_dags self._process_task_instances(dag, tis_out) File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 905, in _process_task_instances session=session): File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/usr/lib/python2.7/site-packages/airflow/models.py"", line 1116, in are_dependencies_met session=session): File ""/usr/lib/python2.7/site-packages/airflow/models.py"", line 1140, in get_failed_dep_statuses dep_context): File ""/usr/lib/python2.7/site-packages/airflow/ti_deps/deps/base_ti_dep.py"", line 94, in get_dep_statuses for dep_status in self._get_dep_statuses(ti, session, dep_context): File ""/usr/lib/python2.7/site-packages/airflow/ti_deps/deps/prev_dagrun_dep.py"", line 47, in _get_dep_statuses if dag.previous_schedule(ti.execution_date) < ti.task.start_date: TypeError: can't compare datetime.datetime to NoneType I have added some debug code to the file 'prev_dagrun_dep.py: dag = ti.task.dag print 'Start dates:' print 'previous_exection_date: %s'%(dag.previous_schedule(ti.execution_date)) print 'current start date: %s'%(ti.task.start_date) if dag.catchup: if dag.previous_schedule(ti.execution_date) < ti.task.start_date: And this is the output I get: Start dates: previous_exection_date: None current start date: 2017-03-19 00:00:00 I think it is normall that the previous_exection_date is null, since it is the first time this dag is being run. But why is the start_date of the dag important, and not the start date of the run? I have the feeling the cause is the 'schedule_interval', which is set to None. Please find an example and it's log file as an attachment to this mail. Bert"
32,AIRFLOW-1050,AIRFLOW,1490674660000,1491544846000,Bug,Resolved,Blocker,1,1336,"Retries ignored - regression SubDag fails when first operator fails, despite the fact it's configured for retries. Information in UI afterwards are also incorrect. From SubDag prospective it's still {{running}} with operator marked as {{up_for_retry}}, from main DAG prospective, whole run is marked as {{failed}} same as SubDag. See attached screenshots. Latest not affected version is RC4 (310fb58). I tested RC5, 1.8.0 with LocalExecutor and CeleryExecutor. Example code: {code} from datetime import datetime, timedelta from airflow.models import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators.python_operator import PythonOperator from airflow.operators.subdag_operator import SubDagOperator args = { ""start_date"": datetime.today(), } dag = DAG( dag_id=""main"", default_args=args, dagrun_timeout=timedelta(minutes=60), schedule_interval=None, max_active_runs=1 ) sub_dag = DAG( dag_id=""main.test"", default_args=args, schedule_interval=None, ) op = BashOperator( task_id=""first"", dag=sub_dag, bash_command=""echo 1"" ) def throw_error(): raise RuntimeError() op2 = PythonOperator( task_id=""second"", dag=sub_dag, python_callable=throw_error, retries=3, retry_delay=timedelta(0, 20) ) op >> op2 prepare_environment = SubDagOperator( task_id='test', subdag=sub_dag, default_args=args, dag=dag, ) {code}"
33,AIRFLOW-1103,AIRFLOW,1491970176000,1516027138000,Task,Closed,Blocker,1,406,How we can check our developer programmes are pick which environment variables Hi I have updated environment variable .bashrc .profile but those are not picking up from programmes which is running from airflow what is the command to check what are the environment variables are picking by airflow ps eww 22559 can you guid me on this command or any other tool to find which environment variables are taking
34,AIRFLOW-1121,AIRFLOW,1492551627000,1497401870000,Bug,Resolved,Blocker,1,1167,"airflow webserver --pid no longer write out pid file! I've run into a regression with the webserver. It looks like the --pid argument is no longer honored in 1.8.1. The pid file is not being written out! As a result, watchdog processes like monitd, which watch the processes mentioned in the pid file, keep trying to spawn webservers because the pid files are not being written. HISTTIMEFORMAT=""%d/%m/%y %T "" PYTHONPATH=/usr/local/agari/ep-pipeline/production/current/analysis/cluster/:/usr/local/agari/ep-pipeline/production/current/analysis/lookups/ TMP=/data/tmp AIRFLOW_HOME=/data/airflow PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin airflow webserver -p 8080 --pid /data/airflow/pids/airflow-webserver.pid The Patch that might have broken this is https://github.com/apache/incubator-airflow/commit/a9b20a04 If I include -D, things work: airflow webserver -D --pid ~/this.pid sid-as-mbp:~ siddharth$ ls -lrt *pid -rw-r--r-- 1 siddharth staff 5 Apr 18 14:37 this-monitor.pid -rw-r--r-- 1 siddharth staff 5 Apr 18 14:37 this.pid sid-as-mbp:~ siddharth$ cat *pid 8732 8735 However, this doesn't help me as I don't want to run with -D. cc [~sekikn] [~criccomini]"
35,AIRFLOW-1124,AIRFLOW,1492599475000,1503954859000,Bug,Resolved,Blocker,1,166,"Do not set all task instances to scheduled on backfill Backfills are supposed to fill in the gaps, but in 1.8.0 we set all tasks to scheduled, so they will run again."
36,AIRFLOW-1127,AIRFLOW,1492631470000,1494351336000,Improvement,Closed,Blocker,1,497,"Move license notices to LICENSE instead of NOTICE For all the bundled files with different licenses (MIT, BSD, etc), the full texts of these licenses should be in the source tarball preferably at the end of the LICENSE file. webgl-2d needs to be called out as MIT license. Are all the entries in the NOTICE file required or do they just need to be in the LICENSE file? Any additions to the NOTICE have downstream repercussions as they need to be propagated down by any other project using airflow."
37,AIRFLOW-1132,AIRFLOW,1492709647000,1492709880000,Bug,Closed,Blocker,1,1570,"Clear DAG via UI causes exception When I try to clear some of my failed tasks via the UI, I get: {noformat} Traceback (most recent call last): File ""/usr/lib64/python2.7/site-packages/flask/app.py"", line 1988, in wsgi_app response = self.full_dispatch_request() File ""/usr/lib64/python2.7/site-packages/flask/app.py"", line 1641, in full_dispatch_request rv = self.handle_user_exception(e) File ""/usr/lib64/python2.7/site-packages/flask/app.py"", line 1544, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/usr/lib64/python2.7/site-packages/flask/app.py"", line 1639, in full_dispatch_request rv = self.dispatch_request() File ""/usr/lib64/python2.7/site-packages/flask/app.py"", line 1625, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/usr/lib64/python2.7/site-packages/flask_admin/base.py"", line 69, in inner return self._run_view(f, *args, **kwargs) File ""/usr/lib64/python2.7/site-packages/flask_admin/base.py"", line 368, in _run_view return fn(self, *args, **kwargs) File ""/usr/lib64/python2.7/site-packages/flask_login.py"", line 758, in decorated_view return func(*args, **kwargs) File ""/usr/lib/python2.7/site-packages/airflow/www/utils.py"", line 125, in wrapper return f(*args, **kwargs) File ""/usr/lib/python2.7/site-packages/airflow/www/utils.py"", line 172, in wrapper return f(*args, **kwargs) File ""/usr/lib/python2.7/site-packages/airflow/www/views.py"", line 1013, in clear dag = dag.sub_dag( AttributeError: 'NoneType' object has no attribute 'sub_dag' {noformat} These are failed DAGs with a failed task."
38,AIRFLOW-1138,AIRFLOW,1492787582000,1562254362000,Task,Resolved,Blocker,1,152,Add licenses to files in scripts directory These two files need license headers: modified: scripts/ci/requirements.txt modified: scripts/systemd/airflow
39,AIRFLOW-1142,AIRFLOW,1493079256000,1558131588000,Bug,Resolved,Blocker,1,588,"SubDAG Tasks Not Executed Even Though All Dependencies Met Testing on 1.8.1rc1, we noticed that tasks in subdags were not getting executed even though all dependencies had been met. We were able to create a simple test DAG that re-creates the issue. Attached is a test DAG, the log file of the subdag operator that shows it fails to run even though dependencies are met, and screenshots of what the UI looks like. This is definitely a regression as we have many similarly constructed DAGs that have been running successfully on a pre-v1.8 version (a fork of 1.7.1.3+master) for some time."
40,AIRFLOW-1178,AIRFLOW,1494178685000,1537533752000,Bug,Closed,Blocker,1,1234,"@once may run more than one time My DAG is running second (2nd) time although it is declared as @once. Here's DAG definition : {noformat} main_dag = DAG( dag_id = 'Test-DAG-1', default_args = default_args, # dafeult operators' arguments - see above user_defined_macros = dag_macros, # I do not get different between ## params = dag_macros, # user_defined_macros and params # start_date = datetime.now(), # or e.g. datetime(2015, 6, 1) # 'end_date' = datetime(2016, 1, 1), catchup = True, # Perform scheduler catchup (or only run latest)? # - defaults to True schedule_interval = '@once', # '@once'=None? # doesn't create multiple dag runs automatically concurrency = 3, # task instances allowed to run concurrently max_active_runs = 1, # only one DAG run at a time dagrun_timeout = timedelta(days=4), # no way this dag should ran for 4 days orientation = 'TB', # default graph view ) {noformat} As a workaround for AIRFLOW-1013 I changed catchup from False to True. Suggested on dev list. It ""worked around"" AIRFLOW-1013 execution, but screwed @once logic - the DAG got scheduled twice (!) which is a no-go for us. The DAG actually has to run not more than 1 time. IMO, catchup=True should be explicitly disallowed for @once schedule."
41,AIRFLOW-1290,AIRFLOW,1496868784000,1496936058000,Bug,Resolved,Blocker,1,330,"Change docs author from ""Maxime Beauchemin"" to ""Apache Airflow"" In `docs/conf.py`, the author of the docs is set to ""Maxime Beauchemin"" who was the original author of the docs, but since then the whole community has taken over authoring the Airflow documentation. Change all references from ""Maxime Beauchemin"" to ""Apache Airflow"""
42,AIRFLOW-1291,AIRFLOW,1496875106000,1505332197000,Bug,Resolved,Blocker,1,49,Update NOTICE and LICENSE files to meet ASF specs
43,AIRFLOW-1294,AIRFLOW,1496906049000,1497367452000,Bug,Resolved,Blocker,1,302,"Backfills can loose tasks to execute due to tasks setting themselves to NONE In the backfills we can loose tasks to execute due to a task setting its own state to NONE if concurrency limits are reached, this makes them fall outside of the scope the backfill is managing hence they will not be executed."
44,AIRFLOW-1296,AIRFLOW,1496967081000,1534855855000,Bug,Resolved,Blocker,1,1963,"DAGs using operators involving cascading skipped tasks fail prematurely So this is basically the same issue as AIRFLOW-872 and AIRFLOW-719. A workaround had fixed this (https://github.com/apache/incubator-airflow/pull/2125), but was later reverted (https://github.com/apache/incubator-airflow/pull/2195). I totally agree with the reason for reverting, but I still think this is an issue. The issue is related to any operators that involves cascading skipped tasks, like ShortCircuitOperator or LatestOnlyOperator. These operators mark only their *direct* downstream task as SKIPPED, but additional downstream tasks from that skipped task is left up to the scheduler to cascade the SKIPPED state (see latest only op docs about this expected behavior https://airflow.incubator.apache.org/concepts.html#latest-run-only). However, instead the scheduler marks the DAG run as FAILED prematurely before the DAG has a chance to skip all downstream tasks. This example DAG should reproduce the issue: https://gist.github.com/dhuang/61d38fb001c3a917edf4817bb0c915f9. Expected result: DAG succeeds with tasks - latest_only (success) -> dummy1 (skipped) -> dummy2 (skipped) -> dummy3 (skipped) Actual result: DAG fails with tasks - latest_only (success) -> dummy1 (skipped) -> dummy2 (none) -> dummy3 (none) I believe the results I'm seeing are because of this deadlock prevention logic, https://github.com/apache/incubator-airflow/blob/1.8.1/airflow/models.py#L4182. While that actual result shown above _could_ mean a deadlock, in this case it shouldn't be. Since this {{update_state}} logic is reached first in each scheduler run, dummy2/dummy3 don't get a chance to cascade the SKIPPED state. Commenting out that block gives me the results I expect. [~bolke] I know you spent awhile trying to reproduce my issue and weren't able to, but I'm still hitting this on a fresh environment, default configs, sqlite/mysql dbs, local/sequential/celery executors, and 1.8.1/master."
45,AIRFLOW-1482,AIRFLOW,1501684834000,1564723828000,Bug,Open,Blocker,1,4596,"Error when try to backfill the example_trigger_controller_dag Hello, Running a backfill command for the {noformat}example_trigger_controller_dag{noformat} example dag, result in the failed task {noformat}test_trigger_dagrun{noformat} It seems to me that the problem comes from the TriggerDagRunOperator in the example_trigger_controller_dag ? Backfill command: {noformat}airflow backfill -s 2017-07-10 -e 2017-07-13 --pool backfill example_trigger_controller_dag{noformat} Tested in 1.8.1 and 1.8.2rc1 Here is the output log from the backfill command : {noformat} [2017-08-02 13:53:00,844] {__init__.py:57} INFO - Using executor CeleryExecutor [2017-08-02 13:53:00,888] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt [2017-08-02 13:53:00,902] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt /var/lib/airflow/local/lib/python2.7/site-packages/airflow/www/app.py:23: FlaskWTFDeprecationWarning: ""flask_wtf.CsrfProtect"" has been renamed to ""CSRFProtect"" and will be removed in 1.0. csrf = CsrfProtect() [2017-08-02 13:53:01,033] {models.py:168} INFO - Filling up the DagBag from /var/lib/airflow/dags [2017-08-02 13:53:01,332] {models.py:1128} INFO - Dependencies all met for <TaskInstance: example_trigger_controller_dag.test_trigger_dagrun 2017-07-10 00:00:00 [scheduled]> [2017-08-02 13:53:01,337] {base_executor.py:50} INFO - Adding to queue: airflow run example_trigger_controller_dag test_trigger_dagrun 2017-07-10T00:00:00 --pickle 1 --local --pool backfill [2017-08-02 13:53:06,267] {celery_executor.py:81} INFO - [celery] queuing (u'example_trigger_controller_dag', u'test_trigger_dagrun', datetime.datetime(2017, 7, 10, 0, 0)) through celery, queue=default [2017-08-02 13:53:06,330] {models.py:4164} INFO - Updating state for <DagRun example_trigger_controller_dag @ 2017-07-10 00:00:00: backfill_2017-07-10T00:00:00, externally triggered: False> considering 1 task(s) [2017-08-02 13:53:06,334] {jobs.py:2020} INFO - [backfill progress] | finished run 0 of 1 | tasks waiting: 0 | succeeded: 0 | kicked_off: 1 | failed: 0 | skipped: 0 | deadlocked: 0 | not ready: 0 [2017-08-02 13:53:11,273] {jobs.py:1743} ERROR - Executor reports task instance <TaskInstance: example_trigger_controller_dag.test_trigger_dagrun 2017-07-10 00:00:00 [queued]> finished (failed) although the task says its queued. Was the task killed externally? [2017-08-02 13:53:11,273] {models.py:1433} ERROR - Executor reports task instance <TaskInstance: example_trigger_controller_dag.test_trigger_dagrun 2017-07-10 00:00:00 [queued]> finished (failed) although the task says its queued. Was the task killed externally? None [2017-08-02 13:53:11,273] {models.py:1457} INFO - Marking task as FAILED. [2017-08-02 13:53:11,279] {models.py:1478} ERROR - Executor reports task instance <TaskInstance: example_trigger_controller_dag.test_trigger_dagrun 2017-07-10 00:00:00 [queued]> finished (failed) although the task says its queued. Was the task killed externally? [2017-08-02 13:53:11,281] {jobs.py:1694} ERROR - Task instance <TaskInstance: example_trigger_controller_dag.test_trigger_dagrun 2017-07-10 00:00:00 [failed]> failed [2017-08-02 13:53:11,283] {models.py:4164} INFO - Updating state for <DagRun example_trigger_controller_dag @ 2017-07-10 00:00:00: backfill_2017-07-10T00:00:00, externally triggered: False> considering 1 task(s) [2017-08-02 13:53:11,285] {models.py:4204} INFO - Marking run <DagRun example_trigger_controller_dag @ 2017-07-10 00:00:00: backfill_2017-07-10T00:00:00, externally triggered: False> failed [2017-08-02 13:53:11,298] {jobs.py:2020} INFO - [backfill progress] | finished run 1 of 1 | tasks waiting: 0 | succeeded: 0 | kicked_off: 0 | failed: 1 | skipped: 0 | deadlocked: 0 | not ready: 0 Traceback (most recent call last): File ""/var/lib/airflow/bin/airflow"", line 28, in <module> args.func(args) File ""/var/lib/airflow/local/lib/python2.7/site-packages/airflow/bin/cli.py"", line 167, in backfill pool=args.pool) File ""/var/lib/airflow/local/lib/python2.7/site-packages/airflow/models.py"", line 3373, in run job.run() File ""/var/lib/airflow/local/lib/python2.7/site-packages/airflow/jobs.py"", line 201, in run self._execute() File ""/var/lib/airflow/local/lib/python2.7/site-packages/airflow/jobs.py"", line 2063, in _execute raise AirflowException(err) airflow.exceptions.AirflowException: --------------------------------------------------- Some task instances failed: set([(u'example_trigger_controller_dag', u'test_trigger_dagrun', datetime.datetime(2017, 7, 10, 0, 0))]) {noformat}"
46,AIRFLOW-1494,AIRFLOW,1502177433000,1535980836000,Bug,Resolved,Blocker,1,12327,"backfill job failed because new retry comes out when a valid job is running I have a spark job, wrapped in a BASH command to run. From fail log, i found airflow try to rerun the job while the job is running. Then a series strange things happened. My job finally failed. Logs Below: [2017-08-07 23:24:35,903] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:35,903] {bash_operator.py:94} INFO - 17/08/07 23:24:35 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:36,904] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:36,904] {bash_operator.py:94} INFO - 17/08/07 23:24:36 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:37,905] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:37,904] {bash_operator.py:94} INFO - 17/08/07 23:24:37 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:38,906] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:38,906] {bash_operator.py:94} INFO - 17/08/07 23:24:38 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:38,947] {cli.py:391} INFO - Loading pickle id 148 [2017-08-07 23:24:39,020] {base_task_runner.py:112} INFO - Running: ['bash', '-c', u'airflow run generated_daily submit_operator 2017-08-05T00:00:00 --pickle 148 --job_id 15372 --raw'] [2017-08-07 23:24:39,426] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,426] {__init__.py:57} INFO - Using executor CeleryExecutor [2017-08-07 23:24:39,495] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,494] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt [2017-08-07 23:24:39,519] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,519] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt [2017-08-07 23:24:39,710] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,710] {cli.py:391} INFO - Loading pickle id 148 [2017-08-07 23:24:39,772] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,772] {models.py:1120} INFO - Dependencies not met for <TaskInstance: generated_daily.submit_operator 2017-08-05 00:00:00 [running]>, dependency 'Task Instance Not Already Running' FAILED: Task is already running, it started on 2017-08-07 20:55:12.727910. [2017-08-07 23:24:39,777] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,777] {models.py:1120} INFO - Dependencies not met for <TaskInstance: generated_daily.submit_operator 2017-08-05 00:00:00 [running]>, dependency 'Task Instance State' FAILED: Task is in the 'running' state which is not a valid state for execution. The task must be cleared in order to be run. [2017-08-07 23:24:39,907] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,907] {bash_operator.py:94} INFO - 17/08/07 23:24:39 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:40,908] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:40,908] {bash_operator.py:94} INFO - 17/08/07 23:24:40 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:41,909] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:41,909] {bash_operator.py:94} INFO - 17/08/07 23:24:41 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:42,911] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:42,910] {bash_operator.py:94} INFO - 17/08/07 23:24:42 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:43,912] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:43,912] {bash_operator.py:94} INFO - 17/08/07 23:24:43 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:44,120] {jobs.py:2148} WARNING - Recorded pid 116446 is not a descendant of the current pid 33416 [2017-08-07 23:24:44,914] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:44,913] {bash_operator.py:94} INFO - 17/08/07 23:24:44 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:45,914] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:45,914] {bash_operator.py:94} INFO - 17/08/07 23:24:45 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:46,032] {jobs.py:2155} WARNING - State of this instance has been externally set to up_for_retry. Taking the poison pill. So long. [2017-08-07 23:24:46,081] {helpers.py:220} WARNING - Terminating descendant processes of ['/usr/bin/python /usr/local/bin/airflow run generated_daily submit_operator 2017-08-05T00:00:00 --pickle 148 --job_id 15015 --raw'] PID: 116446 [2017-08-07 23:24:46,081] {helpers.py:224} WARNING - Terminating descendant process ['bash', '/tmp/airflowtmpCFeIBn/submit_operatorGbO01R'] PID: 116496 [2017-08-07 23:24:46,087] {helpers.py:224} WARNING - Terminating descendant process ['/bin/sh', '/mnt/data/firework-0.2.0/bin/spark-submit', '--spark-version', '1.6.2', '--master', 'yarn-cluster', '--num-executors', '128', '--class', 'com.hulu.reco.metrics.middlelayer.BatchApplication', '--conf', 'spark.date=20170805', '--conf', 'spark.yarn.maxAppAttempts=1', '--conf', 'spark.driver.memory=8g', '--conf', 'spark.executor.memory=36g', '--conf', 'spark.executor.cores=8', '--conf', 'spark.user.javahome.enabled=true', '--conf', 'spark.user.javahome.path=/usr/lib/jvm/hulu-oracle-jdk8', '--conf', 'spark.output=hdfs://warehousestore/user/pcdm/generated_prod/20170805', '--queue', 'spark', '/home/deploy/middlelayer-1.0-SNAPSHOT-jar-with-dependencies.jar'] PID: 116498 [2017-08-07 23:24:46,091] {helpers.py:224} WARNING - Terminating descendant process ['/usr/java/jdk8/jdk1.8.0_92-1//bin/java', '-cp', '/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/conf/:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/assembly/target/scala-2.10/spark-assembly-1.6.2-hadoop2.6.0-cdh5.7.3-201612201803.jar:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/lib_managed/jars/datanucleus-core-3.2.10.jar:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/hadoop-conf/', 'org.apache.spark.deploy.SparkSubmit', '--master', 'yarn-cluster', '--conf', 'spark.executor.memory=36g', '--conf', 'spark.driver.memory=8g', '--conf', 'spark.user.javahome.path=/usr/lib/jvm/hulu-oracle-jdk8', '--conf', 'spark.output=hdfs://warehousestore/user/pcdm/generated_prod/20170805', '--conf', 'spark.yarn.maxAppAttempts=1', '--conf', 'spark.executor.cores=8', '--conf', 'spark.date=20170805', '--conf', 'spark.user.javahome.enabled=true', '--class', 'com.hulu.reco.metrics.middlelayer.BatchApplication', '--num-executors', '128', '--queue', 'spark', '/home/deploy/middlelayer-1.0-SNAPSHOT-jar-with-dependencies.jar'] PID: 116761 [2017-08-07 23:24:46,096] {helpers.py:231} WARNING - Waiting up to 5s for processes to exit... [2017-08-07 23:24:46,099] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:46,099] {bash_operator.py:94} INFO - 17/08/07 23:24:46 INFO util.ShutdownHookManager: Shutdown hook called [2017-08-07 23:24:46,100] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:46,100] {bash_operator.py:94} INFO - 17/08/07 23:24:46 INFO util.ShutdownHookManager: Deleting directory /mnt/data/tmp/spark-b5f3c795-7ea2-4ef1-a47c-2507b0747580 [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:46,118] {bash_operator.py:97} INFO - Command exited with return code -15 [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:46,118] {models.py:1417} ERROR - Bash command failed [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: Traceback (most recent call last): [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1374, in run [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: result = task_copy.execute(context=context) [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/lib/python2.7/dist-packages/airflow/operators/bash_operator.py"", line 100, in execute [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: raise AirflowException(""Bash command failed"") [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: AirflowException: Bash command failed [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:46,120] {models.py:1433} INFO - Marking task as UP_FOR_RETRY [2017-08-07 23:24:46,123] {helpers.py:234} WARNING - Done waiting [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:46,130] {models.py:1462} ERROR - Bash command failed [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: Traceback (most recent call last): [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/bin/airflow"", line 28, in <module> [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: args.func(args) [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 422, in run [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask: pool=args.pool, [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/lib/python2.7/dist-packages/airflow/utils/db.py"", line 53, in wrapper [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask: result = func(*args, **kwargs) [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1374, in run [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask: result = task_copy.execute(context=context) [2017-08-07 23:24:46,133] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/lib/python2.7/dist-packages/airflow/operators/bash_operator.py"", line 100, in execute [2017-08-07 23:24:46,133] {base_task_runner.py:95} INFO - Subtask: raise AirflowException(""Bash command failed"") [2017-08-07 23:24:46,133] {base_task_runner.py:95} INFO - Subtask: airflow.exceptions.AirflowException: Bash command failed [2017-08-07 23:24:51,034] {jobs.py:2083} INFO - Task exited with return code 1 [2017-08-07 23:33:27,872] {cli.py:391} INFO - Loading pickle id 148 [2017-08-07 23:33:27,940] {base_task_runner.py:112} INFO - Running: ['bash', '-c', u'airflow run generated_daily submit_operator 2017-08-05T00:00:00 --pickle 148 --job_id 15376 --raw'] [2017-08-07 23:33:28,302] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:33:28,301] {__init__.py:57} INFO - Using executor CeleryExecutor [2017-08-07 23:33:28,359] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:33:28,358] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt [2017-08-07 23:33:28,378] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:33:28,378] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt [2017-08-07 23:33:28,543] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:33:28,543] {cli.py:391} INFO - Loading pickle id 148 [2017-08-07 23:33:28,601] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:33:28,601] {models.py:1120} INFO - Dependencies not met for <TaskInstance: generated_daily.submit_operator 2017-08-05 00:00:00 [up_for_retry]>, dependency 'Not In Retry Period' FAILED: Task is not ready for retry yet but will be retried automatically. Current date is 2017-08-07T23:33:28.601339 and task will be retried at 2017-08-08T00:24:46.119758. [2017-08-07 23:33:32,954] {jobs.py:2083} INFO - Task exited with return code 0"
47,AIRFLOW-1539,AIRFLOW,1503958220000,1535980907000,Bug,Closed,Blocker,1,1925,"python3 error import Dag Seeing the following error when using python3 (fine with python2) {code} Traceback (most recent call last): File ""/opt/conda/lib/python3.6/site-packages/airflow/models.py"", line 263, in process_file m = imp.load_source(mod_name, filepath) File ""/opt/conda/lib/python3.6/imp.py"", line 172, in load_source module = _load(spec) File ""<frozen importlib._bootstrap>"", line 675, in _load File ""<frozen importlib._bootstrap>"", line 655, in _load_unlocked File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed File ""/root/airflow/dags/LoadS3ToHiveTest.py"", line 2, in <module> from LoadS3ToHive import default_args, download_s3_to_hdfs_templated_command, generate_download_s3_to_hdfs_task, \ File ""/root/airflow/dags/LoadS3ToHive.py"", line 74, in <module> globals()[task_name] = DAG(task_name, default_args=default_args, schedule_interval=None) File ""/opt/conda/lib/python3.6/site-packages/airflow/models.py"", line 2664, in __init__ self.fileloc = inspect.getsourcefile(inspect.stack()[1][0]) File ""/opt/conda/lib/python3.6/inspect.py"", line 1465, in stack return getouterframes(sys._getframe(1), context) File ""/opt/conda/lib/python3.6/inspect.py"", line 1442, in getouterframes frameinfo = (frame,) + getframeinfo(frame, context) File ""/opt/conda/lib/python3.6/inspect.py"", line 1411, in getframeinfo filename = getsourcefile(frame) or getfile(frame) File ""/opt/conda/lib/python3.6/inspect.py"", line 666, in getsourcefile if getattr(getmodule(object, filename), '__loader__', None) is not None: File ""/opt/conda/lib/python3.6/inspect.py"", line 703, in getmodule if ismodule(module) and hasattr(module, '__file__'): File ""/opt/conda/lib/python3.6/site-packages/airflow/utils/timeout.py"", line 38, in handle_timeout raise AirflowTaskTimeout(self.error_message) airflow.exceptions.AirflowTaskTimeout: Timeout {code}"
48,AIRFLOW-1641,AIRFLOW,1506332962000,1509112710000,Bug,Closed,Blocker,1,794,"Task gets stuck in queued state Hello, I have one dag with ~20 tasks. The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind. There are some interactions with AWS and a remote DB. I only use LocalExecutor. What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. The manual workaround is to restart the task manually by clearing it. Does anyone have ideas about the issue behind, and how to avoid it for the future? Thanks in advance for your help. PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]"
49,AIRFLOW-1693,AIRFLOW,1507533365000,1558123699000,Bug,Resolved,Blocker,1,1744,"airflow initdb throws errors I am having installation problems. The log for `airflow initdb` are Traceback (most recent call last): File ""/Users/gauripradeep/ENVS/scheduler/bin/airflow"", line 17, in <module> from airflow import configuration File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/airflow/__init__.py"", line 30, in <module> from airflow import settings File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/airflow/settings.py"", line 159, in <module> configure_orm() File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/airflow/settings.py"", line 147, in configure_orm engine = create_engine(SQL_ALCHEMY_CONN, **engine_args) File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/sqlalchemy/engine/__init__.py"", line 391, in create_engine return strategy.create(*args, **kwargs) File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py"", line 80, in create dbapi = dialect_cls.dbapi(**dbapi_args) File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/sqlalchemy/dialects/mysql/mysqldb.py"", line 110, in dbapi return __import__('MySQLdb') File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/MySQLdb/__init__.py"", line 19, in <module> import _mysql ImportError: dlopen(/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/_mysql.cpython-36m-darwin.so, 2): Library not loaded: /usr/local/opt/mysql/lib/libmysqlclient.20.dylib Referenced from: /Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/_mysql.cpython-36m-darwin.so Reason: image not found Other commands and dependencies pip3 install mysqlclient and pip3 install mysql-connector-python-rf pip3 install airflow[mysql]"
50,AIRFLOW-1711,AIRFLOW,1507839702000,1509388549000,Bug,Resolved,Blocker,1,460,"Ldap Attributes not always a ""list"" part 2 in the LDAP auth module `group_contains_user` checks for `resp['attributes'].get(user_name_attr)[0] == username` Some Ldaps apparently have this as a simple string `resp['attributes'].get(user_name_attr) == username` also should be checked. But really a test should be done to see if the return is a 'list' and perform the check differently. If its not a list, python will check both arguments and exit with an error."
51,AIRFLOW-1731,AIRFLOW,1508334610000,1509113011000,Bug,Resolved,Blocker,1,222,Import custom config on PYTHONPATH Currently the PYTHONPATH does not contain the required path to import a custom config as described. This needs to be fixed and the instructions needs to be updated based on user feedback.
52,AIRFLOW-1744,AIRFLOW,1508524368000,1508870800000,Bug,Resolved,Blocker,1,257,"task.retries can be False When adding the max_tries field task.retries can be False (e.g. in case of a faulty day). At least Postgres will not accept ""False"" for an integer field. It is proposed to set it to try_number in case try_number > 0 otherwise to 1."
53,AIRFLOW-1767,AIRFLOW,1509471930000,1509474151000,Bug,Resolved,Blocker,1,285,Airflow Scheduler no longer schedules DAGs The Airflow Scheduler no longer schedules DAGs after this commit on master: https://github.com/apache/incubator-airflow/commit/73549763eac74142b7c4018422bb2f8c897b45a8 Workers never receive any tasks and the scheduler never adjusts DAG state.
54,AIRFLOW-1891,AIRFLOW,1512640209000,1555601916000,Bug,Resolved,Blocker,1,455,Non-ascii character in default configuration template PR for AIRFLOW-966 introduced a non-ascii character (http://unicode.org/cldr/utility/character.jsp?a=2019) in default_airflow.cfg file that blocks Airflow from running when there is no prior configuration (or the configuration is generated from environment variables for example). More details here: https://github.com/apache/incubator-airflow/commit/aa737a582c687e7105ef934ffc4da3dc78438235#r26094657
55,AIRFLOW-1912,AIRFLOW,1513078723000,1535980389000,Bug,Resolved,Blocker,1,291,The log facility airflow.processor should not propagate The root logger will write to stdout. If redirection is used which is the case for processors and task runs (not runners) this can end up in an endless loop in case propagation is True. airflow.processor should not propagate therefore.
56,AIRFLOW-2007,AIRFLOW,1516088029000,1517666939000,Bug,Closed,Blocker,1,1080,"task failed first and then become green When I start to run a dag which contain only one task, the task failed immediately and then become green later. The task just call linux shell command to copy a big file from a folder to another. When I start the task, it become red(failed)(meanwhile, the dagrun become red too), but the ""copy"" process is running on the shell background normally. Then because the dagrun failed, the dag starts the next dag run and failed again. After the process finished normally, the task become green but the dagrun don't. Check: 1.I have checked the logs but there is no error log found. 2.After check the airflow database, I found task_instance table is inserted a failed task record when the task starts. dag_run table is inserted a running record. After a while the dag_run become failed. 3.For the job table, it is inserted a running record, and become success after the shell command finished.(which is normally process) My question: 1.Why the task failed immediately when it started. 2.Where does the airflow code modify the task_instance table?"
57,AIRFLOW-2008,AIRFLOW,1516125932000,1516280244000,Bug,Resolved,Blocker,1,96,Column default of timezone.utcnow() should be a callable Otherwise it will be all the same value
58,AIRFLOW-2058,AIRFLOW,1517538607000,1620203452000,Bug,Closed,Blocker,1,2540,"Scheduler uses MainThread for DAG file processing By reading the [source code |https://github.com/apache/incubator-airflow/blob/61ff29e578d1121ab4606fe122fb4e2db8f075b9/airflow/utils/dag_processing.py#L538] it appears the scheduler will process each DAG file, either a .py or .zip, using a new process. If I understand correctly, in theory what should happen in terms of processing a .zip file is that the dedicated process will add the .zip file to the PYTHONPATH, and load the file's module and dependency. When the DAG read is done, the process gets destroyed. And since the PYTHONPATH is process scoped, it won't pollute other processes. However by printing out the threads and process id, it looks like Airflow scheduler can sometimes accidentally pick up the main process instead of creating a new one, and that's when collision happens. Here is snippet of the PYTHONPATH when advanced_dag_dependency-1.zip is being processed. As you can see when it's executed by MainThread, it contains other .zip files. When it's using dedicated thread, only required .zip is added. sys.path :['/root/airflow/dags/yang_subdag_2.zip', '/root/airflow/dags/yang_subdag_2.zip', '/root/airflow/dags/yang_subdag_1.zip', '/root/airflow/dags/yang_subdag_1.zip', '/root/airflow/dags/advanced_dag_dependency-2.zip', '/root/airflow/dags/advanced_dag_dependency-2.zip', '/root/airflow/dags/advanced_dag_dependency-1.zip', '/root/airflow/dags/advanced_dag_dependency-1.zip', '/root/airflow/dags/yang_subdag_1', '/usr/local/bin', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/PILcompat', '/root/airflow/config', '/root/airflow/dags', '/root/airflow/plugins'] Print from MyFirstOperator in Dag 1 process id: 5059 thread id: <_MainThread(*MainThread*, started 140339858560768)> sys.path :[u'/root/airflow/dags/advanced_dag_dependency-1.zip', '/usr/local/bin', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/PILcompat', '/root/airflow/config', '/root/airflow/dags', '/root/airflow/plugins'] Print from MyFirstOperator in Dag 1 process id: 5076 thread id: <_MainThread(*DagFileProcessor283*, started 140137838294784)>"
59,AIRFLOW-2287,AIRFLOW,1523004855000,1523690070000,Improvement,Resolved,Blocker,1,978,"Missing and incorrect license headers * {color:#454545}a few files are missing licenses, like docs/Makefile{color} * {color:#454545}please fix year in notice (""2016 and onwards makes it a little bard to work out when copyright would expire){color} * {color:#454545}LICENSE is OK but some license texts are missing i.e. Bootstrap Toggle, normalize.css, parallel.js. Note that in order to comply with the terms of the the licenses the full text of the license MUST be included.{color} * {color:#454545}also note that ace and d3 are under a BSD 3 clause not BSD 2 clause{color} * {color:#454545} A large number of files are missing the correct ASF header. (see below){color} ** {color:#454545}Re incorrect header not perfect but shows scope of the issue:{color} *** {color:#454545} find . -name ""*.*"" -exec grep ""contributor license"" {} \; -print | wc{color} *** {color:#454545} find . -name ""*.*"" -exec grep ""[http://www.apache.org/licenses/LICENSE-2.0]"" {} \; -print | wc{color}"
60,AIRFLOW-2288,AIRFLOW,1523004955000,1540313991000,Improvement,Resolved,Blocker,1,131,Source tarball should not extract to root {color:#454545}the src tarball extracting to the current{color} directory was surprising.
61,AIRFLOW-2289,AIRFLOW,1523005004000,1585344026000,Improvement,Closed,Blocker,1,37,Add additional quick start to INSTALL
62,AIRFLOW-2290,AIRFLOW,1523005155000,1562254457000,Improvement,Open,Blocker,1,35,Include CVE references in changelog
63,AIRFLOW-2320,AIRFLOW,1523636561000,1535980215000,Bug,Open,Blocker,1,932,"Can not run DAGs since upgraded from Airflow 1.7 We installed Airflow 1.7, and used it for several months. I used PIP to uninstall ariflow 1.7 and install 1.9 (gory details are [here|[https://stackoverflow.com/questions/49544320/airflow-initdb-failed-importerror-no-module-named-log-logging-mixin]).] Since then, I haven't had a single DAG run. I renamed and moved log files to match the 1.9 expectations, but still nothing happens. I have a ""run every 40 minutes"" DAG, it hasn't run since 3/28. When I manually trigger it, no log file is created, nothing happens except I get a running DAG listed under ""DAG Runs"" (I do NOT get anything listed under ""Recent Tasks"", and ""Last Run"" does not get updated. I have a ""Run once"" DAG that I created. I triggered it, same behavior. I have also tried running the example_bash_operator DAG. Same behavior. (I've attached the example_bash_operator.py file, so we're clear what I tried to run)"
64,AIRFLOW-2355,AIRFLOW,1524299192000,1535911214000,Bug,Resolved,Blocker,1,383,"Airflow trigger tag parameters in subdag The command airflow {color:#8eb021}+_trigger_dag -c ""\{'name':'value'}""_+{color} sends conf parameters only to the parent DAG. I'm using SubDags that are dependent on these parameters. And no parameters are recieved by the SubDag. From source code of SubDag operator I see that there is no way of passing these trigger parameters to a Subdag."
65,AIRFLOW-2374,AIRFLOW,1524661485000,1535911731000,Bug,Resolved,Blocker,1,3966,"Airflow fails to show logs When viewing a log in the webserver, the page shows a loading gif and the log never appears. Looking in the Javascript console, the problem appears to be error 500 when loading the {{get_logs_with_metadata}} endpoint, givving the following trace: {code:java} ____/ ( ( ) ) \___ /( ( ( ) _ )) ) )\ (( ( )( ) ) ( ) ) ((/ ( _( ) ( _) ) ( () ) ) ( ( ( (_) (( ( ) .((_ ) . )_ ( ( ) ( ( ) ) ) . ) ( ) ( ( ( ( ) ( _ ( _) ). ) . ) ) ( ) ( ( ( ) ( ) ( )) ) _)( ) ) ) ( ( ( \ ) ( (_ ( ) ( ) ) ) ) )) ( ) ( ( ( ( (_ ( ) ( _ ) ) ( ) ) ) ( ( ( ( ( ) (_ ) ) ) _) ) _( ( ) (( ( )( ( _ ) _) _(_ ( (_ ) (_((__(_(__(( ( ( | ) ) ) )_))__))_)___) ((__) \\||lll|l||/// \_)) ( /(/ ( ) ) )\ ) ( ( ( ( | | ) ) )\ ) ( /(| / ( )) ) ) )) ) ( ( ((((_(|)_))))) ) ( ||\(|(|)|/|| ) ( |(||(||)|||| ) ( //|/l|||)|\\ \ ) (/ / // /|//||||\\ \ \ \ _) ------------------------------------------------------------------------------- Node: airflow-nods-dev ------------------------------------------------------------------------------- Traceback (most recent call last): File ""/opt/airflow/src/apache-airflow/airflow/utils/log/gcs_task_handler.py"", line 113, in _read remote_log = self.gcs_read(remote_loc) File ""/opt/airflow/src/apache-airflow/airflow/utils/log/gcs_task_handler.py"", line 131, in gcs_read return self.hook.download(bkt, blob).decode() File ""/opt/airflow/src/apache-airflow/airflow/contrib/hooks/gcs_hook.py"", line 107, in download .get_media(bucket=bucket, object=object) \ File ""/usr/local/lib/python3.6/dist-packages/oauth2client/_helpers.py"", line 133, in positional_wrapper return wrapped(*args, **kwargs) File ""/usr/local/lib/python3.6/dist-packages/googleapiclient/http.py"", line 841, in execute raise HttpError(resp, content, uri=self.uri) googleapiclient.errors.HttpError: <HttpError 404 when requesting https://www.googleapis.com/storage/v1/b/bucket-af/o/test-logs%2Fgeneric_transfer_single%2Ftransfer_file%2F2018-04-25T13%3A00%3A51.250983%2B00%3A00%2F1.log?alt=media returned ""Not Found""> During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/usr/local/lib/python3.6/dist-packages/flask/app.py"", line 1982, in wsgi_app response = self.full_dispatch_request() File ""/usr/local/lib/python3.6/dist-packages/flask/app.py"", line 1614, in full_dispatch_request rv = self.handle_user_exception(e) File ""/usr/local/lib/python3.6/dist-packages/flask/app.py"", line 1517, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/usr/local/lib/python3.6/dist-packages/flask/_compat.py"", line 33, in reraise raise value File ""/usr/local/lib/python3.6/dist-packages/flask/app.py"", line 1612, in full_dispatch_request rv = self.dispatch_request() File ""/usr/local/lib/python3.6/dist-packages/flask/app.py"", line 1598, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/usr/local/lib/python3.6/dist-packages/flask_admin/base.py"", line 69, in inner return self._run_view(f, *args, **kwargs) File ""/usr/local/lib/python3.6/dist-packages/flask_admin/base.py"", line 368, in _run_view return fn(self, *args, **kwargs) File ""/usr/local/lib/python3.6/dist-packages/flask_login.py"", line 758, in decorated_view return func(*args, **kwargs) File ""/opt/airflow/src/apache-airflow/airflow/www/utils.py"", line 269, in wrapper return f(*args, **kwargs) File ""/opt/airflow/src/apache-airflow/airflow/utils/db.py"", line 74, in wrapper return func(*args, **kwargs) File ""/opt/airflow/src/apache-airflow/airflow/www/views.py"", line 770, in get_logs_with_metadata logs, metadatas = handler.read(ti, try_number, metadata=metadata) File ""/opt/airflow/src/apache-airflow/airflow/utils/log/file_task_handler.py"", line 164, in read log, metadata = self._read(task_instance, try_number, metadata) File ""/opt/airflow/src/apache-airflow/airflow/utils/log/gcs_task_handler.py"", line 120, in _read log += super(GCSTaskHandler, self)._read(ti, try_number) TypeError: must be str, not tuple{code}"
66,AIRFLOW-2387,AIRFLOW,1524850925000,1524858483000,Bug,Closed,Blocker,1,265,Flask 0.12.3 broke web component Recent release of flask 0.12.3 broke the webinterface with this message. Since requirement is >0.12 and < 0.13 any recent build will be broke {code:java} AttributeError: 'Blueprint' object has no attribute 'json_encoder' flask{code}
67,AIRFLOW-2453,AIRFLOW,1526027178000,1535911582000,Bug,Resolved,Blocker,1,1878,"Airflow init fail due lack of default settings kubernetes executor Since last changes in kubernetes config management for kubernetes executor, startup of airflow raise an exception if you dont have setted kubernetes/git_subpath which is was working previous this commit: [https://github.com/apache/incubator-airflow/commit/16bae5634df24132b37eb752fe816f51bf7e83ca#diff-b1d8d65aeaa7d031dfe5b197d6c5aa69L131] This is the trace: [2018-05-11 08:21:57,164] \{configuration.py:212} WARNING - section/key [kubernetes/git_subpath] not found in config Traceback (most recent call last): File ""/usr/local/bin/airflow"", line 22, in <module> from airflow.bin.cli import CLIFactory File ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 57, in <module> from airflow import jobs, settings File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs.py"", line 71, in <module> class BaseJob(Base, LoggingMixin): File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs.py"", line 103, in BaseJob executor=executors.GetDefaultExecutor(), File ""/usr/local/lib/python2.7/dist-packages/airflow/executors/__init__.py"", line 45, in GetDefaultExecutor DEFAULT_EXECUTOR = _get_executor(executor_name) File ""/usr/local/lib/python2.7/dist-packages/airflow/executors/__init__.py"", line 83, in _get_executor return KubernetesExecutor() File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/executors/kubernetes_executor.py"", line 440, in __init__ self.kube_config = KubeConfig() File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/executors/kubernetes_executor.py"", line 117, in __init__ self.git_subpath = conf.get(self.kubernetes_section, 'git_subpath') File ""/usr/local/lib/python2.7/dist-packages/airflow/configuration.py"", line 217, in get ""in config"".format(**locals())) airflow.exceptions.AirflowConfigException: section/key [kubernetes/git_subpath] not found in config"
68,AIRFLOW-2462,AIRFLOW,1526283251000,1535911320000,Bug,Resolved,Blocker,1,303,"airflow.contrib.auth.backends.password_auth.PasswordUser exists bug PasswordUser {quote} @password.setter def _set_password(self, plaintext): self._password = generate_password_hash(plaintext, 12) if PY3: self._password = str(self._password, 'utf-8') {quote} _set_password should be renamed as password."
69,AIRFLOW-2466,AIRFLOW,1526337663000,1535911347000,Bug,Resolved,Blocker,1,457,"_change_state_for_tis_without_dagrun ignores task_id on non-sqlite dbs This filter is supposed to choose tasks based on the subquery: [https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L1013] The filter, however, does not specify task_id, so it will indiscriminately change all tasks on a DagRun if anything matches. This can result in tasks succeeding and then being remarked as failed when a DagRun fails because of a different task."
70,AIRFLOW-2533,AIRFLOW,1527591716000,1535911323000,Bug,Resolved,Blocker,1,397,"Kubernetes worker configuration improperly sets path to DAG on worker node When triggering a DAG using the kubernetes executor, the path to the DAG on the worker node is not properly set due to the modification of the command that is sent to the pod for the worker node to perform. See !Screen Shot 2018-05-24 at 5.06.25 PM.png! This means that the only DAG's that are working at the example DAGs."
71,AIRFLOW-2573,AIRFLOW,1528320260000,1547920258000,Bug,Resolved,Blocker,1,560,"Cast TIMESTAMP field to float rather than int In current bigquery_hook.py, we have a `_bq_cast(string_field, bq_type)` function that help casts a BigQuery row to the appropriate data types. {quote}elif bq_type == 'INTEGER' or bq_type == 'TIMESTAMP': return int(string_field) {quote} However, when a bq_type equals to 'TIMESTAMP', it causes ValueError. {quote}>>> int('1.458668898E9') ValueError: invalid literal for int() with base 10: '1.458668898E9' {quote} Because 'TIMESTAMP' in bigquery is stored as double in python, thus should be cast to float instead."
72,AIRFLOW-2624,AIRFLOW,1529025673000,1535911173000,Bug,Closed,Blocker,1,416,"Airflow webserver broken out of the box `airflow webserver` and then click on any DAG, I get ``` File ""/Users/kevin_yang/ext_repos/incubator-airflow/airflow/www/utils.py"", line 364, in view_func return f(*args, **kwargs) File ""/Users/kevin_yang/ext_repos/incubator-airflow/airflow/www/utils.py"", line 251, in wrapper user = current_user.user.username AttributeError: 'NoneType' object has no attribute 'username' ```"
73,AIRFLOW-2697,AIRFLOW,1530279448000,1579455908000,Improvement,Open,Blocker,1,590,"Drop snakebite in favour of pyarrow The current HdfsHook relies on the snakebite library, which is unfortunately not compatible with Python 3. To add Python 3 support for the HdfsHook requires switching to a different library for interacting with HDFS. The hdfs3 library is an attractive alternative, as it supports Python 3 and seems to be stable and relatively well supported. Update: hdfs3 doesn't get any updates anymore. The best library right now seems to be pyarrow: https://arrow.apache.org/docs/python/filesystems.html Therefore I would like to upgrade to pyarrow instead of hdfs3."
74,AIRFLOW-2834,AIRFLOW,1533104423000,1554986424000,Bug,Resolved,Blocker,1,262,can not see the dag page after build from the newest code in github after build and deploy the newest version of code from github. got the web server opened and the dags page blank with the following error in request resource. !image-2018-08-01-14-20-09-256.png!
75,AIRFLOW-2859,AIRFLOW,1533536201000,1535027289000,Bug,Resolved,Blocker,1,186,DateTimes returned from the database are not converted to UTC This is due to the fact that sqlalchemy-utcdatetime does not convert to UTC when the database returns datetimes with tzinfo.
76,AIRFLOW-2870,AIRFLOW,1533681611000,1534076792000,Bug,Closed,Blocker,1,1493,"Migrations fail when upgrading from below cc1e65623dc7_add_max_tries_column_to_task_instance Running migrations from below cc1e65623dc7_add_max_tries_column_to_task_instance.py fail with: {noformat} INFO [alembic.runtime.migration] Context impl PostgresqlImpl. INFO [alembic.runtime.migration] Will assume transactional DDL. INFO [alembic.runtime.migration] Running upgrade 127d2bf2dfa7 -> cc1e65623dc7, add max tries column to task instance Traceback (most recent call last): File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1182, in _execute_context context) File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 470, in do_execute cursor.execute(statement, parameters) psycopg2.ProgrammingError: column task_instance.executor_config does not exist LINE 1: ...ued_dttm, task_instance.pid AS task_instance_pid, task_insta... {noformat} The failure is occurring because cc1e65623dc7_add_max_tries_column_to_task_instance.py imports TaskInstance from the current code version, which has changes to the task_instance table that are not expected by the migration. Specifically, 27c6a30d7c24_add_executor_config_to_task_instance.py adds an executor_config column that does not exist as of when cc1e65623dc7_add_max_tries_column_to_task_instance.py is run. It is worth noting that this will not be observed for new installs because the migration branches on table existence/non-existence at a point that will hide the issue from new installs."
77,AIRFLOW-2881,AIRFLOW,1533835113000,1555353758000,Task,Resolved,Blocker,1,385,Add compile_assets to Release Process Documentation AIRFLOW-2691 introduced a `compile_assets` command in setup.py that is required during a release. This should be added to the [release notes|https://cwiki.apache.org/confluence/display/AIRFLOW/Releasing+Airflow#ReleasingAirflow-PublishingtoPyPi] after 1.10.x has been released. {code:java} python setup.py compile_assets sdist {code}
78,AIRFLOW-2898,AIRFLOW,1534254553000,1559208556000,Bug,Closed,Blocker,1,595,"Task not entering queued state for pool I have a pool of 3 and have several jobs (over 10) which use the pool. Tasks timeout (after 10 mins) from being stuck in scheduled state when the tasks should be in queued state for the pool. to reproduce: use attached dag, and create a pool called ""backfill"" with 2 slots start the dag, and go to this url - > <your_server_addr_here>/admin/taskinstance/?flt0_pool_equals=backfill you'll see two light green ""running"" state, loads of white ""scheduled"" state and none in grey ""queued"" state. what I expect would be two in running, the rest in queued state."
79,AIRFLOW-2934,AIRFLOW,1534932541000,1558132710000,Bug,Resolved,Blocker,1,373,"Pools not respected for internal subdag tasks I'm trying to have some subdags execute one task at a time. The way I found was to create a first pool for the SubdagOperators (pool1 in the attached code file) and a second one for the internal tasks (pool2). However, it appears that pools for subdag elements are not being respected. Running airflow 1.9.0 with LocalExecutor."
80,AIRFLOW-2952,AIRFLOW,1535063109000,1547067304000,Bug,Resolved,Blocker,1,481,Dockerized CI pipeline has silently broken integration testing for KubernetesExecutor [~gcuriel] [~bolke] [~Fokko] Looking at all recent builds the new CI pipeline is silently reverting the kubernetes tests to the normal airflow tests. Before https://travis-ci.org/apache/incubator-airflow/jobs/418914949#L1007 After: [https://travis-ci.org/apache/incubator-airflow/jobs/419062412#L4970] This means that kubernetes builds will pass without actually testing on a kubernetes cluster.
81,AIRFLOW-3032,AIRFLOW,1536570441000,1572965437000,Bug,Resolved,Blocker,1,726,"_pickle.UnpicklingError with using remote MySQL Server Hello, I am running Airflow 1.9.0 successfully with a localhost MySQL database, version 5.7.23. I switched sql_alchemy_conn = mysql://airflow:<password>@<remote-host>:3306/airflow in order to use the proper MySQL server - same version 5.7.23. I created a dump from my local instance to the remote one. Issue: * When tasks are executed by the scheduler everything runs fine, tasks are executed and DB updated * When manually triggering a task via the webserver, I am getting ""_pickle.UnpicklingError"" please see error__log.txt for full log In the end, I only changed this one line in airflow.cfg which is causing that I can not use it with a remote MySQL server. Best, Max"
82,AIRFLOW-3036,AIRFLOW,1536627406000,1612366338000,Bug,Reopened,Blocker,1,3213,"Upgrading to Airflow 1.10 not possible using GCP Cloud SQL for MYSQL The upgrade path to airflow 1.10 seems impossible for users of MySQL in Google's Cloud SQL service given new mysql requirements for 1.10. When executing ""airflow upgradedb"" ``` INFO [alembic.runtime.migration] Running upgrade d2ae31099d61 -> 0e2a74e0fc9f, Add time zone awareness Traceback (most recent call last): File ""/usr/local/bin/airflow"", line 32, in <module> args.func(args) File ""/usr/local/lib/python3.6/site-packages/airflow/bin/cli.py"", line 1002, in initdb db_utils.initdb(settings.RBAC) File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 92, in initdb upgradedb() File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 346, in upgradedb command.upgrade(config, 'heads') File ""/usr/local/lib/python3.6/site-packages/alembic/command.py"", line 174, in upgrade script.run_env() File ""/usr/local/lib/python3.6/site-packages/alembic/script/base.py"", line 416, in run_env util.load_python_file(self.dir, 'env.py') File ""/usr/local/lib/python3.6/site-packages/alembic/util/pyfiles.py"", line 93, in load_python_file module = load_module_py(module_id, path) File ""/usr/local/lib/python3.6/site-packages/alembic/util/compat.py"", line 68, in load_module_py module_id, path).load_module(module_id) File ""<frozen importlib._bootstrap_external>"", line 399, in _check_name_wrapper File ""<frozen importlib._bootstrap_external>"", line 823, in load_module File ""<frozen importlib._bootstrap_external>"", line 682, in load_module File ""<frozen importlib._bootstrap>"", line 265, in _load_module_shim File ""<frozen importlib._bootstrap>"", line 684, in _load File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed File ""/usr/local/lib/python3.6/site-packages/airflow/migrations/env.py"", line 91, in <module> run_migrations_online() File ""/usr/local/lib/python3.6/site-packages/airflow/migrations/env.py"", line 86, in run_migrations_online context.run_migrations() File ""<string>"", line 8, in run_migrations File ""/usr/local/lib/python3.6/site-packages/alembic/runtime/environment.py"", line 807, in run_migrations self.get_context().run_migrations(**kw) File ""/usr/local/lib/python3.6/site-packages/alembic/runtime/migration.py"", line 321, in run_migrations step.migration_fn(**kw) File ""/usr/local/lib/python3.6/site-packages/airflow/migrations/versions/0e2a74e0fc9f_add_time_zone_awareness.py"", line 46, in upgrade raise Exception(""Global variable explicit_defaults_for_timestamp needs to be on (1) for mysql"") Exception: Global variable explicit_defaults_for_timestamp needs to be on (1) for mysql ``` Reading documentation for upgrading to airflow 1.10, it seems the requirement for explicit_defaults_for_timestamp=1 was intentional. However, MySQL on Google Cloud SQL does not support configuring this variable and it is off by default. Users of MySQL and Cloud SQL do not have an upgrade path to 1.10. Alas, so close to the mythical Kubernetes Executor. In GCP, Cloud SQL is _the_ hosted MySQL solution. [https://cloud.google.com/sql/docs/mysql/flags]"
83,AIRFLOW-3118,AIRFLOW,1537955437000,1543246477000,Bug,Resolved,Blocker,1,1245,"DAGs not successful on new installation When trying out Airflow, on localhost, none of the DAG runs are getting to the 'success' state. They are getting stuck in 'running', or I manually label them as failed: !image-2018-09-26-12-39-03-094.png! h2. Steps to reproduce # create new conda environment ** conda create -n airflow ** source activate airflow # install airflow ** pip install apache-airflow # initialize Airflow db ** airflow initdb # disable default paused setting in airflow.cfg ** dags_are_paused_at_creation = False # {color:#6a8759}run airflow and airflow scheduler (in separate terminal){color} ** {color:#6a8759}airflow scheduler{color} ** {color:#6a8759}airflow webserver{color} # {color:#6a8759}unpause example_bash_operator{color} ** {color:#6a8759}airflow unpause example_bash_operator{color} # {color:#6a8759}log in to Airflow UI{color} # {color:#6a8759}turn on example_bash_operator{color} # {color:#6a8759}click ""Trigger DAG"" in `example_bash_operator` row{color} h2. {color:#6a8759}Observed result{color} {color:#6a8759}The `example_bash_operator` never leaves the ""running"" state.{color} h2. {color:#6a8759}Expected result{color} {color:#6a8759}The `example_bash_operator` would quickly enter the ""success"" state{color}"
84,AIRFLOW-3164,AIRFLOW,1538760099000,1543351951000,Bug,Resolved,Blocker,1,130,verify certificate of LDAP server Currently we dont verify the certificate of the Ldap server this can lead to security incidents.
85,AIRFLOW-3180,AIRFLOW,1539139409000,1539157943000,Bug,Closed,Blocker,1,4751,"Chinese characters all become gibberish when using BashOperator beeline command insert data into hive table with BashOperator ,i use beeline to insert data into hive ,hql with chinese characters ,after dag run success,hive data contain unreadable code. python : {code:java} # -*- coding: utf-8 -*- import airflow from airflow.models import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators.python_operator import BranchPythonOperator from datetime import datetime import time from datetime import timedelta import sys import pendulum local_tz = pendulum.timezone(""Asia/Shanghai"") reload(sys) sys.setdefaultencoding('utf-8') default_args = { 'owner': 'airflow', 'depends_on_past':False, 'start_date':datetime(2018,10,9,19,22,20,tzinfo=local_tz), 'retries':0 } dag = DAG( 'inserthiveutf8', default_args=default_args, description='null', catchup=False, schedule_interval=None ) adf37 = r"""""" beeline -u ""jdbc:hive2://10.138.***.***:30010/di_zz"" -n ""*****"" -p ""*****"" -e ""insert into di_zz.tt_wms_inout_detail_new(fac_id) values ('')"" """""" abcd8491539084126613 =BashOperator( task_id='abcd8491539084126613', bash_command=adf37, dag=dag){code} i have tried this: {code:java} abcd8491539084126613 =BashOperator( task_id='abcd8491539084126613', bash_command=""sh ~/insert.sh "", dag=dag){code} this: {code:java} export LANG=en_US.UTF-8 beeline -u ""jdbc:hive2://10.138.***.***:30010/di_zz"" -n ""*****"" -p ""*****"" -e 'insert into di_zz.tt_wms_inout_detail_new(fac_id) values ("""")'{code} this: {code:java} beeline -u ""jdbc:hive2://10.138.***.***:30010/di_zz"" -n ""*****"" -p ""*****"" -f ~/hql.sql{code} log: {code:java} [2018-10-09 21:00:58,485] {bash_operator.py:110} INFO - INFO : Compiling command(queryId=hive_20181009210000_89390a92-c4de-413f-9958-4d7da1065ef9): insert into di_zz.tt_wms_inout_detail_new(fac_id) values (""???"") [2018-10-09 21:00:58,485] {bash_operator.py:110} INFO - INFO : Semantic Analysis Completed [2018-10-09 21:00:58,486] {bash_operator.py:110} INFO - INFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_col0, type:timestamp, comment:null), FieldSchema(name:_col1, type:string, comment:null), FieldSchema(name:_col2, type:void, comment:null), FieldSchema(name:_col3, type:void, comment:null), FieldSchema(name:_col4, type:void, comment:null), FieldSchema(name:_col5, type:void, comment:null), FieldSchema(name:_col6, type:void, comment:null), FieldSchema(name:_col7, type:bigint, comment:null), FieldSchema(name:_col8, type:bigint, comment:null), FieldSchema(name:_col9, type:bigint, comment:null), FieldSchema(name:_col10, type:void, comment:null), FieldSchema(name:_col11, type:timestamp, comment:null)], properties:null) [2018-10-09 21:00:58,486] {bash_operator.py:110} INFO - INFO : Completed compiling command(queryId=hive_20181009210000_89390a92-c4de-413f-9958-4d7da1065ef9); Time taken: 0.291 seconds [2018-10-09 21:00:58,486] {bash_operator.py:110} INFO - INFO : Executing command(queryId=hive_20181009210000_89390a92-c4de-413f-9958-4d7da1065ef9): insert into di_zz.tt_wms_inout_detail_new(fac_id) values (""???"") [2018-10-09 21:00:58,486] {bash_operator.py:110} INFO - INFO : Query ID = hive_20181009210000_89390a92-c4de-413f-9958-4d7da1065ef9 {code} data: {code:java} +------------------------------------+---------------------------------+-----------------------------------+----------------------------------+------------------------------------+------------------------------------+---------------------------------------+-----------------------------------+----------------------------------+-----------------------------------+-------------------------------------------+--------------------------------------+--+ | tt_wms_inout_detail_new.stat_date | tt_wms_inout_detail_new.fac_id | tt_wms_inout_detail_new.fac_name | tt_wms_inout_detail_new.ware_id | tt_wms_inout_detail_new.ware_name | tt_wms_inout_detail_new.ware_type | tt_wms_inout_detail_new.product_code | tt_wms_inout_detail_new.ware_cnt | tt_wms_inout_detail_new.ware_in | tt_wms_inout_detail_new.ware_out | tt_wms_inout_detail_new.sap_factory_name | tt_wms_inout_detail_new.di_etl_date | +------------------------------------+---------------------------------+-----------------------------------+----------------------------------+------------------------------------+------------------------------------+---------------------------------------+-----------------------------------+----------------------------------+-----------------------------------+-------------------------------------------+--------------------------------------+--+ | NULL |  | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | {code} this is a simple example .the production script is comlex more Thanks in advance."
86,AIRFLOW-3224,AIRFLOW,1539779882000,1558149209000,Bug,Resolved,Blocker,1,807,"Flask Errors when Installing Airflow 1.10 in Kubernetes I am currently working on deploying Apache Airflow 1.10.0 to a Kubernetes cluster. I am running into some dependency issues with Flask. If I use the current version of flask-login (0.4.1), I receive this error: {{apache-airflow 1.10.0 has requirement flask-login==0.2.11, but you'll have flask-login 0.4.1 which is incompatible.}} With this error, the UI won't render, and instead, I see a text bomb followed by many flask-appbuilder/flask-login warnings. If I use the Airflow's requirement of flask-login (0.2.11), I receive this error: {{flask-appbuilder 1.12.0 has requirement Flask-Login<0.5,>=0.3, but you'll have flask-login 0.2.11 which is incompatible.}} With this error, the UI renders with Airflow 1.9 features and CeleryExecutor won't work."
87,AIRFLOW-3259,AIRFLOW,1540496248000,1547913373000,Bug,Resolved,Blocker,1,1116,"Internal Server Error when creating charts because of sort function h2. {color:#205081}*Problem*{color} h3. When you try to create a chart and display metrics you get an internal server error (See picture below). h3. !Screen Shot 2018-10-25 at 8.01.09 PM.png|width=605,height=222! h2. {color:#205081}*Cause*{color} h3. When you click on the url rendering the internal server error you get this: (See picture below) h2. !Screen Shot 2018-10-25 at 8.01.52 PM.png|width=605,height=535! h3. A dependency issue: the function ""sort"" was deprecated and is no longer a part of pandas Dataframe and thus it creates a problem when creating charts in airflow. h2. {color:#205081}*Solution*{color} h3. The sort function needs to replaced by sort_values instead (See code below) {code:java} # Replace this df = df.sort(df.columns[0]) # By this df = df.sort_values(by=df.columns[0]) {code} In views.py (See picture below) !Screen Shot 2018-10-25 at 8.02.50 PM.png|width=603,height=593! h2. {color:#205081}*Result*{color} The data is loaded and the chart displayed :D !Screen Shot 2018-10-25 at 8.16.53 PM.png|width=607,height=245!"
88,AIRFLOW-3270,AIRFLOW,1540832243000,1556107413000,Bug,Closed,Blocker,1,989,"Apache airflow 1.10.0 integration with LDAP anonmyously Please advise what to include in airflow.cfg when going to integrate with LDAP anonymously ? We are using DS389 as LDAP server vendor name. {noformat} [webserver] authenticate = True auth_backend = airflow.contrib.auth.backends.ldap_auth {noformat} And {noformat} [ldap] uri = ldap://nsp-daf178e8.ad1.prd.us-phx.odc.im:389 user_filter = memberOf=cn=rvs-all-prd_usphx,ou=groups,dc=odc,dc=im user_name_attr = uid group_member_attr = superuser_filter = memberOf=cn=rvd-sudo_all-prd_usphx,ou=groups,dc=odc,dc=im data_profiler_filter = bind_user = bind_password = basedn = ou=people,dc=odc,dc=im cacert = /opt/orchestration/airflow/ldap_ca.crt search_scope = LEVEL {noformat} I am hitting below exception: {noformat} File ""/usr/local/lib/python3.5/site-packages/ldap3/operation/search.py"", line 215, in parse_filter raise LDAPInvalidFilterError('malformed filter') ldap3.core.exceptions.LDAPInvalidFilterError: malformed filter {noformat}"
89,AIRFLOW-3277,AIRFLOW,1540929816000,1543520989000,Bug,Resolved,Blocker,1,415,"Invalid timezone transition handling for cron schedules `following_schedule` converts to naive time by using the local time zone. In case of a DST transition, say 3AM -> 2AM (""summer time to winter time"") we generate date times that could overlap with earlier schedules. Therefore a DAG that should run every 5 minutes will not do so if it has already seen the schedule. We should not convert to naive and keep UTC."
90,AIRFLOW-3279,AIRFLOW,1540983715000,1555353752000,Bug,Closed,Blocker,1,4322,"Documentation for Google Logging unclear The documentation of how to install logging to a Google Cloud bucket is unclear. I am now following the tutorial on the airflow page: [https://airflow.apache.org/howto/write-logs.html] Here I find it unclear what part of the 'logger' I have to adjust in the `{{airflow/config_templates/airflow_local_settings.py}}`. The adjustment states: # Update the airflow.task and airflow.tas_runner blocks to be 'gcs.task' instead of 'file.task'. 'loggers': Unknown macro: \{ 'airflow.task'} However what I find in the template is: |'loggers': \{\| \|'airflow.processor': { \\\\\\\\| \\\\\\\\|'handlers': ['processor'], \\\\\\\\| \\\\\\\\|'level': LOG_LEVEL, \\\\\\\\| \\\\\\\\|'propagate': False, \\\\\\\\| \\\\\\\\|},| |'airflow.task': { \| \|'handlers': ['task'], \| \|'level': LOG_LEVEL, \| \|'propagate': False, \| \|},| |'flask_appbuilder': { \| \|'handler': ['console'], \| \|'level': FAB_LOG_LEVEL, \| \|'propagate': True, \| \|}| }, Since for me it is very important to do it right at the first time I hope some clarity can be provided in what has to be adjusted in the logger. Is it only the 'airflow.task' or more? Furthermore, at step 6 it is a little unclear what remote_log_conn_id means. I would propose to add a little more information to make this more clear. The current error I am facing is: Traceback (most recent call last): File ""/usr/local/bin/airflow"", line 16, in <module> from airflow import configuration File ""/usr/local/lib/python2.7/site-packages/airflow/__init__.py"", line 31, in <module> from airflow import settings File ""/usr/local/lib/python2.7/site-packages/airflow/settings.py"", line 198, in <module> configure_logging() File ""/usr/local/lib/python2.7/site-packages/airflow/logging_config.py"", line 71, in configure_logging dictConfig(logging_config) File ""/usr/local/lib/python2.7/logging/config.py"", line 794, in dictConfig dictConfigClass(config).configure() File ""/usr/local/lib/python2.7/logging/config.py"", line 568, in configure handler = self.configure_handler(handlers[name]) File ""/usr/local/lib/python2.7/logging/config.py"", line 733, in configure_handler result = factory(**kwargs) File ""/usr/local/lib/python2.7/site-packages/airflow/utils/log/gcs_task_handler.py"", line 30, in __init__ super(GCSTaskHandler, self).__init__(base_log_folder, filename_template) File ""/usr/local/lib/python2.7/site-packages/airflow/utils/log/file_task_handler.py"", line 46, in __init__ self.filename_jinja_template = Template(self.filename_template) File ""/usr/local/lib/python2.7/site-packages/jinja2/environment.py"", line 926, in __new__ return env.from_string(source, template_class=cls) File ""/usr/local/lib/python2.7/site-packages/jinja2/environment.py"", line 862, in from_string return cls.from_code(self, self.compile(source), globals, None) File ""/usr/local/lib/python2.7/site-packages/jinja2/environment.py"", line 565, in compile self.handle_exception(exc_info, source_hint=source_hint) File ""/usr/local/lib/python2.7/site-packages/jinja2/environment.py"", line 754, in handle_exception reraise(exc_type, exc_value, tb) File ""<unknown>"", line 1, in template jinja2.exceptions.TemplateSyntaxError: expected token ':', got '}' Error in atexit._run_exitfuncs: Traceback (most recent call last): File ""/usr/local/lib/python2.7/atexit.py"", line 24, in _run_exitfuncs func(*targs, **kargs) File ""/usr/local/lib/python2.7/logging/__init__.py"", line 1676, in shutdown h.close() File ""/usr/local/lib/python2.7/site-packages/airflow/utils/log/gcs_task_handler.py"", line 73, in close if self.closed: AttributeError: 'GCSTaskHandler' object has no attribute 'closed' Error in sys.exitfunc: Traceback (most recent call last): File ""/usr/local/lib/python2.7/atexit.py"", line 24, in _run_exitfuncs func(*targs, **kargs) File ""/usr/local/lib/python2.7/logging/__init__.py"", line 1676, in shutdown h.close() File ""/usr/local/lib/python2.7/site-packages/airflow/utils/log/gcs_task_handler.py"", line 73, in close if self.closed: AttributeError: 'GCSTaskHandler' object has no attribute 'closed' If I look at the Airflow code I see the following code for the GcsTaskHandler: https://github.com/apache/incubator-airflow/blob/v1-10-stable/airflow/utils/log/gcs_task_handler.py Here the closed attributed indeed refers to nowhere, does somebody know how to resolve this issue?"
91,AIRFLOW-3326,AIRFLOW,1541993099000,1551956209000,Bug,Closed,Blocker,1,2645,"High Sierra Complaining 'in progress in another thread when fork() was called' Inside the plugins folder, I have a hook that is a child class of BigQueryHook. {code:java} // code from airflow.contrib.hooks.bigquery_hook import BigQueryHook class BQHook(BigQueryHook): pass{code} When I run the airflow server, it keeps throwing messages complaining 'in progress in another thread when fork() was called', and I can't use the web server UI at all. {code:java} // messages from terminal objc[15098]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. objc[15098]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug. [2018-11-12 14:03:40 +1100] [15102] [INFO] Booting worker with pid: 15102 [2018-11-12 14:03:40,792] {__init__.py:51} INFO - Using executor SequentialExecutor [2018-11-12 14:03:40,851] {base_hook.py:83} INFO - Using connection to: https://custom-data-z00100-dev.appspot.com/ objc[15099]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. objc[15099]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug. [2018-11-12 14:03:40 +1100] [15103] [INFO] Booting worker with pid: 15103 [2018-11-12 14:03:40,902] {base_hook.py:83} INFO - Using connection to: https://custom-data-z00100-dev.appspot.com/ objc[15101]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. objc[15101]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug. [2018-11-12 14:03:40 +1100] [15104] [INFO] Booting worker with pid: 15104 [2018-11-12 14:03:40,948] {base_hook.py:83} INFO - Using connection to: https://custom-data-z00100-dev.appspot.com/ objc[15100]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. objc[15100]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug. {code}"
92,AIRFLOW-3339,AIRFLOW,1542197133000,1542231430000,Bug,Resolved,Blocker,1,3497,"Timezone error when start_date in default_args From slack: nicor88 8:34 AM {quote} Hey all, we are having issue with our scheduler, in the latest release 1.10.1 due to this error: {noformat} Process DagFileProcessor94249-Process: Traceback (most recent call last): File ""/usr/local/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap self.run() File ""/usr/local/lib/python3.6/multiprocessing/process.py"", line 93, in run self._target(*self._args, **self._kwargs) File ""/usr/local/lib/python3.6/site-packages/airflow/jobs.py"", line 389, in helper pickle_dags) File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 74, in wrapper return func(*args, **kwargs) File ""/usr/local/lib/python3.6/site-packages/airflow/jobs.py"", line 1846, in process_file self._process_dags(dagbag, dags, ti_keys_to_schedule) File ""/usr/local/lib/python3.6/site-packages/airflow/jobs.py"", line 1426, in _process_dags dag_run = self.create_dag_run(dag) File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 74, in wrapper return func(*args, **kwargs) File ""/usr/local/lib/python3.6/site-packages/airflow/jobs.py"", line 835, in create_dag_run next_start = dag.following_schedule(now) File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 3396, in following_schedule tz = pendulum.timezone(self.timezone.name) AttributeError: 'NoneType' object has no attribute 'name' {noformat} We have a timezone setup in the config {quote} eamon [Today at 9:41 AM] {quote} @nicor88 I saw that in the logs also, fwiw it seemed to fail at the first dag run but then subsequent attempts seemed to work. {quote}  nicor88 [3 hours ago] {quote} I tried but the daily jobs are stacked {quote}  eamon [2 hours ago] {quote} so nothing getting scheduled? This is the behaviour I've observed also. When I saw that particular error, it seemed to correct itself so I discounted that as the root cause but could be it. {quote}  eamon [2 hours ago] {quote} maybe try a task without a timezone. {quote}  porn [1 hour ago] {quote} dammit, same problem here ({{1.10.1-rc1}}) {quote}  nicor88 [1 hour ago] {quote} we have timezone setup as utc in the config, and the start_date is setup like: datetime(2018, 7, 23) {quote}  nicor88 [1 hour ago] {quote} we reverted to the stable release 1.10.0 {quote}  porn [1 hour ago] {quote} {{default_timezone = utc}} in config too, the DAG parameters: {noformat} schedule_interval = ""0 14 * * *"", start_date = datetime(2017, 8, 29, hour=1) {noformat} {quote}  porn [1 hour ago] {quote} I need to add that this happened to me only when tried to manually execute (from UI) the task that is periodical, but haven't ran yet. Can you @nicor88 confirm this was your case too? {quote}  nicor88 [1 hour ago] {quote} in our case the daily jobs were not scheduled at all {quote}  nicor88 [1 hour ago] {quote} thats why I got suspiscious and check (edited) {quote}  nicor88 [1 hour ago] {quote} we install Airflow from Github directly, pointing to the stable branch. Now we recovered pointing to a specific release {quote}  porn [1 hour ago] {quote} it is a pre-release actually {quote} nicor88 [1 hour ago] {quote} its my bad that we use the v1-10-stable branch from Github {quote}  nicor88 [1 hour ago] {quote} instead of using the tagged release (edited) {quote}  nicor88 [1 hour ago] {quote} lesson learned {quote}  porn [14 minutes ago] {quote} ok, just downgraded to {{v1.10.1b1}} and got the same error {quote}"
93,AIRFLOW-3351,AIRFLOW,1542278910000,1549373211000,Bug,Open,Blocker,1,13039,"Airflow webserver intermitent broken After completing the airflow 1.10.0 integration with LDAP anonymously (AIRFLOW-3270), we started to hit ""Internal Server Error"" with below exception stack, we tried to clean up the browser cache, it sometimes works and sometimes error our. Please advise the resolution to avoid this issue. {code:java} During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/usr/local/lib/python3.5/site-packages/gunicorn/workers/sync.py"", line 135, in handle self.handle_request(listener, req, client, addr) File ""/usr/local/lib/python3.5/site-packages/gunicorn/workers/sync.py"", line 176, in handle_request respiter = self.wsgi(environ, resp.start_response) File ""/usr/local/lib/python3.5/site-packages/werkzeug/wsgi.py"", line 826, in __call__ return app(environ, start_response) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1997, in __call__ return self.wsgi_app(environ, start_response) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1985, in wsgi_app response = self.handle_exception(e) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1547, in handle_exception return self.finalize_request(handler(e), from_error_handler=True) File ""/usr/local/lib/python3.5/site-packages/airflow/www/views.py"", line 716, in show_traceback info=traceback.format_exc()), 500 File ""/usr/local/lib/python3.5/site-packages/flask/templating.py"", line 132, in render_template ctx.app.update_template_context(context) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 764, in update_template_context context.update(func()) File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 825, in _user_context_processor return dict(current_user=_get_user()) File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 794, in _get_user current_app.login_manager._load_user() File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 363, in _load_user return self.reload_user() File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 325, in reload_user user = self.user_callback(user_id) File ""/usr/local/lib/python3.5/site-packages/airflow/utils/db.py"", line 74, in wrapper return func(*args, **kwargs) File ""/usr/local/lib/python3.5/site-packages/airflow/contrib/auth/backends/ldap_auth.py"", line 278, in load_user return LdapUser(user) File ""<string>"", line 4, in __init__ File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/state.py"", line 414, in _initialize_instance manager.dispatch.init_failure(self, args, kwargs) File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/util/langhelpers.py"", line 66, in __exit__ compat.reraise(exc_type, exc_value, exc_tb) File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/util/compat.py"", line 187, in reraise raise value File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/state.py"", line 411, in _initialize_instance return manager.original_init(*mixed[1:], **kwargs) File ""/usr/local/lib/python3.5/site-packages/airflow/contrib/auth/backends/ldap_auth.py"", line 157, in __init__ user.username) AttributeError: 'NoneType' object has no attribute 'username' 127.0.0.1 - - [15/Nov/2018:10:47:31 +0000] ""GET /admin/ HTTP/1.1"" 500 0 ""-"" ""-"" [2018-11-15 10:47:38,590] ERROR in app: Exception on /favicon.ico [GET] Traceback (most recent call last): File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1612, in full_dispatch_request rv = self.dispatch_request() File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1590, in dispatch_request self.raise_routing_exception(req) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1573, in raise_routing_exception raise request.routing_exception File ""/usr/local/lib/python3.5/site-packages/flask/ctx.py"", line 294, in match_request self.url_adapter.match(return_rule=True) File ""/usr/local/lib/python3.5/site-packages/werkzeug/routing.py"", line 1581, in match raise NotFound() werkzeug.exceptions.NotFound: 404 Not Found: The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again. During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1982, in wsgi_app response = self.full_dispatch_request() File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1614, in full_dispatch_request rv = self.handle_user_exception(e) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1512, in handle_user_exception return self.handle_http_exception(e) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1471, in handle_http_exception return handler(e) File ""/usr/local/lib/python3.5/site-packages/airflow/www/views.py"", line 707, in circles 'airflow/circles.html', hostname=get_hostname()), 404 File ""/usr/local/lib/python3.5/site-packages/flask/templating.py"", line 132, in render_template ctx.app.update_template_context(context) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 764, in update_template_context context.update(func()) File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 825, in _user_context_processor return dict(current_user=_get_user()) File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 794, in _get_user current_app.login_manager._load_user() File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 363, in _load_user return self.reload_user() File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 325, in reload_user user = self.user_callback(user_id) File ""/usr/local/lib/python3.5/site-packages/airflow/utils/db.py"", line 74, in wrapper return func(*args, **kwargs) File ""/usr/local/lib/python3.5/site-packages/airflow/contrib/auth/backends/ldap_auth.py"", line 278, in load_user return LdapUser(user) File ""<string>"", line 4, in __init__ File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/state.py"", line 414, in _initialize_instance manager.dispatch.init_failure(self, args, kwargs) File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/util/langhelpers.py"", line 66, in __exit__ compat.reraise(exc_type, exc_value, exc_tb) File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/util/compat.py"", line 187, in reraise raise value File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/state.py"", line 411, in _initialize_instance return manager.original_init(*mixed[1:], **kwargs) File ""/usr/local/lib/python3.5/site-packages/airflow/contrib/auth/backends/ldap_auth.py"", line 157, in __init__ user.username) AttributeError: 'NoneType' object has no attribute 'username' [2018-11-15 10:47:38 +0000] [78] [ERROR] Error handling request /favicon.ico Traceback (most recent call last): File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1612, in full_dispatch_request rv = self.dispatch_request() File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1590, in dispatch_request self.raise_routing_exception(req) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1573, in raise_routing_exception raise request.routing_exception File ""/usr/local/lib/python3.5/site-packages/flask/ctx.py"", line 294, in match_request self.url_adapter.match(return_rule=True) File ""/usr/local/lib/python3.5/site-packages/werkzeug/routing.py"", line 1581, in match raise NotFound() werkzeug.exceptions.NotFound: 404 Not Found: The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again. During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1982, in wsgi_app response = self.full_dispatch_request() File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1614, in full_dispatch_request rv = self.handle_user_exception(e) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1512, in handle_user_exception return self.handle_http_exception(e) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1471, in handle_http_exception return handler(e) File ""/usr/local/lib/python3.5/site-packages/airflow/www/views.py"", line 707, in circles 'airflow/circles.html', hostname=get_hostname()), 404 File ""/usr/local/lib/python3.5/site-packages/flask/templating.py"", line 132, in render_template ctx.app.update_template_context(context) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 764, in update_template_context context.update(func()) File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 825, in _user_context_processor return dict(current_user=_get_user()) File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 794, in _get_user current_app.login_manager._load_user() File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 363, in _load_user return self.reload_user() File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 325, in reload_user user = self.user_callback(user_id) File ""/usr/local/lib/python3.5/site-packages/airflow/utils/db.py"", line 74, in wrapper return func(*args, **kwargs) File ""/usr/local/lib/python3.5/site-packages/airflow/contrib/auth/backends/ldap_auth.py"", line 278, in load_user return LdapUser(user) File ""<string>"", line 4, in __init__ File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/state.py"", line 414, in _initialize_instance manager.dispatch.init_failure(self, args, kwargs) File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/util/langhelpers.py"", line 66, in __exit__ compat.reraise(exc_type, exc_value, exc_tb) File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/util/compat.py"", line 187, in reraise raise value File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/state.py"", line 411, in _initialize_instance return manager.original_init(*mixed[1:], **kwargs) File ""/usr/local/lib/python3.5/site-packages/airflow/contrib/auth/backends/ldap_auth.py"", line 157, in __init__ user.username) AttributeError: 'NoneType' object has no attribute 'username' During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/usr/local/lib/python3.5/site-packages/gunicorn/workers/sync.py"", line 135, in handle self.handle_request(listener, req, client, addr) File ""/usr/local/lib/python3.5/site-packages/gunicorn/workers/sync.py"", line 176, in handle_request respiter = self.wsgi(environ, resp.start_response) File ""/usr/local/lib/python3.5/site-packages/werkzeug/wsgi.py"", line 826, in __call__ return app(environ, start_response) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1997, in __call__ return self.wsgi_app(environ, start_response) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1985, in wsgi_app response = self.handle_exception(e) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1547, in handle_exception return self.finalize_request(handler(e), from_error_handler=True) File ""/usr/local/lib/python3.5/site-packages/airflow/www/views.py"", line 716, in show_traceback info=traceback.format_exc()), 500 File ""/usr/local/lib/python3.5/site-packages/flask/templating.py"", line 132, in render_template ctx.app.update_template_context(context) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 764, in update_template_context context.update(func()) File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 825, in _user_context_processor return dict(current_user=_get_user()) File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 794, in _get_user current_app.login_manager._load_user() File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 363, in _load_user return self.reload_user() File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 325, in reload_user user = self.user_callback(user_id) File ""/usr/local/lib/python3.5/site-packages/airflow/utils/db.py"", line 74, in wrapper return func(*args, **kwargs) File ""/usr/local/lib/python3.5/site-packages/airflow/contrib/auth/backends/ldap_auth.py"", line 278, in load_user return LdapUser(user) File ""<string>"", line 4, in __init__ File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/state.py"", line 414, in _initialize_instance manager.dispatch.init_failure(self, args, kwargs) File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/util/langhelpers.py"", line 66, in __exit__ compat.reraise(exc_type, exc_value, exc_tb) File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/util/compat.py"", line 187, in reraise raise value File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/state.py"", line 411, in _initialize_instance return manager.original_init(*mixed[1:], **kwargs) File ""/usr/local/lib/python3.5/site-packages/airflow/contrib/auth/backends/ldap_auth.py"", line 157, in __init__ user.username) AttributeError: 'NoneType' object has no attribute 'username' 127.0.0.1 - - [15/Nov/2018:10:47:38 +0000] ""GET /favicon.ico HTTP/1.1"" 500 0 ""-"" ""-"" {code}"
94,AIRFLOW-3372,AIRFLOW,1542716581000,1560247189000,Bug,Closed,Blocker,1,6864,"Unable to start airflow scheduler *I have installed airflow in kubernetes cluster.When i am installing airflow ,i am unable to start the scheduler.The below is the log of scheduler container.* [2018-11-20 12:02:40,860] {{__init__.py:51}} INFO - Using executor SequentialExecutor [2018-11-20 12:02:40,973] {{cli_action_loggers.py:69}} ERROR - Failed on pre-execution callback using <function default_action_log at 0x7f26b730b620> Traceback (most recent call last): File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py"", line 1182, in _execute_context context) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/default.py"", line 470, in do_execute cursor.execute(statement, parameters) sqlite3.OperationalError: no such table: log The above exception was the direct cause of the following exception: Traceback (most recent call last): File ""/usr/local/lib/python3.5/dist-packages/airflow/utils/cli_action_loggers.py"", line 67, in on_pre_execution cb(**kwargs) File ""/usr/local/lib/python3.5/dist-packages/airflow/utils/cli_action_loggers.py"", line 99, in default_action_log session.commit() File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py"", line 927, in commit self.transaction.commit() File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py"", line 467, in commit self._prepare_impl() File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py"", line 447, in _prepare_impl self.session.flush() File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py"", line 2209, in flush self._flush(objects) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py"", line 2329, in _flush transaction.rollback(_capture_exception=True) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/langhelpers.py"", line 66, in __exit__ compat.reraise(exc_type, exc_value, exc_tb) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/compat.py"", line 187, in reraise raise value File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py"", line 2293, in _flush flush_context.execute() File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/unitofwork.py"", line 389, in execute rec.execute(self) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/unitofwork.py"", line 548, in execute uow File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/persistence.py"", line 181, in save_obj mapper, table, insert) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/persistence.py"", line 835, in _emit_insert_statements execute(statement, params) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py"", line 945, in execute return meth(self, multiparams, params) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/elements.py"", line 263, in _execute_on_connection return connection._execute_clauseelement(self, multiparams, params) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py"", line 1053, in _execute_clauseelement compiled_sql, distilled_params File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py"", line 1189, in _execute_context context) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py"", line 1402, in _handle_dbapi_exception exc_info File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/compat.py"", line 203, in raise_from_cause reraise(type(exception), exception, tb=exc_tb, cause=cause) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/compat.py"", line 186, in reraise raise value.with_traceback(tb) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py"", line 1182, in _execute_context context) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/default.py"", line 470, in do_execute cursor.execute(statement, parameters) sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: log [SQL: 'INSERT INTO log (dttm, dag_id, task_id, event, execution_date, owner, extra) VALUES (?, ?, ?, ?, ?, ?, ?)'] [parameters: ('2018-11-20 12:02:40.969353', None, None, 'cli_scheduler', None, 'airflow', ' {""host_name"": ""airflow-airflow-scheduler-5b5f8b9549-89dmn"", ""full_command"": ""[\'/usr/local/bin/airflow\', \'scheduler\', \'-n\', \'-1\', \'-p\']""} ')] ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ _ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ ____/____/|__/ [2018-11-20 12:02:40,977] {{jobs.py:580}} *{color:#FF0000}ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1{color}* /usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py:513: SAWarning: Session's state has been changed on a non-active transaction - this state will be discarded. ""Session's state has been changed on "" Traceback (most recent call last): File ""/usr/local/bin/airflow"", line 32, in <module> args.func(args) File ""/usr/local/lib/python3.5/dist-packages/airflow/utils/cli.py"", line 74, in wrapper return f(*args, **kwargs) File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 925, in scheduler job.run() File ""/usr/local/lib/python3.5/dist-packages/airflow/jobs.py"", line 196, in run session.commit() File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py"", line 927, in commit self.transaction.commit() File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py"", line 465, in commit self._assert_active(prepared_ok=True) File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py"", line 276, in _assert_active % self._rollback_exception sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.OperationalError) no such table: log [SQL: 'INSERT INTO log (dttm, dag_id, task_id, event, execution_date, owner, extra) VALUES (?, ?, ?, ?, ?, ?, ?)'] [parameters: ('2018-11-20 12:02:40.969353', None, None, 'cli_scheduler', None, 'airflow', ' {""host_name"": ""airflow-airflow-scheduler-5b5f8b9549-89dmn"", ""full_command"": ""[\'/usr/local/bin/airflow\', \'scheduler\', \'-n\', \'-1\', \'-p\']""} ')] {color:#FF0000}*The pod status is like below*{color} [root@kubernetes-cpal-master-0 kube-airflow]# *kubectl get pod* NAME READY STATUS RESTARTS AGE airflow-airflow-flower-6668559cf7-ll5bn 1/1 Running 0 19m ~{color:#FF0000}*airflow-airflow-scheduler-5b5f8b9549-89dmn 0/1 CrashLoopBackOff 8 19m*{color}~ airflow-airflow-web-89d8fb554-9ztss 1/1 Running 0 19m airflow-airflow-web-89d8fb554-f4mbm 1/1 Running 0 19m airflow-airflow-worker-0 1/1 Running 0 19m airflow-postgresql-9df799579-swg8q 1/1 Running 0 19m airflow-redis-7d75b85f7-26lsx 1/1 Running 0 19m centos 0/1 CrashLoopBackOff 1278 29d logger-deepinsights-fluentd-759ffcfc5d-fjmml 1/1 Running 8 33d"
95,AIRFLOW-3400,AIRFLOW,1543246034000,1556014262000,Improvement,Closed,Blocker,1,350,"Remove python-nvd3 We are using python-nvd3 for generating charts from python side, and this pulls in slugify which by default uses a GPL'd dependency. This dep chian is the cause of needing to pass {{SLUGIFY_USES_TEXT_UNIDECODE=yes}} or similar, and that is annoying. So we should remove python-nvd3, probably still continue using nvd3.js and d3.js."
96,AIRFLOW-3443,AIRFLOW,1543962067000,1558148777000,Bug,Resolved,Blocker,1,507,"KubernetesPodOperator image_pull_secrets must be a valid parameter We've been successfully using the KubernetesPodOperator in our company with a local Docker registry, but when switching to a private repository such as Amazon ECR, Airflow wasn't able to pull the secrets from the cluster. I have made a change in the *make_pod()* function on *kubernetes_pod_operator.py* to support a new *image_pull_secrets* field. This works great on our end, and the community could benefit from it for the version 10.0.1"
97,AIRFLOW-3445,AIRFLOW,1544005709000,1550208929000,Bug,Closed,Blocker,1,1758,"MariaDB explicit_defaults_for_timestamp = 1 Does not work. {{Running into an issue when running }} {{`airflow upgradedb`}} {{ going from `1.9` -> `1.10.1`}} {{}} {code:java} `sqlalchemy.exc.OperationalError: (_mysql_exceptions.OperationalError) (1193, ""Unknown system variable 'explicit_defaults_for_timestamp'"") [SQL: 'SELECT @@explicit_defaults_for_timestamp']`{code} {{I saw this link on the airflow website.}} {{[https://airflow.readthedocs.io/en/stable/faq.html#how-to-fix-exception-global-variable-explicit-defaults-for-timestamp-needs-to-be-on-1|http://example.com]}} {{Here it says you can set}} {code:java} `explicit_defaults_for_timestamp = 1`{code} {{in the _my.cnf_ file. However I am using Mariadb and when I add this to the _my.cnf_ file the}} {noformat} mariadb.service{noformat} {{fails to start up. Has anyone else come across this issue?}} The output from {code:java} `SHOW VARIABLES like '%version%'`{code} was {code:java} `+-------------------------+----------------------+` `| Variable_name | Value |` `+-------------------------+----------------------+` `| innodb_version | 5.5.59-MariaDB-38.11 |` `| protocol_version | 10 |` `| slave_type_conversions | |` `| version | 5.5.60-MariaDB |` `| version_comment | MariaDB Server |` `| version_compile_machine | x86_64 |` `| version_compile_os | Linux |` `+-------------------------+----------------------+`{code} The MariaDB does not have the argument as it is a MySQL only feature. [https://mariadb.com/kb/en/library/system-variable-differences-between-mariadb-100-and-mysql-56/|http://example.com] There may need to be a check for MariaDB before upgrading, as mentioned by Ash in this Slack thread. [https://apache-airflow.slack.com/archives/CCQB40SQJ/p1543918149008100|http://example.com]"
98,AIRFLOW-3452,AIRFLOW,1544054302000,1547913927000,Improvement,Resolved,Blocker,1,549,"Cannot view dags at /home page I checked out the latest master branch(commit {{[9dce1f0|https://github.com/apache/incubator-airflow/commit/9dce1f0740f69af0ee86709a1a34a002b245aa3e]}}) and restarted my Airflow webserver. But I cannot view any dag at the home page. I inspected the frontend code and found there's a {{style=""display:none;""}} on the \{{main-content}}, and the source code says so at [https://github.com/apache/incubator-airflow/blob/master/airflow/www_rbac/templates/airflow/dags.html#L31] . Is this a known issue? How should I fix it?"
99,AIRFLOW-3630,AIRFLOW,1546594033000,1547066216000,Bug,Resolved,Blocker,1,395,"Google Cloud SQL query Operator cleanup will not close the connection on error When there is an error during Cloud SQL Query, there is a cleanup in post_execute_method. This cleanup is supposed to delete the connection, however it turns out that post_execute is not executed when execute() method throws an error (it was bad assumption). The connection should be closed always in finally clause."
100,AIRFLOW-3684,AIRFLOW,1547233014000,1559153088000,Bug,Open,Blocker,1,594,"Update bundled plugins to work with FAB-based UI We have one plugin in-tree (airflow.contrib.plugins.metastore_browser.main) that has a flask_blueprint - with the deprecation of the old UI in AIRFLOW-3303 this won't work anymore, so we should update that plugin. It might also be worth issuing a warning if a plugin has a {{admin_views}} attribute and no {{appbuilder_views}} (allow both to let a plugin support old and new Airflows.) We possibly need to update the plugin integration in the FAB-app to still support flask_blueprints (To support things that aren't views, such as static files)."
101,AIRFLOW-3706,AIRFLOW,1547485217000,1553069630000,Bug,Resolved,Blocker,1,363,Tooltip Display Issue for a Task Instance I am facing a tooltip display issue in 1.10.2b2 when hovering over a task instance the tooltip rectangle that is being displayed is not automatically resizing - for 1.10.1 it does. I am using the RBAC UI. I tested it with Firefox 64.0.2 (64-bit) and Safari Version 11.1.2 (13605.3.8) on mac OS High Sierra Version 10.13.6
102,AIRFLOW-3899,AIRFLOW,1550263608000,1562254598000,Bug,Open,Blocker,1,905,"Backfill command failing with pickle exception i have a dag which pretty much just prints the contents of the `-conf` param. When i try to execute the *backfill* command for a particular execution date , i am getting the following Exception: .... .... File ""/Users/ellison/.pyenv/versions/2.7.15/lib/python2.7/pickle.py"", line 313, in save (t.__name__, obj)) sqlalchemy.exc.StatementError: (pickle.PicklingError) Can't pickle 'poll' object: <select.poll object at 0x1077eefc0> [SQL: u'INSERT INTO dag_pickle (pickle, created_dttm, pickle_hash) VALUES (%(pickle)s, %(created_dttm)s, %(pickle_hash)s) RETURNING dag_pickle.id'] [parameters: [\{'pickle_hash': 1216885146430908086, 'pickle': <DAG: my_dag_id>}]] The command i am trying is {{airflow backfill my_dag_id --conf '\{""alexis"": ""fire""}' -s 2019-02-14T23:00:00+00:00 -e 2019-02-14T23:00:00+00:00}} Do you guys have any thoughts about what is going on?"
103,AIRFLOW-4089,AIRFLOW,1552563587000,1560940257000,Bug,Resolved,Blocker,1,863,"pytz.utc has no attribute name According to the [docs|https://airflow.readthedocs.io/en/latest/timezone.html#time-zone-aware-dags], even though pendulum is recommended, a DAG's timezone may also be set by using pytz. However, if one uses pytz, models.DAG breaks when trying to use {code:python} self.timezone.name {code} pytz timezones do not have a name attribute This error is not immediately apparent, but it manifests under certain circumstances, such as preventing the scheduler from creating new DAGRuns for catching up from previous dates if the DAG has a schedule_interval of {code:python} 0 0 * * * {code} This can be fixed by either changing the code not to rely on the existence of a *name* attribute on the timezone object, or changing the existing documentation in order to clearly state that pendulum *must* be used and not pytz or python's timezone"
104,AIRFLOW-4176,AIRFLOW,1553776827000,1583092980000,Bug,Open,Blocker,1,672,"[security] webui shows password - admin/log/?flt1_extra_contains=conn_password First setup hivecli connection: {noformat} source /home/ec2-user/venv/bin/activate; airflow connections -a \ --conn_id query_hive --conn_type hive_cli --conn_host domainhere \ --conn_port 10000 --conn_schema default \ --conn_extra ""{\""use_beeline\"":\""true\"", \""ssl-options\"":\""ssl=true;sslTrustStore=path-${RUNTIME_ENV}.jks;trustStorePassword=${QUERY_JKS_PASW}\""}"" \ --conn_login ${QUERY_HIVE_USER} --conn_password ${QUERY_HIVE_PASW} {noformat} On the webui navigate to domain/admin/log/?flt1_extra_contains=conn_password and you will be able to see cleartext user and password! see attachment"
105,AIRFLOW-4297,AIRFLOW,1555071909000,1563791181000,Bug,Resolved,Blocker,1,1695,"Manually triggerd DAG with no schedule_interval breaks scheduler {code:title=example_dag.py} timezone = ""UTC"" local_tz = pendulum.timezone(timezone) start_date = datetime.datetime.strptime('2019-03-28 07:57:00', ""%Y-%m-%d %H:%M:%S"") start_date = start_date.replace(tzinfo=local_tz) default_args = { 'owner': 'DaniRC' } dag = DAG('testsla', default_args=default_args, start_date=start_date, concurrency=1, max_active_runs=1, default_view='tree', orientation='TB', catchup=False, schedule_interval=None ) {code} If this DAG is triggered, then this error occurs: {noformat} Process DagFileProcessor5303-Process: Traceback (most recent call last): File ""/usr/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap self.run() File ""/usr/lib/python3.6/multiprocessing/process.py"", line 93, in run self._target(*self._args, **self._kwargs) File ""/usr/lib/python3.6/site-packages/airflow/jobs.py"", line 402, in helper pickle_dags) File ""/usr/lib/python3.6/site-packages/airflow/utils/db.py"", line 73, in wrapper return func(*args, **kwargs) File ""/usr/lib/python3.6/site-packages/airflow/jobs.py"", line 1760, in process_file self._process_dags(dagbag, dags, ti_keys_to_schedule) File ""/usr/lib/python3.6/site-packages/airflow/jobs.py"", line 1452, in _process_dags self.manage_slas(dag) File ""/usr/lib/python3.6/site-packages/airflow/utils/db.py"", line 73, in wrapper return func(*args, **kwargs) File ""/usr/lib/python3.6/site-packages/airflow/jobs.py"", line 662, in manage_slas while dttm < timezone.utcnow(): TypeError: '<' not supported between instances of 'NoneType' and 'datetime.datetime' {noformat} After this happens the scheduler also won't schedule any further tasks for _ANY_ dag."
106,AIRFLOW-4298,AIRFLOW,1555086534000,1561458469000,Improvement,Resolved,Blocker,1,13249,"Stop Scheduler warning repeatedly about ""connection invalidated"" I have some strange issue with scheduler after upgrade to 1.10.3 from 1.10.2. DAG tasks runs only once. After that scheduler logs looks like this: {noformat} Apr 11 09:21:33 airflow.infra airflow[32739]: [2019-04-11 09:21:33,094] {{sqlalchemy.py:81}} WARNING - DB connection invalidated. Reconnecting... Apr 11 09:21:44 airflow.infra airflow[32739]: [2019-04-11 09:21:44,105] {{sqlalchemy.py:81}} WARNING - DB connection invalidated. Reconnecting... Apr 11 09:21:55 airflow.infra airflow[32739]: [2019-04-11 09:21:55,114] {{sqlalchemy.py:81}} WARNING - DB connection invalidated. Reconnecting... Apr 11 09:22:06 airflow.infra airflow[32739]: [2019-04-11 09:22:06,123] {{sqlalchemy.py:81}} WARNING - DB connection invalidated. Reconnecting... Apr 11 09:22:17 airflow.infra airflow[32739]: [2019-04-11 09:22:17,131] {{sqlalchemy.py:81}} WARNING - DB connection invalidated. Reconnecting... Apr 11 09:22:28 airflow.infra airflow[32739]: [2019-04-11 09:22:28,143] {{sqlalchemy.py:81}} WARNING - DB connection invalidated. Reconnecting... {noformat} Logs from Scheduler with *DEBUG* level: {noformat} Apr 11 09:00:47 airflow.infra airflow[17403]: [2019-04-11 09:00:47,720] {{settings.py:154}} DEBUG - Setting up DB connection pool (PID 17447) Apr 11 09:00:47 airflow.infra airflow[17403]: [2019-04-11 09:00:47,720] {{settings.py:182}} INFO - settings.configure_orm(): Using pool settings. pool_size=100, pool_recycle=3600, pid=17447 Apr 11 09:00:48 airflow.infra airflow[17403]: [2019-04-11 09:00:48,450] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 17449) Apr 11 09:00:48 airflow.infra airflow[17403]: [2019-04-11 09:00:48,535] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 17448) Apr 11 09:00:48 airflow.infra airflow[17403]: [2019-04-11 09:00:48,706] {{jobs.py:1663}} DEBUG - Sleeping for 1.00 seconds to prevent excessive logging . . . Apr 11 09:01:29 airflow.infra airflow[17403]: [2019-04-11 09:01:29,884] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 21866) Apr 11 09:01:30 airflow.infra airflow[17403]: [2019-04-11 09:01:30,492] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 21865) Apr 11 09:01:30 airflow.infra airflow[17403]: [2019-04-11 09:01:30,785] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor73-Process, stopped)> Apr 11 09:01:30 airflow.infra airflow[17403]: [2019-04-11 09:01:30,786] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor74-Process, stopped)> Apr 11 09:01:30 airflow.infra airflow[17403]: [2019-04-11 09:01:30,790] {{sqlalchemy.py:81}} WARNING - DB connection invalidated. Reconnecting... Apr 11 09:01:31 airflow.infra airflow[17403]: [2019-04-11 09:01:31,765] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 21910) Apr 11 09:01:31 airflow.infra airflow[17403]: [2019-04-11 09:01:31,786] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor76-Process, stopped)> Apr 11 09:01:31 airflow.infra airflow[17403]: [2019-04-11 09:01:31,866] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 21909) Apr 11 09:01:32 airflow.infra airflow[17403]: [2019-04-11 09:01:32,468] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 21921) Apr 11 09:01:32 airflow.infra airflow[17403]: [2019-04-11 09:01:32,787] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor75-Process, stopped)> Apr 11 09:01:32 airflow.infra airflow[17403]: [2019-04-11 09:01:32,787] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor77-Process, stopped)> Apr 11 09:01:33 airflow.infra airflow[17403]: [2019-04-11 09:01:33,358] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 21933) Apr 11 09:01:33 airflow.infra airflow[17403]: [2019-04-11 09:01:33,521] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 21934) Apr 11 09:01:33 airflow.infra airflow[17403]: [2019-04-11 09:01:33,788] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor78-Process, stopped)> Apr 11 09:01:33 airflow.infra airflow[17403]: [2019-04-11 09:01:33,789] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor79-Process, stopped)> Apr 11 09:01:34 airflow.infra airflow[17403]: [2019-04-11 09:01:34,456] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 21949) Apr 11 09:01:34 airflow.infra airflow[17403]: [2019-04-11 09:01:34,679] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 21950) Apr 11 09:01:34 airflow.infra airflow[17403]: [2019-04-11 09:01:34,789] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor80-Process, stopped)> Apr 11 09:01:34 airflow.infra airflow[17403]: [2019-04-11 09:01:34,791] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor81-Process, stopped)> Apr 11 09:01:34 airflow.infra airflow[17403]: [2019-04-11 09:01:34,892] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 21964) Apr 11 09:01:35 airflow.infra airflow[17403]: [2019-04-11 09:01:35,405] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 21963) Apr 11 09:01:35 airflow.infra airflow[17403]: [2019-04-11 09:01:35,790] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor82-Process, stopped)> Apr 11 09:01:35 airflow.infra airflow[17403]: [2019-04-11 09:01:35,791] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor83-Process, stopped)> Apr 11 09:01:35 airflow.infra airflow[17403]: [2019-04-11 09:01:35,834] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 21994) Apr 11 09:01:36 airflow.infra airflow[17403]: [2019-04-11 09:01:36,462] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 21993) Apr 11 09:01:36 airflow.infra airflow[17403]: [2019-04-11 09:01:36,792] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor84-Process, stopped)> Apr 11 09:01:36 airflow.infra airflow[17403]: [2019-04-11 09:01:36,793] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor85-Process, stopped)> Apr 11 09:01:37 airflow.infra airflow[17403]: [2019-04-11 09:01:37,509] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 22003) Apr 11 09:01:37 airflow.infra airflow[17403]: [2019-04-11 09:01:37,672] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 22004) Apr 11 09:01:37 airflow.infra airflow[17403]: [2019-04-11 09:01:37,793] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor86-Process, stopped)> Apr 11 09:01:37 airflow.infra airflow[17403]: [2019-04-11 09:01:37,794] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor87-Process, stopped)> Apr 11 09:01:38 airflow.infra airflow[17403]: [2019-04-11 09:01:38,472] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 22010) Apr 11 09:01:38 airflow.infra airflow[17403]: [2019-04-11 09:01:38,795] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor88-Process, stopped)> Apr 11 09:01:39 airflow.infra airflow[17403]: [2019-04-11 09:01:39,522] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 22054) Apr 11 09:01:39 airflow.infra airflow[17403]: [2019-04-11 09:01:39,588] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 22053) Apr 11 09:01:39 airflow.infra airflow[17403]: [2019-04-11 09:01:39,796] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor89-Process, stopped)> Apr 11 09:01:39 airflow.infra airflow[17403]: [2019-04-11 09:01:39,796] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor90-Process, stopped)> Apr 11 09:01:39 airflow.infra airflow[17403]: [2019-04-11 09:01:39,888] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 22060) Apr 11 09:01:40 airflow.infra airflow[17403]: [2019-04-11 09:01:40,460] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 22061) Apr 11 09:01:40 airflow.infra airflow[17403]: [2019-04-11 09:01:40,798] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor91-Process, stopped)> Apr 11 09:01:40 airflow.infra airflow[17403]: [2019-04-11 09:01:40,799] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor92-Process, stopped)> Apr 11 09:01:41 airflow.infra airflow[17403]: [2019-04-11 09:01:41,459] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 22088) Apr 11 09:01:41 airflow.infra airflow[17403]: [2019-04-11 09:01:41,478] {{settings.py:206}} DEBUG - Disposing DB connection pool (PID 22089) Apr 11 09:01:41 airflow.infra airflow[17403]: [2019-04-11 09:01:41,799] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor93-Process, stopped)> Apr 11 09:01:41 airflow.infra airflow[17403]: [2019-04-11 09:01:41,799] {{jobs.py:496}} DEBUG - Waiting for <Process(DagFileProcessor94-Process, stopped)> Apr 11 09:01:41 airflow.infra airflow[17403]: [2019-04-11 09:01:41,804] {{sqlalchemy.py:81}} WARNING - DB connection invalidated. Reconnecting... {noformat} Webserver and Worker works fine. If I ran task by hands - it works. I'm using postgresql in docker as Metadata DB. I'm using CeleryExecutor and RabbitMQ as broker. Related parts of airflow.cfg: {noformat} # The executor class that airflow should use. Choices include # SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor executor = CeleryExecutor # The SqlAlchemy connection string to the metadata database. # SqlAlchemy supports many different database engine, more information # their website sql_alchemy_conn = postgresql+psycopg2://{{airflow_db_user}}:{{airflow_db_pass}}@{{airflow_db_host}}:12411/airflow # If SqlAlchemy should pool database connections. sql_alchemy_pool_enabled = True # The SqlAlchemy pool size is the maximum number of database connections # in the pool. 0 indicates no limit. sql_alchemy_pool_size = 100 # The SqlAlchemy pool recycle is the number of seconds a connection # can be idle in the pool before it is invalidated. This config does # not apply to sqlite. If the number of DB connections is ever exceeded, # a lower config value will allow the system to recover faster. sql_alchemy_pool_recycle = 3600 # How many seconds to retry re-establishing a DB connection after # disconnects. Setting this to 0 disables retries. sql_alchemy_reconnect_timeout = 300 # The amount of parallelism as a setting to the executor. This defines # the max number of task instances that should run simultaneously # on this airflow installation parallelism = 116 # The number of task instances allowed to run concurrently by the scheduler dag_concurrency = 16 # Are DAGs paused by default at creation dags_are_paused_at_creation = True # When not using pools, tasks are run in the ""default pool"", # whose size is guided by this config element non_pooled_task_slot_count = 128 # The maximum number of active DAG runs per DAG max_active_runs_per_dag = 4 {noformat} {noformat} [scheduler] # Task instances listen for external kill signal (when you clear tasks # from the CLI or the UI), this defines the frequency at which they should # listen (in seconds). job_heartbeat_sec = 5 # The scheduler constantly tries to trigger new tasks (look at the # scheduler section in the docs for more information). This defines # how often the scheduler should run (in seconds). scheduler_heartbeat_sec = 5 # after how much time should the scheduler terminate in seconds # -1 indicates to run continuously (see also num_runs) run_duration = -1 # after how much time (seconds) a new DAGs should be picked up from the filesystem min_file_process_interval = 0 # How often (in seconds) to scan the DAGs directory for new files. Default to 5 minutes. dag_dir_list_interval = 300 # How often should stats be printed to the logs print_stats_interval = 30 child_process_log_directory = {{ airflow_child_scheduler_logs_folder }} # Local task jobs periodically heartbeat to the DB. If the job has # not heartbeat in this many seconds, the scheduler will mark the # associated task instance as failed and will re-schedule the task. scheduler_zombie_task_threshold = 300 # Turn off scheduler catchup by setting this to False. # Default behavior is unchanged and # Command Line Backfills still work, but the scheduler # will not do scheduler catchup if this is False, # however it can be set on a per DAG basis in the # DAG definition (catchup) catchup_by_default = True # This changes the batch size of queries in the scheduling main loop. # If this is too high, SQL query performance may be impacted by one # or more of the following: # - reversion to full table scan # - complexity of query predicate # - excessive locking # # Additionally, you may hit the maximum allowable query length for your db. # # Set this to 0 for no limit (not advised) max_tis_per_query = 0 # Statsd (https://github.com/etsy/statsd) integration settings statsd_on = True statsd_host = localhost statsd_port = 8125 statsd_prefix = airflow # The scheduler can run multiple threads in parallel to schedule dags. # This defines how many threads will run. max_threads = 2 authenticate = False {noformat} DB configuration: {noformat} -c max_connections=1000 -c shared_buffers=1GB -c effective_cache_size=3GB -c maintenance_work_mem=256MB -c checkpoint_completion_target=0.7 -c wal_buffers=16MB -c default_statistics_target=100 -c random_page_cost=1.1 -c max_wal_size=2GB -c work_mem=104857kB -c min_wal_size=1GB -c max_wal_senders=5 -c max_worker_processes=2 -c max_parallel_workers_per_gather=1 -c max_parallel_workers=2 {noformat}"
107,AIRFLOW-4346,AIRFLOW,1555543831000,1573876205000,Bug,Open,Blocker,1,549,"Kubernetes Executor Fails for Large Wide DAGs When running large DAGsthose with parallelism of over 100 task instances to be running concurrently--several tasks fail on the executor and are reported to the database, but the scheduler is never aware of them failing. Attached are: - A test DAG that we can use to replicate the issue. - The configmap-airflow.yaml file I will be available to answer any other questions that are raised about our configuration. We are running this on GKE and giving the scheduler and web pod a base 100m for execution."
108,AIRFLOW-4355,AIRFLOW,1555590361000,1608899372000,Bug,In Progress,Blocker,1,18336,"Externally triggered DAG is marked as 'success' even if a task has been 'removed'! note: all my dags are purely externally triggered *Issue:* Dag has 5 parallel tasks that ran successfully and 1 final task that somehow got 'removed' state (prior dag runs had 'failed' state) and never ran successfully but still the DAG is showing success! *Command ran* (note that previous commands like airflow trigger_dag -e 20190412 qsr_coremytbl were run before and failed for valid reason (ie python task failing) ): airflow trigger_dag -e 20190412T08:00 qsr_coremytbl --conf '\{""hourstr"":""08""}' *some logs on prior instance of airflow (ec2 was autohealed):* [2019-04-18 08:29:40,678] \{logging_mixin.py:95} INFO - [2019-04-18 08:29:40,678] {__init__.py:4897} WARNING - Failed to get task '<TaskInstance: qsr_coremytbl.REPAIR_HIVE_schemeh.mytbl 2019-04-12 08:00:00+00:00 [None]>' for dag '<DAG: qsr_coremytbl>'. Marking it as removed. [2019-04-18 08:29:43,582] \{logging_mixin.py:95} INFO - [2019-04-18 08:29:43,582] {__init__.py:4906} INFO - Restoring task '<TaskInstance: qsr_coremytbl.REPAIR_HIVE_schemeh.mytbl 2019-04-12 08:00:00+00:00 [removed]>' which was previously removed from DAG '<DAG: qsr_coremytbl>' [2019-04-18 08:29:43,618] \{jobs.py:1787} INFO - Creating / updating <TaskInstance: qsr_coremytbl.REPAIR_HIVE_schemeh.mytbl 2019-04-12 08:00:00+00:00 [scheduled]> in ORM [2019-04-18 08:29:43,676] \{logging_mixin.py:95} INFO - [2019-04-18 08:29:43,676] {__init__.py:4897} WARNING - Failed to get task '<TaskInstance: qsr_coremytbl.REPAIR_HIVE_schemeh.mytbl 2019-04-12 08:00:00+00:00 [scheduled]>' for dag '<DAG: qsr_coremytbl>'. Marking it as removed. *some logs on newer ec2:* [myuser@host logs]$ grep -i hive -R * | sed 's#[0-9]#x#g' | sort | uniq -c | grep -v 'airflow-webserver-access.log' 2 audit/airflow-audit.log:xxxx-xx-xx xx:xx:xx.xxxxxx xxxx qsr_coremytbl REPAIR_HIVE_schemeh.mytbl log xxxx-xx-xx xx:xx:xx.xxxxxx rsawyerx [('execution_date', u'xxxx-xx-xxTxx:xx:xx+xx:xx'), ('task_id', u'REPAIR_HIVE_schemeh.mytbl'), ('dag_id', u'qsr_coremytbl')] 1 audit/airflow-audit.log:xxxx-xx-xx xx:xx:xx.xxxxxx xxxx qsr_coremytbl REPAIR_HIVE_schemeh.mytbl log xxxx-xx-xx xx:xx:xx.xxxxxx rsawyerx [('execution_date', u'xxxx-xx-xxTxx:xx:xx+xx:xx'), ('task_id', u'REPAIR_HIVE_schemeh.mytbl'), ('dag_id', u'qsr_coremytbl'), ('format', u'json')] 1 audit/airflow-audit.log:xxxx-xx-xx xx:xx:xx.xxxxxx xxxx qsr_coremytbl REPAIR_HIVE_schemeh.mytbl rendered xxxx-xx-xx xx:xx:xx.xxxxxx rsawyerx [('execution_date', u'xxxx-xx-xxTxx:xx:xx+xx:xx'), ('task_id', u'REPAIR_HIVE_schemeh.mytbl'), ('dag_id', u'qsr_coremytbl')] 1 audit/airflow-audit.log:xxxx-xx-xx xx:xx:xx.xxxxxx xxxx qsr_coremytbl REPAIR_HIVE_schemeh.mytbl task xxxx-xx-xx xx:xx:xx.xxxxxx rsawyerx [('execution_date', u'xxxx-xx-xxTxx:xx:xx+xx:xx'), ('task_id', u'REPAIR_HIVE_schemeh.mytbl'), ('dag_id', u'qsr_coremytbl')] 1 scheduler/latest/qsr_dag_generation.py.log:[xxxx-xx-xx xx:xx:xx,xxx] \{jobs.py:xxxx} INFO - Creating / updating <TaskInstance: qsr_coremytbl.REPAIR_HIVE_schemeh.mytbl xxxx-xx-xx xx:xx:xx+xx:xx [scheduled]> in ORM 71 scheduler/latest/qsr_dag_generation.py.log:[xxxx-xx-xx xx:xx:xx,xxx] \{logging_mixin.py:xx} INFO - [xxxx-xx-xx xx:xx:xx,xxx] {__init__.py:xxxx} INFO - Restoring task '<TaskInstance: qsr_coremytbl.REPAIR_HIVE_schemeh.mytbl xxxx-xx-xx xx:xx:xx+xx:xx [removed]>' which was previously removed from DAG '<DAG: qsr_coremytbl>' 1 scheduler/xxxx-xx-xx/qsr_dag_generation.py.log:[xxxx-xx-xx xx:xx:xx,xxx] \{jobs.py:xxxx} INFO - Creating / updating <TaskInstance: qsr_coremytbl.REPAIR_HIVE_schemeh.mytbl xxxx-xx-xx xx:xx:xx+xx:xx [scheduled]> in ORM 71 scheduler/xxxx-xx-xx/qsr_dag_generation.py.log:[xxxx-xx-xx xx:xx:xx,xxx] \{logging_mixin.py:xx} INFO - [xxxx-xx-xx xx:xx:xx,xxx] {__init__.py:xxxx} INFO - Restoring task '<TaskInstance: qsr_coremytbl.REPAIR_HIVE_schemeh.mytbl xxxx-xx-xx xx:xx:xx+xx:xx [removed]>' which was previously removed from DAG '<DAG: qsr_coremytbl>' mysql> *select * from task_instance where task_id like '%REP%';#* +------------------------------+-----------------+++--------------------------------------------------------++--------------------------------------++-----------------------------++------------------------------------------------------++--------------++--------------------------++--------------------------------------------++------------------+----------------- |task_id|dag_id|execution_date|start_date|end_date|duration|state|try_number|hostname|unixname|job_id|pool|queue|priority_weight|operator|queued_dttm|pid|max_tries|executor_config| +------------------------------+-----------------+++--------------------------------------------------------++--------------------------------------++-----------------------------++------------------------------------------------------++--------------++--------------------------++--------------------------------------------++------------------+----------------- |REPAIR_HIVE_mytbl|qsr_coremytbl|2019-04-13 00:00:00.000000|2019-04-17 20:29:19.639059|2019-04-17 20:29:33.700252|14.0612|failed|1|ip-ec2|myuser|305|NULL|default|1|PythonOperator|2019-04-17 20:29:13.604354|899|0|}q .| |REPAIR_HIVE_schemeh.mytbl|qsr_coremytbl|2019-01-01 00:00:00.000000|2019-04-17 21:30:11.439127|2019-04-17 21:30:11.439142|NULL|upstream_failed|0| |myuser|NULL|NULL|default|1|NULL|NULL|NULL|0|}q .| |REPAIR_HIVE_schemeh.mytbl|qsr_coremytbl|2019-01-02 00:00:00.000000|2019-04-17 21:46:34.163607|2019-04-17 21:46:34.163627|NULL|upstream_failed|0| |myuser|NULL|NULL|default|1|NULL|NULL|NULL|0|}q .| |REPAIR_HIVE_schemeh.mytbl|qsr_coremytbl|2019-01-03 00:00:00.000000|2019-04-17 21:50:48.541224|2019-04-17 21:50:48.541239|NULL|upstream_failed|0| |myuser|NULL|NULL|default|1|NULL|NULL|NULL|0|}q .| |REPAIR_HIVE_schemeh.mytbl|qsr_coremytbl|2019-01-03 05:00:00.000000|2019-04-17 22:00:24.286685|2019-04-17 22:00:24.286709|NULL|upstream_failed|0| |myuser|NULL|NULL|default|1|NULL|NULL|NULL|0|}q .| |REPAIR_HIVE_schemeh.mytbl|qsr_coremytbl|2019-04-11 00:00:00.000000|2019-04-17 21:26:08.621737|2019-04-17 21:26:22.686882|14.0651|failed|1|ip-ec2|myuser|316|NULL|default|1|PythonOperator|2019-04-17 21:26:03.083885|29638|0|}q .| |REPAIR_HIVE_schemeh.mytbl|qsr_coremytbl|2019-04-11 17:00:00.000000|2019-04-17 22:44:52.900860|2019-04-17 22:45:04.403179|11.5023|failed|1|ip-ec2|myuser|348|NULL|default|1|PythonOperator|2019-04-17 22:44:47.895363|10815|0|}q .| |REPAIR_HIVE_schemeh.mytbl|qsr_coremytbl|2019-04-11 22:00:00.000000|2019-04-17 22:37:28.409799|2019-04-17 22:37:41.448494|13.0387|failed|1|ip-ec2|myuser|342|NULL|default|1|PythonOperator|2019-04-17 22:37:23.449554|28697|0|}q .| |REPAIR_HIVE_schemeh.mytbl|qsr_coremytbl|2019-04-12 00:00:00.000000|2019-04-17 21:02:36.365150|2019-04-17 21:02:36.365165|NULL|upstream_failed|0| |myuser|NULL|NULL|default|1|NULL|NULL|NULL|0|}q .| |REPAIR_HIVE_schemeh.mytbl|qsr_coremytbl|2019-04-12 07:00:00.000000|2019-04-18 08:24:59.552695|2019-04-18 08:24:59.552710|NULL|removed|0| |myuser|NULL|NULL|default|1|NULL|NULL|NULL|0|}q .| |REPAIR_HIVE_schemeh.mytbl|qsr_coremytbl|2019-04-12 08:00:00.000000|NULL|NULL|NULL|removed|0| |myuser|NULL|NULL|default|1|NULL|NULL|NULL|0|}q .| +------------------------------+-----------------+++--------------------------------------------------------++--------------------------------------++-----------------------------++------------------------------------------------------++--------------++--------------------------++--------------------------------------------++------------------+----------------- 11 rows in set (0.00 sec) *Task Instance Details* Dependencies Blocking Task From Getting Scheduled Dependency Reason Task Instance State Task is in the 'removed' state which is not a valid state for execution. The task must be cleared in order to be run. Dagrun Running Task instance's dagrun was not in the 'running' state but in the state 'success'. Attribute: python_callable def repair_hive_table(hive_cli_conn_id, schema, table, drop_partitions_first=False): conn = BaseHook.get_connection(hive_cli_conn_id) ssl_options = conn.extra_dejson.get('ssl-options') jdbc_url = ""jdbc:hive2://\{conn.host}:\{conn.port}/\{conn.schema};\{ssl_options}"".format(**locals()) sqlstmt = 'MSCK REPAIR TABLE %s.%s;' % (schema, table) hive_command = ' '.join([ '/home/myuser/spark_home/bin/beeline', '-u', '""%s""' % jdbc_url, '-n', conn.login, '-w', '/home/myuser/spark_home/hivepw', '-e', '""%s""' % sqlstmt ]) # note - do not log the command which contains truststore and hive user passwords logging.info(""Executing following SQL statement: %s"" % sqlstmt) process = subprocess.Popen(hive_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True) (output, error) = process.communicate() logging.info(output) if process.returncode != 0: logging.error('Hive process returned code %d' % process.returncode) raise Exception('Hive process returned code %d' % process.returncode) Task Instance Attributes Attribute Value dag_id qsr_coremytbl duration None end_date None execution_date 2019-04-12T08:00:00+00:00 executor_config {} generate_command <function generate_command at 0x7f6526f97ed8> hostname is_premature False job_id None key ('qsr_coremytbl', u'REPAIR_HIVE_schemeh.mytbl', <Pendulum [2019-04-12T08:00:00+00:00]>, 1) log <logging.Logger object at 0x7f65267a2d50> log_filepath /home/myuser/airflow/logs/qsr_coremytbl/REPAIR_HIVE_schemeh.mytbl/2019-04-12T08:00:00+00:00.log log_url [https://domain/admin/airflow/log?dag_id=qsr_coremytbl&task_id=REPAIR_HIVE_schemeh.mytbl&execution_date=2019-04-12T08%3A00%3A00%2B00%3A00] logger <logging.Logger object at 0x7f65267a2d50> mark_success_url [https://domain/admin/airflow/success?task_id=REPAIR_HIVE_schemeh.mytbl&dag_id=qsr_coremytbl&execution_date=2019-04-12T08%3A00%3A00%2B00%3A00&upstream=false&downstream=false] max_tries 0 metadata MetaData(bind=None) next_try_number 1 operator None pid None pool None previous_ti None priority_weight 1 queue default queued_dttm None raw False run_as_user None start_date None state removed task <Task(PythonOperator): REPAIR_HIVE_schemeh.mytbl> task_id REPAIR_HIVE_schemeh.mytbl test_mode False try_number 1 unixname myuser Task Attributes Attribute Value dag <DAG: qsr_coremytbl> dag_id qsr_coremytbl depends_on_past False deps set([<TIDep(Not In Retry Period)>, <TIDep(Trigger Rule)>, <TIDep(Previous Dagrun State)>]) downstream_list [] downstream_task_ids set([]) email None email_on_failure True email_on_retry True end_date None execution_timeout None executor_config {} inlets [] lineage_data None log <logging.Logger object at 0x7f65167edad0> logger <logging.Logger object at 0x7f65167edad0> max_retry_delay None on_failure_callback None on_retry_callback None on_success_callback None op_args [] op_kwargs \{'table': u'mytbl', 'hive_cli_conn_id': 'query_hive', 'schema': u'schemeh'} outlets [] owner airflow params {} pool None priority_weight 1 priority_weight_total 1 provide_context False queue default resources {'disk': {'_qty': 512, '_units_str': 'MB', '_name': 'Disk'} , 'gpus': \{'_qty': 0, '_units_str': 'gpu(s)', '_name': 'GPU'}, 'ram': \{'_qty': 512, '_units_str': 'MB', '_name': 'RAM'}, 'cpus': \{'_qty': 1, '_units_str': 'core(s)', '_name': 'CPU'}} retries 0 retry_delay 0:05:00 retry_exponential_backoff False run_as_user None schedule_interval None shallow_copy_attrs ('python_callable', 'op_kwargs') sla None start_date 2017-06-01T00:00:00+00:00 task_concurrency None task_id REPAIR_HIVE_schemeh.mytbl task_type PythonOperator template_ext [] template_fields ('templates_dict', 'op_args', 'op_kwargs') templates_dict None trigger_rule all_success ui_color #ffefeb ui_fgcolor #000 upstream_list [<Task(SparkSubmitOperator): coremytbl_FR__H>, <Task(SparkSubmitOperator): coremytbl_BE__H>, <Task(SparkSubmitOperator): coremytbl_NL__H>, <Task(SparkSubmitOperator): coremytbl_DE__H>, <Task(SparkSubmitOperator): coremytbl_DAGNAME__H>] upstream_task_ids set([u'coremytbl_FR__H', u'coremytbl_BE__H', u'coremytbl_NL__H', u'coremytbl_DE__H', u'coremytbl_DAGNAME__H']) wait_for_downstream False weight_rule downstream DAG code: {noformat} import datetime as dt import glob import json import logging import os import subprocess #import urllib from airflow import DAG from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator from airflow.operators.python_operator import PythonOperator from airflow.hooks.base_hook import BaseHook # Globals DATALAKE_S3ROOT = subprocess.check_output("". /etc/pipeline/profile; dig -t TXT +short qsrs3.$pipelineBranch.sss.$networkDomainName | tail -1 | tr -d '\""' | sed -e s#^s3://##"", shell=True).strip() default_args = { 'owner': 'airflow', 'start_date': dt.datetime(2017, 6, 1), 'retries': 0, 'retry_delay': dt.timedelta(minutes=5), } def repair_hive_table(hive_cli_conn_id, schema, table, drop_partitions_first=False): conn = BaseHook.get_connection(hive_cli_conn_id) ssl_options = conn.extra_dejson.get('ssl-options') jdbc_url = ""jdbc:hive2://{conn.host}:{conn.port}/{conn.schema};{ssl_options}"".format(**locals()) sqlstmt = 'MSCK REPAIR TABLE %s.%s;' % (schema, table) hive_command = ' '.join([ '/home/myuser/spark_home/bin/beeline', '-u', '""%s""' % jdbc_url, '-n', conn.login, '-w', '/home/myuser/spark_home/hivepw', '-e', '""%s""' % sqlstmt ]) # note - do not log the command which contains truststore and hive user passwords logging.info(""Executing following SQL statement: %s"" % sqlstmt) process = subprocess.Popen(hive_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True) (output, error) = process.communicate() logging.info(output) if process.returncode != 0: logging.error('Hive process returned code %d' % process.returncode) raise Exception('Hive process returned code %d' % process.returncode) def create_spark_submit_operator(dag, pool, runstream, iz_strip_name, table, filename): task_id = ""%s_%s"" % (runstream, iz_strip_name) #need to handle validation and import filename = iz_strip_name + '_input.yaml' file_path = os.path.join('/home/myuser/jsonconfigs/si/job', runstream, 'jar/input', filename) return SparkSubmitOperator( dag = dag, # need to tell airflow that this task belongs to the dag we defined above task_id = task_id, pool = pool, #params={""lob"": lob}, # the sql file above have a template in it for a 'lob' paramater - this is how we pass it in #bash_command='echo ""Data Load task 1 {{ params.lob }} here""' conn_id='process', java_class='com.cimp.jar.jar', application='s3a://%s/jar.jar' % DATALAKE_S3ROOT, #total_executor_cores='16', executor_cores='2', executor_memory='8g', driver_memory='2g', conf={""spark.sql.parquet.writeLegacyFormat"" : ""true"" ,""spark.executor.extraJavaOptions"" : ""-Dinput_wildcard={{ ds_nodash }}{{ '*'+dag_run.conf['hourstr'] if dag_run.conf['hourstr'] else '' }} -Ddatestr={{ ds_nodash }} -Dhourstr={{ dag_run.conf['hourstr'] if dag_run.conf['hourstr'] else '' }}"" ,""spark.driver.extraJavaOptions"" : ""-Dinput_wildcard={{ ds_nodash }}{{ '*'+dag_run.conf['hourstr'] if dag_run.conf['hourstr'] else '' }} -Ddatestr={{ ds_nodash }} -Dhourstr={{ dag_run.conf['hourstr'] if dag_run.conf['hourstr'] else '' }}"" }, #num_executors='2', name=""%s_{{ ds_nodash }}"" % task_id, verbose=True, #dont know how will work (where do the prior exports get made) or if commandline way can be used #env_vars={""input_wildcard"": os.environ['input_wildcard'],""datestr"": os.environ['datestr'],""hourstr"": os.environ['hourstr']}, application_args=[ ""--config"", ""%s"" % file_path ], ) def create_load_dag(dag_id, runstream): dag = DAG(dag_id, # give the dag a name schedule_interval=None, # define how often you want it to run - you can pass cron expressions here default_args=default_args # pass the default args defined above or you can override them here if you want this dag to behave a little different ) file_list = os.path.join('/home/myuser/jsonconfigs/si/job', runstream, 'file_list.json') process = subprocess.Popen([""cat"", file_list], stdout=subprocess.PIPE) decoded_data = json.load(process.stdout) table_repair_dependencies = {} # (schema, table) -> [load tasks] for file in decoded_data['files']: srcfile = file['name'] iz_strip_name = file['iz_strip_name'] schema = file['Schema'] hive_table = file['hive_table'] # One staging task stagingTask = create_spark_submit_operator(dag, None, runstream, iz_strip_name, srcfile, ""QSRLOAD"") stagingTask.doc_md = """"""\ QSR jar LOAD for %s to %s.%s """""" % (srcfile,schema,hive_table) try : table_repair_dependencies[(schema, hive_table)].append(stagingTask) except KeyError: table_repair_dependencies[(schema, hive_table)] = [stagingTask] # table repair tasks for (schema, object_name) in table_repair_dependencies.keys() : repairHiveTask = PythonOperator(dag = dag, task_id=""REPAIR_HIVE_%s.%s"" % (schema,object_name), python_callable=repair_hive_table, op_kwargs={'hive_cli_conn_id' : 'query_hive', 'schema' : schema, 'table' : object_name}) repairHiveTask << table_repair_dependencies[(schema, object_name)] return dag # dynamically generate all dags for entry in os.listdir('/home/myuser/jsonconfigs/si/job'): file_list = glob.glob(os.path.join('/home/myuser/jsonconfigs/si/job', entry, 'file_list.json')) if file_list : runstream = entry dag_id = 'qsr_%s' % runstream globals()[dag_id] = create_load_dag(dag_id, runstream) {noformat} *after encountering this issue i ran*: airflow clear mycooldag below was the output....as you can see the REPAIR_HIVE task was never successful so I don't know how the dag can be overall 'success' state!!! <TaskInstance: mycooldag.REPAIR_HIVE_schemeh.mytbl 2019-01-01 00:00:00+00:00 [upstream_failed]> <TaskInstance: mycooldag.REPAIR_HIVE_schemeh.mytbl 2019-01-02 00:00:00+00:00 [upstream_failed]> <TaskInstance: mycooldag.REPAIR_HIVE_schemeh.mytbl 2019-01-03 00:00:00+00:00 [upstream_failed]> <TaskInstance: mycooldag.REPAIR_HIVE_schemeh.mytbl 2019-01-03 05:00:00+00:00 [upstream_failed]> <TaskInstance: mycooldag.REPAIR_HIVE_schemeh.mytbl 2019-04-11 00:00:00+00:00 [failed]> <TaskInstance: mycooldag.REPAIR_HIVE_schemeh.mytbl 2019-04-11 17:00:00+00:00 [failed]> <TaskInstance: mycooldag.REPAIR_HIVE_schemeh.mytbl 2019-04-11 22:00:00+00:00 [failed]> <TaskInstance: mycooldag.REPAIR_HIVE_schemeh.mytbl 2019-04-12 00:00:00+00:00 [upstream_failed]> <TaskInstance: mycooldag.REPAIR_HIVE_schemeh.mytbl 2019-04-12 07:00:00+00:00 [removed]> <TaskInstance: mycooldag.REPAIR_HIVE_schemeh.mytbl 2019-04-12 08:00:00+00:00 [removed]>"
109,AIRFLOW-4363,AIRFLOW,1555746637000,1593525562000,Bug,Resolved,Blocker,1,2048,"Encounter JSON Decode Error when using docker operator *[Description]* . When using the docker_operator, I experienced some issue while using Mac OS 10.14.4. The error was json.JSONDecodeError. After my investigation about this error, I found that there are several messages for logging aren't well separated, for example it contains \n inside one single message which should be split into 2 to more different messages. *[Update]* Confirmed that issue came from the implementation in airflow, issue cannot be solved by just passing `decode` to parameter in docker.pull method in docker api. *[Solution]* . For now, I use try-catch to run the original implementation, and in the exception part I split the message to list and then parse it. Looking for simpler solution to this non critical but still blocking point. *[Logs]* {docker_operator.py:188} INFO - Starting docker container from image hello-world {docker_operator.py:202} INFO - Pulling docker image hello-world {docker_operator.py:207} INFO - Pulling from library/hello-world {docker_operator.py:207} INFO - Pulling fs layer {docker_operator.py:207} INFO - Downloading {docker_operator.py:207} INFO - Downloading {docker_operator.py:207} INFO - Download complete {docker_operator.py:207} INFO - Extracting {docker_operator.py:207} INFO - Extracting {docker_operator.py:207} INFO - Pull complete {docker_operator.py:207} INFO - Digest: sha256:92695bc579f31df7a63da6922075d0666e565ceccad16b59c3374d2cf4e8e50e {docker_operator.py:207} INFO - Pulling from library/hello-world {docker_operator.py:207} INFO - Digest: sha256:1a67c1115b199aa9d964d5da5646917cbac2d5450c71a1deed7b1bfb79c2c82d {models.py:1788} ERROR - Extra data: line 2 column 1 (char 70) Traceback (most recent call last): line 1657, in _run_raw_task, result = task_copy.execute(context=context) line 205, in execute output = json.loads(line) line 354, in loads, return _default_decoder.decode(s) line 342, in decode, raise JSONDecodeError(""Extra data"", s, end) json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 70)"
110,AIRFLOW-4390,AIRFLOW,1556011944000,1556721430000,Improvement,Open,Blocker,1,439,"manage_slas() needs refactoring As analyzed in https://issues.apache.org/jira/browse/AIRFLOW-4297?focusedCommentId=16822945&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16822945 , current implementation of manage_slas() has a few flaws, and is not functioning in some cases. We need to refactor it somehow. As discussed in [https://github.com/apache/airflow/pull/5150] , we put it as a blocker for 2.0.0."
111,AIRFLOW-4424,AIRFLOW,1556426937000,1577812917000,Bug,Open,Blocker,1,752,"Scheduler does not terminate after num_runs when executor is KubernetesExecutor When using the executor like the CeleryExecutor and num_runs is set on the scheduler, the scheduler pod restarts after num runs have completed. After switching to KubernetesExecutor, the scheduler logs: [2019-04-26 19:20:43,562] \{{kubernetes_executor.py:770}} INFO - Shutting down Kubernetes executor However, the scheduler process does not complete. This leads to the scheduler pod never restarting and running num_runs again. Resulted in having to roll back to CeleryExecutor because if num_runs is -1, the scheduler builds up tons of defunct processes, which is eventually making tasks not able to be scheduled as the underlying nodes have run out of file descriptors."
112,AIRFLOW-4452,AIRFLOW,1556768697000,1556872870000,Bug,Resolved,Blocker,1,3124,"Webserver and Scheduler keep crashing because of slackclient update Webserver and Scheduler get into a crash loop if Airflow is installed with slack dependencies. Airflow relies on slackclient which released a new major version (2.0.0) today ([https://pypi.org/project/slackclient/#history]). This new version seems to be incompatible with Airflow causing the webserver to get into a crash loop. The root cause of the issue is that Airflow doesn't pin requirements for slackclient: [https://github.com/apache/airflow/blob/v1-10-stable/setup.py#L229] {code:java} slack = ['slackclient>=1.0.0']{code} This is the exception in the logs due to this error: {code:java} File ""/Users/abhishek.ray/airflow/dags/test_dag.py"", line 3, in <module> from airflow.operators import SlackAPIPostOperator File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/airflow/utils/helpers.py"", line 372, in __getattr__ loaded_attribute = self._load_attribute(attribute) File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/airflow/utils/helpers.py"", line 336, in _load_attribute self._loaded_modules[module] = imp.load_module(module, f, filename, description) File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/imp.py"", line 235, in load_module return load_source(name, filename, file) File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/imp.py"", line 172, in load_source module = _load(spec) File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/airflow/operators/slack_operator.py"", line 24, in <module> from airflow.hooks.slack_hook import SlackHook File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/airflow/hooks/slack_hook.py"", line 20, in <module> from slackclient import SlackClient File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/slackclient/__init__.py"", line 1, in <module> from .client import SlackClient # noqa File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/slackclient/client.py"", line 8, in <module> from .server import Server File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/slackclient/server.py"", line 14, in <module> from websocket import create_connection ModuleNotFoundError: No module named 'websocket' {code} This is how to reproduce this issue: Install apache airflow with slack: {code:java} pip install apache-airflow[slack]==1.10.1{code} Create a DAG which uses *SlackAPIPostOperator* {code:java} from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators import SlackAPIPostOperator dag_default_args = { ""owner"": ""airflow"", ""depends_on_past"": False, ""start_date"": datetime(2019, 4, 22), ""email"": [""airflow@airflow.com""], ""email_on_failure"": False, ""email_on_retry"": False, ""retries"": 1, ""catchup"": True, } dag = DAG(""test_dag"", default_args=dag_default_args, schedule_interval=""@daily"") BashOperator(task_id=""print_date"", bash_command=""date"", dag=dag){code} I think the fix should be pretty straightforward to add a max version for slackclient."
113,AIRFLOW-4470,AIRFLOW,1557228812000,1616147984000,Bug,Open,Blocker,1,1741,"RBAC Github Enterprise OAuth provider callback URL? Hi all, Quick question, when using RBAC with OAuth providers (1.10.2): * we are not specifying the {{authenticate}} or {{auth_backend}} in the [webserver] section of \{{airflow.cfg}}anymore * Instead, we set the OAuth provider config in the flask-appbuilder's {{webserver_config.py}}: {code:java} # Adapting Google OAuth example to Github: OAUTH_PROVIDERS = [ {'name':'github', 'icon':'fa-github', 'token_key':'access_token', 'remote_app': { 'base_url':'https://github.corporate-domain.com/login', 'access_token_url':'https://github.corporate-domain.com/login/oauth/access_token', 'authorize_url':'https://github.corporate-domain.com/login/oauth/authorize', 'request_token_url': None, 'consumer_key': 'XXXXXXXXXXXX', 'consumer_secret': 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX', } } ] {code} _Question:_ * so what callback URL do we specify in the app? {{http:/webapp/ghe_oauth/callback}} would not work right? (example with github entreprise) No matter what I specify for the callback url (/ghe_oauth/callback or [http://webapp.com|http://webapp.com/]), I get an error message about {{redirect_uri}} mismatch: {code:java} {{error=redirect_uri_mismatch&error_description=The+redirect_uri+MUST+match+the+registered+callback+URL+for+this+application }}{code} _Docs ref:_ Here is how you setup OAuth with Github Entreprise on Airflow _*without*_ RBAC: [https://airflow.apache.org/security.html#github-enterprise-ghe-authentication] And here is how you setup OAuth via the {{webserver_config.py}} of flask_appbuilder used by airflow _*with*_RBAC: [https://flask-appbuilder.readthedocs.io/en/latest/security.html#authentication-oauth] What's the *callback url* when using RBAC and OAuth with Airflow?"
114,AIRFLOW-4525,AIRFLOW,1558046970000,1576849878000,Bug,Open,Blocker,1,6009,"Trigger Dag Operator causes duplicate key exceptions and can cause runaway dag spawning as it is not atomic at the DB level (on Postgres at least.) When using the TriggerDagRunOperator there is a problem in the code which loops round subdags scheduling them. You will not see this issue if you only have one level of sub dag, but if your sub dags have sub dags then you will see it. The code pops an item off the list (non unique) and schedules it. It then appends all sub dags of the dag it popped off the list to the current list. It keeps doing this until the list is empty. The problem is that <<top level dag>>.subdags returns _*all*_ subdags at _*all*_ levels. So when you process a <<top level dag.first level subdag>> it calls <<first level subdag>>.subdags and once agains this will append all its subdags, _*which are already in the list*_. Thus you are now certain you will get a duplicate key exception as the same dag ID and run ID are present twice. Up to and including 1.10.2 this is not a significant problem most of the time. You see the duplicate key errors in the logs but it does not cause the operator to raise and hence the task actually succeeds. That said, you do get a load of ""running"" sub dags in the console which never really do anything as they aren't invoked from the parent dag when it wants them to run and hence have no ""task instance"" connection to that dag. *+However, in 1.10.3 this causes havoc.+* Firstly, it no longer exits cleanly. It causes the operator to raise an error and so it fails. Worse, since the statements it has executed to schedule is dag are _*not*_ in the same transaction, all the dags before the first duplicate _*are triggered*_. But since the task will subsequently be retried (if configured) _*they will be triggered again.*_ Because the logic to generate the run ID use now() as part of the key they generate, subsequent invocations will have a different run ID and hence will cause all the dags before the first duplicate exception to be scheduled repeatedly, up to the maximum retry limit. You still get all the orphaned sub dag entries I mentioned from 10.2, but you get many many copies of them. I'm not sure what the best fix is (or if it's my place to suggest one) but from what I've seen the cleanest approach is either to use a set, to avoid duplicate entries, rather than the current list based approach OR continue to use the list with it's ""pop"" semantics but keep track of items already processed and avoid re-appending them. This would fix the current problem, but to be honest it feels semantically *_incorrect_* to trigger the sub dags in this way. The top level dag invokes the sub dags as task instances like any other and you're going behind its back invoking them this way. Moreover, the semantic contract of the TriggerDagRunOperator is that it takes a single dag ID as input, implicitly creating the expectation that this is the _*only dag which will be triggered.*_ Scheduling the sub dags as part of doing this feels wrong and actually creates an error whilst doing nothing to help the operation of the platform (unless there is a different configuration set up I am not thinking of which is entirely possible.) But as far as I can discern, if you _*only*_ trigger the top level dag you've been _*asked*_ to trigger then actually, everything will work just fine. The SubDagOperator which wraps the sub dags will trigger the sub dag anyway at the right time, based on whatever dependencies are in the top level dag (which might be none, in which case any sub dags will get scheduled automatically. The reason I know this of course is that the first time you trigger the top level DAG in the UI, only one row is written to the dag_run table, only the top level dag is triggered, and yet, it works just fine... If there is some scenario which should still require the sub dags to be triggered, I think it's important this this sort of operator is atomic (or at the very least idempotent.) Otherwise you can risk significant issues in a production environment with ""over-triggering"" Dags. Even if concurrent dag configuration prevents them from running concurrently the list of scheduled dags can in theory grow forever (or to max_retries of the TriggerDagRunOperator task) and can cause some significant undesirable side effects. From what I can see, using a transaction would perhaps be complex (and not cross platform friendly), but at the very least the dag entries should perhaps be added to the DB with _*no*_ state and then convert them all to RUNNING once you know they've all successfully inserted and any primary key issues are resolved. The state is not part of the primary key so this would not cause a problem. The worst case outcome under this approach occurs only if some form of DB failure between the inserts and the state update occurs. This potentially means the dags never started, but I think that's a ""better worst case"" than the current situation where multiple unintended triggers can happen. I have set this as a Blocker because I cannot find any way around it without modifying the core code myself and we, like many others I suspect, have dags which start with a sensor waiting for incoming data and then process it and trigger another instance of themselves to wait once again. We are currently using 1.10.2 but this is a blocker for us upgrading to 1.10.3. I can't find any way to stop the duplicate key errors from happening whatever I do unless I completely re-work my entire dag layout, which just makes them look highly complex and would obliterate the nice modular approach we've managed to build in our platform (and indeed which the Airflow platform encourages.) Please let me know if you need anything else. I've never contributed to an Apache project and would need a little guidance and support if I were to try to address it myself. I'm willing to try though as I use Airflow a lot and would love to give something back. Would just need a little early pointing in the right direction. :)"
115,AIRFLOW-4842,AIRFLOW,1561370695000,1561458474000,Sub-task,Resolved,Blocker,1,49,Issue deprecation notice on Py2 on 1.10 releases.
116,AIRFLOW-5035,AIRFLOW,1563979891000,1564485300000,Bug,Resolved,Blocker,1,437,"Dag parsing process can leave orphan processes under heavy load As reported by James Meickle on the mailing list, under certain cases it is possible that the multiprocessing.Manager process can end up orphaned when the scheduler shuts down. This is relating to a new merge in 1.10.4 (i.e. 1.10.3 wasn't affected) The ""orphan"" process is massively exasperated but having a {{--run-duration 600}}, but we should try and fix this if we can."
117,AIRFLOW-5080,AIRFLOW,1564557165000,1573842513000,Bug,Closed,Blocker,1,807,"npm: not found with ./airflow/www_rbac/compile_assets.sh h2. Problem When I run [scripts/ci/kubernetes/docker/build.sh|https://github.com/apache/airflow/blob/1.10.3/scripts/ci/kubernetes/docker/build.sh#L45] to build Docker Image, I encountered the following error. {code} running compile_assets ./airflow/www_rbac/compile_assets.sh: 26: ./airflow/www_rbac/compile_assets.sh: npm: not found {code} So, http://YOUR_HOSTNAME/static didn't create. I got `500 Internal Server Error` with it on a browser(Chrome). h2. Proposal I think it needs to install npm in [scripts/ci/kubernetes/docker/compile.sh|https://github.com/apache/airflow/blob/1.10.3/scripts/ci/kubernetes/docker/compile.sh#L30]. {code} apt-get install -y --no-install-recommends git nodejs npm {code} It works for me. Please consider my proposal."
118,AIRFLOW-5179,AIRFLOW,1565646915000,1576282341000,Bug,Resolved,Blocker,1,375,"Top level __init__.py breaks imports The recent commit [3724c2aaf4cfee4a60f6c7231777bfb256090c7c|https://github.com/apache/airflow/commit/3724c2aaf4cfee4a60f6c7231777bfb256090c7c] to master introduced a {{__init__.py}} file in the project root folder, which basically breaks all imports in local development ({{pip install -e .}}) as it turns the project root into a package."
119,AIRFLOW-5191,AIRFLOW,1565692378000,1582649092000,Bug,Open,Blocker,1,1041,"SubDag is marked failed Dear all, after having upgraded from Airflow version 1.10.2 to 1.10.4, we experience strange and very problematic behaviour of SubDags (which are crucial for our environment and used frequently). Tasks inside the SubDag failing and awaiting retry (""up-for-retry"") mark the SubDag ""failed"" (while in 1.10.2, the SubDag was still in ""running""-state). This is particularly problematic for downstream tasks depending on the state of the SubDag. Since we have downstream tasks triggered on ""all_done"", the downstream task is triggered by the ""failed"" SubDag although a SubDag-internal task is awaiting retry and might (in our case: most likely) yield successfully processed data. This data is thus not available to the prematurely triggered task downstream of the SubDag. This is a severe problem for us and worth rolling back to 1.10.2 if there is no quick solution or work-around to this issue! We urgently need help on this matter. Thanks allot in advance, any suggestions and input is highly appreciated! Cheers Oliver"
120,AIRFLOW-5224,AIRFLOW,1565864243000,1576282348000,Bug,Resolved,Blocker,1,941,"gcs_to_bq.GoogleCloudStorageToBigQueryOperator - Specify Encoding for BQ ingestion Hi, The current business project we are enabling has been built completely on GCP components with composer with airflow being one of the key process. We have built various data pipelines using airflow for multiple work-streams where data is being ingested from gcs bucket to Big query. Based on the recent updates on Google BQ infra end, there seems to be some tightened validations on UTF-8 characters which has resulted in mutiple failures of our existing business process. On further analysis we found out that while ingesting data to BQ from a Google bucket the encoding needs to be explicitly specified going forward but the below operator currently doesn't supply any params to specify explicit encoding _*gcs_to_bq.GoogleCloudStorageToBigQueryOperator*_ Could someone please treat this as a priority and help us with a fix to bring us back in BAU mode"
121,AIRFLOW-5249,AIRFLOW,1566178575000,1575879161000,Bug,Resolved,Blocker,1,2762,"BigQueryCheckOperator fails for datasets outside of 'US' region When I try to use the BigQueryCheckOperator or BigQueryValueCheckOperator on a dataset that is not in the 'US' location my task fails with the following error {code:java} [2019-08-15 07:26:19,378] {__init__.py:1580} ERROR - BigQuery job status check failed. Final error was: 404 Traceback (most recent call last): File ""/usr/local/lib/python3.6/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 1241, in run_with_configuration jobId=self.running_job_id).execute() File ""/usr/local/lib/python3.6/site-packages/googleapiclient/_helpers.py"", line 130, in positional_wrapper return wrapped(*args, **kwargs) File ""/usr/local/lib/python3.6/site-packages/googleapiclient/http.py"", line 855, in execute raise HttpError(resp, content, uri=self.uri) googleapiclient.errors.HttpError: <HttpError 404 when requesting https://www.googleapis.com/bigquery/v2/projects/PROJECT/jobs/job_ISDpiVtd7U1p-6N9wT378LfwoFHc?alt=json returned ""Not found: Job PROJECT:job_ISDpiVtd7U1p-6N9wT378LfwoFHc""> During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/usr/local/lib/python3.6/site-packages/airflow/models/__init__.py"", line 1441, in _run_raw_task result = task_copy.execute(context=context) File ""/usr/local/lib/python3.6/site-packages/airflow/operators/check_operator.py"", line 81, in execute records = self.get_db_hook().get_first(self.sql) File ""/usr/local/lib/python3.6/site-packages/airflow/hooks/dbapi_hook.py"", line 138, in get_first cur.execute(sql) File ""/usr/local/lib/python3.6/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 1821, in execute self.job_id = self.run_query(sql) File ""/usr/local/lib/python3.6/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 849, in run_query return self.run_with_configuration(configuration) File ""/usr/local/lib/python3.6/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 1263, in run_with_configuration format(err.resp.status)) Exception: BigQuery job status check failed. Final error was: 404 [2019-08-15 07:26:19,388] {__init__.py:1611} INFO - Marking task as FAILED. {code} This is the same error I get when I try to run the BigQuery operator without specifying a location. When I run the same operator on a dataset that is in the US region It succeeds. The BigQueryCheckOperator does not accept a location as one of its arguments and does not pass a location to the BigQueryHook, I believe this is the source of the problem. I realise a task (AIRFLOW-3601) was already created to fix a similar issue to this one, but the referenced task calls out the two operators I'm having an issue with as out of scope and after commenting on that task I have not received a response."
122,AIRFLOW-5271,AIRFLOW,1566370319000,1572939640000,Bug,Closed,Blocker,1,3730,"EmrCreateJobFlowOperator throwing error in airflow 1.10.4 version h3. *ERROR LOGS:* {{[2019-08-21 05:39:42,970] \{emr_create_job_flow_operator.py:66} INFO - Creating JobFlow using aws-conn-id: aws_default, emr-conn-id: emr_default [2019-08-21 05:39:42,981] \{logging_mixin.py:95} INFO - [[34m2019-08-21 05:39:42,980[0m] \{[34mconnection.py:[0m296} ERROR[0m - Expecting property name enclosed in double quotes: line 1 column 2 (char 1)[0m [31mTraceback (most recent call last): File ""/home/ubuntu/.local/lib/python2.7/site-packages/airflow/models/connection.py"", line 294, in extra_dejson obj = json.loads(self.extra) File ""/usr/lib/python2.7/json/__init__.py"", line 339, in loads return _default_decoder.decode(s) File ""/usr/lib/python2.7/json/decoder.py"", line 364, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File ""/usr/lib/python2.7/json/decoder.py"", line 380, in raw_decode obj, end = self.scan_once(s, idx) ValueError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)[0m [2019-08-21 05:39:42,982] \{logging_mixin.py:95} INFO - [[34m2019-08-21 05:39:42,981[0m] \{[34mconnection.py:[0m297} ERROR[0m - Failed parsing the json for conn_id [1maws_default[0m[0m [2019-08-21 05:39:43,054] \{taskinstance.py:1047} ERROR - Parameter validation failed: Unknown parameter in input: ""TerminationProtected"", must be one of: Name, LogUri, AdditionalInfo, AmiVersion, ReleaseLabel, Instances, Steps, BootstrapActions, SupportedProducts, NewSupportedProducts, Applications, Configurations, VisibleToAllUsers, JobFlowRole, ServiceRole, Tags, SecurityConfiguration, AutoScalingRole, ScaleDownBehavior, CustomAmiId, EbsRootVolumeSize, RepoUpgradeOnBoot, KerberosAttributes Unknown parameter in input: ""KeepJobFlowAliveWhenNoSteps"", must be one of: Name, LogUri, AdditionalInfo, AmiVersion, ReleaseLabel, Instances, Steps, BootstrapActions, SupportedProducts, NewSupportedProducts, Applications, Configurations, VisibleToAllUsers, JobFlowRole, ServiceRole, Tags, SecurityConfiguration, AutoScalingRole, ScaleDownBehavior, CustomAmiId, EbsRootVolumeSize, RepoUpgradeOnBoot, KerberosAttributes Unknown parameter in input: ""Ec2SubnetId"", must be one of: Name, LogUri, AdditionalInfo, AmiVersion, ReleaseLabel, Instances, Steps, BootstrapActions, SupportedProducts, NewSupportedProducts, Applications, Configurations, VisibleToAllUsers, JobFlowRole, ServiceRole, Tags, SecurityConfiguration, AutoScalingRole, ScaleDownBehavior, CustomAmiId, EbsRootVolumeSize, RepoUpgradeOnBoot, KerberosAttributes Unknown parameter in input: ""Ec2KeyName"", must be one of: Name, LogUri, AdditionalInfo, AmiVersion, ReleaseLabel, Instances, Steps, BootstrapActions, SupportedProducts, NewSupportedProducts, Applications, Configurations, VisibleToAllUsers, JobFlowRole, ServiceRole, Tags, SecurityConfiguration, AutoScalingRole, ScaleDownBehavior, CustomAmiId, EbsRootVolumeSize, RepoUpgradeOnBoot, KerberosAttributes}} h3. {{*CORRESPONDING DAG CODE:*}} {{}} {noformat} // code placeholder airflow_test_json=json.load(open(airflow_home+'/test.json')) airflow_asset_analytics_creator = EmrCreateJobFlowOperator( task_id='create_asset_analytics_databricks_test', job_flow_overrides=airflow_test_json['Job'], timeout=10, aws_conn_id='aws_default', emr_conn_id='emr_default', dag=dag ) airflow_asset_analytics_sensor = EmrJobFlowSensor( task_id='check_asset_analytics_databricks_stable', job_flow_id=""{{ task_instance.xcom_pull('create_asset_analytics_databricks_test', key='return_value') }}"", aws_conn_id='aws_default', dag=dag ) airflow_asset_analytics_sensor.set_upstream(airflow_asset_analytics_creator) {noformat} {{}} Not sure what more details are required,but the exact code worked flawlessly in v1.10.0."
123,AIRFLOW-5321,AIRFLOW,1566934172000,1567687625000,Bug,Closed,Blocker,1,432,"The memory increases until the scheduler crashed, the only solution is to restart the vm Hi Airflow community, im using airflow 1.10.4, upgraded directly from 1.10.2. im running supervisor for the webserver and the scheduler. Using celeryExecutor, everything was ok up to the update, seems like one of the processes i have (google search console) somehow eats the memory, and the only solution is to restart the vm (AWS - *68 GiB*)."
124,AIRFLOW-5358,AIRFLOW,1567166590000,1567680605000,Improvement,Resolved,Blocker,1,34,Improved mocking of the project ID
125,AIRFLOW-5375,AIRFLOW,1567412454000,1577812915000,Bug,Resolved,Blocker,1,239,"Airflow 1.10.4 doesn`t installing on windows Airflow 1.10.4 has new requirement ""dumb-init"" which has platform requirement ""linux"". https://stackoverflow.com/questions/57466782/attributeerror-msvccompiler-object-has-no-attribute-linker-exe"
126,AIRFLOW-5385,AIRFLOW,1567437945000,1578695314000,Improvement,Resolved,Blocker,1,1973,"SparkSubmit status spend lot of time Hello, we have an issue with SparkSubmitOperator. Airflow DAGs shows that some streaming applications breaks out. I analyzed this behaviour. The SparkSubmitHook is the responsable of check the driver status. We discovered some timeouts and tried to reproduce checking command. This is an execution with `time`: {code:java} time /opt/java/jdk1.8.0_181/jre/bin/java -cp /opt/shared/spark/client/conf/:/opt/shared/spark/client/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --master spark://spark-master.corp.com:6066 --status driver-20190901180337-2749 Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties 19/09/02 17:05:53 INFO RestSubmissionClient: Submitting a request for the status of submission driver-20190901180337-2749 in spark://lgmadbdtpspk01v.corp.logitravelgroup.com:6066. 19/09/02 17:05:59 INFO RestSubmissionClient: Server responded with SubmissionStatusResponse: { ""action"" : ""SubmissionStatusResponse"", ""driverState"" : ""RUNNING"", ""serverSparkVersion"" : ""2.2.1"", ""submissionId"" : ""driver-20190901180337-2749"", ""success"" : true, ""workerHostPort"" : ""172.25.10.194:45441"", ""workerId"" : ""worker-20190821201014-172.25.10.194-45441"" } real 0m11.598s user 0m2.092s sys 0m0.222s{code} We analyzed the Scala code and Spark API. This spark-submit status command ends with a http get request to an url. Using curl, this is the time spent by spark master to return status: {code:java} time curl ""http://spark-master.corp.com:6066/v1/submissions/status/driver-20190901180337-2749"" { ""action"" : ""SubmissionStatusResponse"", ""driverState"" : ""RUNNING"", ""serverSparkVersion"" : ""2.2.1"", ""submissionId"" : ""driver-20190901180337-2749"", ""success"" : true, ""workerHostPort"" : ""172.25.10.194:45441"", ""workerId"" : ""worker-20190821201014-172.25.10.194-45441"" } real 0m0.011s user 0m0.000s sys 0m0.006s {code} Task spends 11.59 seconds with spark submit versus 0.011seconds with curl How can be this behaviour explained?"
127,AIRFLOW-5406,AIRFLOW,1567638173000,1580754117000,Bug,Resolved,Blocker,1,495,"Allow spark_submit_hook to run without pip install kubernetes/airflow[kubernetes] Allow spark_submit_hook to run without pip install kubernetes/airflow[kubernetes] k8s dependencies causing issues for non-k8s users based on 1.10.6: *settings.py:* from airflow.contrib.kubernetes.pod import Pod def pod_mutation_hook(pod): *models/__init__.py* from airflow.models.kubernetes import KubeWorkerIdentifier, KubeResourceVersion *spark_submit_hook.py* from airflow.contrib.kubernetes import kube_client"
128,AIRFLOW-5447,AIRFLOW,1568041938000,1568807851000,Bug,Resolved,Blocker,1,451,"KubernetesExecutor hangs on task queueing Starting in 1.10.4, and continuing in 1.10.5, when using the KubernetesExecutor, with the webserver and scheduler running in the kubernetes cluster, tasks are scheduled, but when added to the task queue, the executor process hangs indefinitely. Based on log messages, it appears to be stuck at this line https://github.com/apache/airflow/blob/v1-10-stable/airflow/contrib/executors/kubernetes_executor.py#L761"
129,AIRFLOW-5664,AIRFLOW,1571182616000,1584349919000,Bug,Resolved,Blocker,1,792,"postgres_to_gcs operator drops milliseconds from timestamps Postgres stores timestamps with microsecond resolution. When using the postgres_to_gcs operator, timestamps are converted to epoch/unix time using the datetime.timetuple() method. This method drops the microseconds and so you'll end up with a storage object that looks like this: {code:java} {""id"": 1, ""last_modified"": 1571038537.0} {""id"": 2, ""last_modified"": 1571038537.0} {""id"": 3, ""last_modified"": 1571038537.0} {code} When it should look like this: {code:java} {""id"": 1, ""last_modified"": 1571038537.123} {""id"": 2, ""last_modified"": 1571038537.400} {""id"": 3, ""last_modified"": 1571038537.455} {code} It would be useful to keep the timestamps' full resolution. I believe the same issue may occur with airflow.operators.mysql_to_gcs."
130,AIRFLOW-5818,AIRFLOW,1572445227000,1581532054000,Bug,Resolved,Blocker,1,674,"Very bad webserver performance when defining many dags with many operators In my scenario I have defined 500 dags, each dag has approximately 1500 operators. This makes webserver impossible to work with even when all dags are paused and nothing is running. The cpu spikes all the time and webserver consumes huge amounts of memory for no reason. To reproduce this use the attched my_dag.template file and duplicate it using the attached dup_dags.py script. The root cause of this issue is that dagbag will load all dags into memory which takes huge cpu and memory unnecessarily. I have already fixed this in: [https://github.com/gary-harpaz/airflow/tree/improve-performance]"
131,AIRFLOW-5864,AIRFLOW,1573122633000,1573122738000,Bug,Open,Blocker,1,727,"RBAC with github_enterprise oauth authentication For the following configuration in webserver_config.py {code:java} // code placeholder OAUTH_PROVIDERS = [{ 'name':'github', 'token_key':'access_token', 'icon':'fa-github', 'remote_app': { 'base_url':'https://github.corporate.domain.com/login', # 'request_token_params':{ # 'scope': 'email profile' # }, 'access_token_url':'https://github.corporate.domain.com/login/oauth/access_token', #'access_token_url': None, 'authorize_url':'https://github.corporate.domain.com/login/oauth/authorize', 'request_token_url': None, 'consumer_key': '******************', 'consumer_secret': '********************', } }] {code} is redirecting to the following !image-2019-11-07-15-59-21-298.png!"
132,AIRFLOW-6019,AIRFLOW,1574251662000,1574875448000,Bug,Closed,Blocker,1,379,KubernetesExecutor iteritems() exception when using Python 3.* When running scheduler using {{KubernetesExecutor}} with Python 3.7 the exception occurs due to the use of {{iteritems()}} within dictionaries. {{iteritems()}} found: - {{airflow/executors/kubernetes_executor.py}} line 277 - {{airflow/executors/kubernetes_executor.py}} line 655 TODO: use {{six.iteritems}} function.
133,AIRFLOW-6020,AIRFLOW,1574253597000,1574526931000,Bug,Resolved,Blocker,1,379,KubernetesExecutor iteritems() exception when using Python 3.* When running scheduler using {{KubernetesExecutor}} with Python 3.7 the exception occurs due to the use of {{iteritems()}} within dictionaries. {{iteritems()}} found: - {{airflow/executors/kubernetes_executor.py}} line 277 - {{airflow/executors/kubernetes_executor.py}} line 655 TODO: use {{six.iteritems}} function.
134,AIRFLOW-6062,AIRFLOW,1574691800000,1593682694000,Bug,Resolved,Blocker,1,391,"Scheduler doesn't delete worker pods from namespaces different than the scheduler's namespace When you run Airflow's task instances as worker pods in different namespaces into a Kubernetes cluster, the scheduler can delete only the pods that are living in the same namespace where the scheduler lives. It's trying to delete all pods that are in the namespace defined in the airflow.cfg file."
135,AIRFLOW-6083,AIRFLOW,1574856671000,1576613746000,Bug,Resolved,Blocker,1,1342,"AwsLambdaHook is not accepting non-default configuration Hello. While using Airflow we have come across a problem with AwsLambdaHook. We are using this hook to launch Lambda function which takes a while to complete (around 5-6 minutes). However, since lambda is invoked through boto3 client, which has default timeout set to [60 seconds|https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-retry-timeout-sdk/], our Airflow interface shows this lambda failing, even though Lambda finishes successfully checking at AWS console. This also causes another side effect: Since boto3 thinks that Lambda has timed out, it automatically spawns another instance, which also times out and this chain lasts 5 times, spawning 5 Lambdas and all these Lambdas show as failed in Airflow interface, while they actually succeed. This can be solved by passing in custom configuration when creating a boto3 client, however, it is not possible to do that when creating AwsLambdaHook as it does not take in this parameter. However, we see that AwsLambdaHook inherits and uses AwsHook's function (get_client_type) to get the boto3 client and this function accepts configuration parameter (which defaults to None), but it is never passed to it from the Lambda's hook, which could be easily achieved and would fix the bug we are facing at the moment."
136,AIRFLOW-6175,AIRFLOW,1575496748000,1584365095000,Bug,Resolved,Blocker,1,570,"Executor changes queued tasks to scheduled without removing from task queue There is a bug caused by scheduler_jobs refactor which leads to task failure and scheduler locking. Essentially when a there is an overflow of tasks going into the scheduler, the tasks are set back to scheduled, but are not removed from the executor's queued_tasks queue. This means that the executor will attempt to run tasks that are in the scheduled state, but those tasks will fail dependency checks. Eventually the queue is filled with scheduled tasks, and the scheduler can no longer run."
137,AIRFLOW-6233,AIRFLOW,1576138986000,1579045654000,Bug,Open,Blocker,1,1085,Task get stuck on scheduled on LocalExecutor Use case description: One DAG with a sequence of tasks run by the LocalExecutor. The number of parallel tasks (6) to be run is higher than the concurrency limit (3) (parallelism and dag_concurrency). An image of the DAG layout is attached below. Expected behaviour: Scheduler uses the three available slots to run as many tasks concurrently as possible. The DAG finishes in some finite time without restarting the scheduler. Example Gantt chart attached below. Observed behaviour: The tasks are not optimally scheduled and after a while the tasks no longer move from the 'scheduled' state to 'queued'. When the scheduler is restarted the tasks are picked up again as expected. The Gantt chart is also in the attachments. Note that the chart does not show the tasks that never get scheduled. You can find the logs in the attachments. How to reproduce: Install vanilla airflow version 1.10.6 and run with LocalExecutor (I used a mysql database). The airflow.cfg and dag file are added to the attachments. The problem does not exist in 1.10.5.
138,AIRFLOW-6505,AIRFLOW,1578455292000,1581603128000,Bug,Closed,Blocker,1,629,"Let emoji encoded properly for json.dumps() -- BaseSQLToGoogleCloudStorageOperator Emoji can't be encoded properly when ` json.dumps()` and 'UTF-8', problem fixed by adding parameter `ensure_ascii=False`. I've created a [PR|[https://github.com/apache/airflow/pull/7399]] to fix this problem. In [line|#L175]] BaseSQLToGoogleCloudStorageOperator. For example the emoji  encoded differentially when in use or not use `ensure_ascii=False`. In Use (correct UTF-8 encode): ""\xf0\x9f\x8d\xbb"" Not Use (only 2 slash):""\\\ud83c\\\udf7b"" Ref: [https://stackoverflow.com/questions/51183947/python-json-dumps-doesnt-encode-emojis-properly]"
139,AIRFLOW-6523,AIRFLOW,1578604381000,1578618789000,Bug,Closed,Blocker,1,354,Broken airflow-worker.service with latest commit The Latest checkin breaks the script : [https://github.com/apache/airflow/commit/5fb64b9f2a567b74d09b10661b7d32c8bbfdefcb] h2. scripts/systemd/airflow-worker.service *The old script had :ExecStart=/bin/airflow worker* *The new one has: ExecStart=/bin/airflow celery worker* *This stops the worker working*
140,AIRFLOW-6567,AIRFLOW,1579076470000,1580060922000,Bug,Closed,Blocker,1,2310,"Configuration broken when using proxy_fix We tried to upgrade from 1.10.6 to 1.10.7 but encountered an issue when trying to access the webserver. We are running the solution in kubernetes at AWS behind Kong as proxy/gateway. 1.10.6 works niceley but after the upgrade we get the following exception: {code:java} [2020-01-15 08:12:02 +0000] [111] [ERROR] Error handling request /1/15/2020 9:12:02 AM Traceback (most recent call last):1/15/2020 9:12:02 AM File ""/usr/local/lib/python3.7/site-packages/gunicorn/workers/sync.py"", line 135, in handle1/15/2020 9:12:02 AM self.handle_request(listener, req, client, addr)1/15/2020 9:12:02 AM File ""/usr/local/lib/python3.7/site-packages/gunicorn/workers/sync.py"", line 176, in handle_request1/15/2020 9:12:02 AM respiter = self.wsgi(environ, resp.start_response)1/15/2020 9:12:02 AM File ""/usr/local/lib/python3.7/site-packages/werkzeug/middleware/dispatcher.py"", line 66, in __call__1/15/2020 9:12:02 AM return app(environ, start_response)1/15/2020 9:12:02 AM File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 2463, in __call__1/15/2020 9:12:02 AM return self.wsgi_app(environ, start_response)1/15/2020 9:12:02 AM File ""/usr/local/lib/python3.7/site-packages/werkzeug/middleware/proxy_fix.py"", line 195, in __call__1/15/2020 9:12:02 AM x_for = self._get_trusted_comma(self.x_for, environ_get(""HTTP_X_FORWARDED_FOR""))1/15/2020 9:12:02 AM File ""/usr/local/lib/python3.7/site-packages/werkzeug/middleware/proxy_fix.py"", line 166, in _get_trusted_comma1/15/2020 9:12:02 AM if len(values) >= trusted:1/15/2020 9:12:02 AM TypeError: '>=' not supported between instances of 'int' and 'str' {code} From my own investigation the issue seem to be the new configuration options for the proxy where the configuration which are fetched returns strings rather then integers as it should. [https://github.com/apache/airflow/pull/6723] {code:java} x_for=conf.get(""webserver"", ""PROXY_FIX_X_FOR"", fallback=1), x_proto=conf.get(""webserver"", ""PROXY_FIX_X_PROTO"", fallback=1), x_host=conf.get(""webserver"", ""PROXY_FIX_X_HOST"", fallback=1), x_port=conf.get(""webserver"", ""PROXY_FIX_X_PORT"", fallback=1), x_prefix=conf.get(""webserver"", ""PROXY_FIX_X_PREFIX"", fallback=1) {code} (row 51-56 in airflow/www/app.py) The following values are expected to be integers and not strings."
141,AIRFLOW-6580,AIRFLOW,1579176730000,1579176798000,Bug,Open,Blocker,1,693,"Killing or marking a task as failed does not kill the Pod in the backend We're using KubernetesPodOperator in Airflow 1.10.2 The pods that we have some NodeAffinity and Tolerations in it. Sometimes the pod gets stuck at a Pending state. *But when the task fails, the Pending pod does not kill itself.* Related to this, when we manually fail a task, the DAG task stops running, but the Pod in the DAG does not get killed and continues running. We have tried setting the 'is_delete_operator_pod' to True. But for some reason the Pod gets killed almost instantly when the execution starts. We have not been able to debug the issue behind this. Does the latest version of Airflow account for this?"
142,AIRFLOW-6750,AIRFLOW,1581028381000,1581073839000,Bug,Resolved,Blocker,1,260,"Airflow breaks on start up after Werkzeug lib updated to 1.0.0 Hi there, The python package Werkzeug has just updated to version 1.0.0 which blocks Airflow Webserver and scheduler start up. Steps to reproduce: pip install apache-airflow==1.10.7 airflow version"
143,AIRFLOW-6778,AIRFLOW,1581459767000,1599216652000,Improvement,Open,Blocker,1,2336,"Add a DAGs PVC Mount Point Option for Workers under Kubernetes Executor The worker pods generated by the Kubernetes Executor force the DAGs PVC to be mounted at the Airflow DAGs folder. This, combined with a general inability to specify arbitrary PVCs on workers (see AIRFLOW-3126 and the linked/duplicated issues), severely constrains the usability of worker pods and the Kubernetes Executor as a whole. For example, if a DAGs-containing PVC is rooted at a Python package (e.g. {{package/}}) that needs to be installed on each worker (e.g. DAGs in {{package/dags/}}, package install point at {{package/setup.py}}, and Airflow DAGs location {{/airflow/dags}}), then the current static mount point logic will only allow a worker to directly mount the entire package into the Airflow DAGs location  while the actual DAGs are in a subdirectory  or exclusively mount the package's sub-path {{package/dags}} (using the existing {{kubernetes.dags_volume_subpath}} config option). While the latter is at least correct, it completely foregoes the required parent directory and it makes the requisite package unavailable for installation (e.g. the files under {{package/}} are not available). -In general, the only approach that seems to work for the Kubernetes Executor is to specify a worker image with all DAG dependencies pre-loaded, which largely voids the usefulness of a single DAGs PVC that can be dynamically updated. At best, one can include a {{requirements.txt}} in the PVC and use it in tandem with an entry-point script built into the image, but that still doesn't help with source installations of custom packages stored and updated in a PVC.- Edit: This isn't even possible, because worker pods are created using [the {{command}} field instead of {{args}}|https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes]! A quick fix for this situation is to allow one to specify the DAGs PVC mount point. With this option, one can mount the PVC anywhere and specify an Airflow DAGs location that works in conjunction with the mount point (e.g. mount the PVC at {{/airflow/package}} and independently set the Airflow DAGs location to {{/airflow/package/dags}}). This option would  in many cases  obviate the need for the marginally useful {{kubernetes.dags_volume_subpath}} options, as well."
144,AIRFLOW-6780,AIRFLOW,1581477615000,1592320277000,Bug,Open,Blocker,1,865,"Use args Instead of command in Kubernetes Executor Worker Pods The worker pods created by the Kubernetes Executor are hard-coded to use [the {{command}} field instead of {{args}}|[https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes]!|https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes]. The result is a worker pod that ignores its image's entry-point and prevents the standard means of executing container preparation code --- among other things. This --- alongside the issues stated in [AIRFLOW-6778|https://issues.apache.org/jira/browse/AIRFLOW-6778] and [AIRFLOW-3126|https://issues.apache.org/jira/browse/AIRFLOW-3126] --- renders worker nodes virtually incompatible with a dynamic, shared DAGs volume (unless dependencies are installed in the image and never change)."
145,AIRFLOW-6799,AIRFLOW,1581633049000,1584737901000,Bug,Closed,Blocker,1,1305,"webgui cannot display all tasks The we have ""to many"" task the graph rendering stops with an Edge 'undefined' is not in graph javascript error, There is no graph in the webgui. Lowering the number of task will enable the the rendering again. Examplecode: from airflow import DAG from airflow.models import DAG from airflow.operators.dummy_operator import DummyOperator from datetime import datetime from datetime import timedelta DAG_task_concurrency = 30 DAG_max_active_runs = 10 MAIN_DAG_ID = 'BUG_IN_GRAPH_DISPLAY' default_args = { 'owner':'prod', 'depends_on_past':False, 'email':['sln@fcoo.dk'], 'email_on_failure':False, 'email_on_retry':False, 'retries':3, 'retry_delay':timedelta(seconds=30), 'queue':'default'} BUG_DAG = DAG(MAIN_DAG_ID, default_args=default_args, catchup=False, orientation='LR', concurrency=DAG_task_concurrency, schedule_interval='@once', max_active_runs=DAG_max_active_runs, start_date=(datetime(2020, 2, 5)) ) # To many tasks max_task = 160 # 156 ok task_range = list(range(0, max_task + 1)) start_task = DummyOperator(task_id='start_task', dag=BUG_DAG) after_all_complete = DummyOperator(task_id='after_all_complete', dag=BUG_DAG) for task_step in task_range: task1 = DummyOperator(task_id='task_{0}'.format(task_step),dag=BUG_DAG) start_task >> task1 >> after_all_complete"
146,AIRFLOW-6806,AIRFLOW,1581671955000,1626772817000,Bug,Closed,Blocker,1,610,"Kubernetes Executor : Sometimes, task is succeeded, but pod is not completed ( keep running ) . Zombie pod is born. The task completes successfully, and the result is able to check on the UI. However, there are cases where the Pod that was processing a completed task is keep running and will not end forever. A zombie pod is born. Use Operator like this, default_pod = BashOperator( task_id=""test"", env=KubePodEnv, retries=1, xcom_push=True, bash_command=""python3 /opt/test.py "", executor_config={ ""KubernetesExecutor"": { ""volumes"": volumes, ""volume_mounts"": volume_mounts, ""affinity"":affinity, } }, dag=dag )"
147,AIRFLOW-6813,AIRFLOW,1581724500000,1600603116000,Bug,Open,Blocker,1,1833,"Error while running airflow resetdb with SQL Server as backend I am connecting my Airflow with the MS SQL Server backend. *airflow initdb* works fine. However when I do *airflow resetdb* I get the following error {{INFO [alembic.runtime.migration] Running upgrade 939bb1e647c8 -> 004c1210f153, increase queue name size limit}} {{INFO [alembic.runtime.migration] Running upgrade c8ffec048a3b -> a56c9515abdc, Remove dag_stat table}} {{INFO [alembic.runtime.migration] Running upgrade a56c9515abdc, 004c1210f153, 74effc47d867, b3b105409875 -> 08364691d074, Merge the four heads back together}} {{INFO [alembic.runtime.migration] Running upgrade 08364691d074 -> fe461863935f, increase_length_for_connection_password}} {{I}}{{NFO [alembic.runtime.migration] Running upgrade fe461863935f -> 7939bcff74ba, Add DagTags table}} {{WARNI [airflow.utils.log.logging_mixin.LoggingMixin] empty cryptography key - values will not be stored encrypted.}} {{Exception ignored in: <function _ConnectionRecord.checkout.<locals>.<lambda> at 0x7f657bf272f0>Traceback (most recent call last):File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/pool/base.py"", line 503, in <lambda>File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/pool/base.py"", line 710, in _finalize_fairy File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/pool/base.py"", line 528, in checkin File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/pool/base.py"", line 387, in _return_conn File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/pool/impl.py"", line 106, in _do_return_conn}} {{TypeError: catching classes that do not inherit from BaseException is not allowed}}"
148,AIRFLOW-6814,AIRFLOW,1581734522000,1584359299000,Bug,Resolved,Blocker,1,9645,"Airflow Scheduler fails for any dag with MS SQL server as the backend I am trying to execute airflow with MS SQL Server as the backend. Eventhough *airflow initdb* works fine, *airflow resetdb* throws an error for which I have already created an issue. After airflow initdb, with some dags in the dags folder, if I run the *airflow scheduler,* then I am getting the following error and it fails for every dag and all the time. I have tried with different (including latest) *pyodbc versions* Please find below the error. ERROR - Process timed out, PID: 94590 [2020-02-15 02:30:52,094] \{celery_executor.py:266} ERROR - Error fetching Celery task state, ignoring it:StatementError(""(builtins.SystemError) <class 'pyodbc.Error'> returned a result with an error set"",) Celery Task ID: ('simple_dag', 'print_date0', datetime.datetime(2020, 2, 14, 23, 0, tzinfo=<Timezone [UTC]>), 1) Traceback (most recent call last): File ""/home/ops/.pyenv/versions/3.6.10/lib/python3.6/encodings/utf_16_le.py"", line 15, in decode def decode(input, errors='strict'): File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/airflow/utils/timeout.py"", line 43, in handle_timeout raise AirflowTaskTimeout(self.error_message) airflow.exceptions.AirflowTaskTimeout: Timeout, PID: 94590 The above exception was the direct cause of the following exception: airflow.exceptions.AirflowTaskTimeout: decoding with 'utf-16le' codec failed (AirflowTaskTimeout: Timeout, PID: 94590) The above exception was the direct cause of the following exception: Traceback (most recent call last): File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1318, in _cursor_execute self.dialect.do_execute(cursor, statement, parameters, context) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 588, in do_execute cursor.execute(statement, parameters) SystemError: <class 'pyodbc.Error'> returned a result with an error set The above exception was the direct cause of the following exception: Traceback (most recent call last): File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/airflow/executors/celery_executor.py"", line 108, in fetch_celery_task_state res = (celery_task[0], celery_task[1].state) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/celery/result.py"", line 475, in state return self._get_task_meta()['status'] File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/celery/result.py"", line 414, in _get_task_meta return self._maybe_set_cache(self.backend.get_task_meta(self.id)) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/celery/backends/base.py"", line 451, in get_task_meta meta = self._get_task_meta_for(task_id) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/celery/backends/database/__init__.py"", line 51, in _inner return fun(*args, **kwargs) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/celery/backends/database/__init__.py"", line 151, in _get_task_meta_for session = self.ResultSession() File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/celery/backends/database/__init__.py"", line 110, in ResultSession **self.engine_options) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/celery/backends/database/session.py"", line 59, in session_factory self.prepare_models(engine) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/celery/backends/database/session.py"", line 54, in prepare_models ResultModelBase.metadata.create_all(engine) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/sql/schema.py"", line 4316, in create_all ddl.SchemaGenerator, self, checkfirst=checkfirst, tables=tables File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 2048, in _run_visitor with self._optional_conn_ctx_manager(connection) as conn: File ""/home/ops/.pyenv/versions/3.6.10/lib/python3.6/contextlib.py"", line 81, in __enter__ return next(self.gen) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 2040, in _optional_conn_ctx_manager with self._contextual_connect() as conn: File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 2242, in _contextual_connect self._wrap_pool_connect(self.pool.connect, None), File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 2276, in _wrap_pool_connect return fn() File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/pool/base.py"", line 363, in connect return _ConnectionFairy._checkout(self) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/pool/base.py"", line 774, in _checkout fairy = _ConnectionRecord.checkout(pool) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/pool/base.py"", line 492, in checkout rec = pool._do_get() File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/pool/impl.py"", line 139, in _do_get self._dec_overflow() File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py"", line 68, in __exit__ compat.reraise(exc_type, exc_value, exc_tb) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 153, in reraise raise value File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/pool/impl.py"", line 136, in _do_get return self._create_connection() File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/pool/base.py"", line 308, in _create_connection return _ConnectionRecord(self) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/pool/base.py"", line 437, in __init__ self.__connect(first_connect_check=True) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/pool/base.py"", line 662, in __connect ).exec_once_unless_exception(self.connection, self) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/event/attr.py"", line 314, in exec_once_unless_exception self._exec_once_impl(True, *args, **kw) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/event/attr.py"", line 285, in _exec_once_impl self(*args, **kw) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/event/attr.py"", line 322, in __call__ fn(*args, **kw) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py"", line 1485, in go return once_fn(*arg, **kw) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py"", line 199, in first_connect dialect.initialize(c) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/dialects/mssql/base.py"", line 2378, in initialize super(MSDialect, self).initialize(connection) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 318, in initialize self.returns_unicode_strings = self._check_unicode_returns(connection) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 415, in _check_unicode_returns results = \{check_unicode(test) for test in tests} File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 415, in <setcomp> results = \{check_unicode(test) for test in tests} File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 384, in check_unicode connection._cursor_execute(cursor, statement, parameters) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1321, in _cursor_execute e, statement, parameters, cursor, context File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1476, in _handle_dbapi_exception util.raise_from_cause(sqlalchemy_exception, exc_info) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 398, in raise_from_cause reraise(type(exception), exception, tb=exc_tb, cause=cause) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 152, in reraise raise value.with_traceback(tb) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1318, in _cursor_execute self.dialect.do_execute(cursor, statement, parameters, context) File ""/home/ops/.pyenv/versions/3.6.10/envs/airflow_env/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 588, in do_execute cursor.execute(statement, parameters) sqlalchemy.exc.StatementError: (builtins.SystemError) <class 'pyodbc.Error'> returned a result with an error set [SQL: SELECT CAST('test unicode returns' AS NVARCHAR(60)) AS anon_1]"
149,AIRFLOW-6815,AIRFLOW,1581764022000,1584358779000,Bug,Resolved,Blocker,1,1369,"Dag Graph View Javascript Error airflow does not display graph view after certain amount of parallel tasks, and gives javascript error in console. [fresh installed airflow|[https://airflow.apache.org/docs/stable/start.html#]] and added the dag in attachment [^my_dag.py] {code:java} dagre-d3.js:3704 Uncaught Error: Edge 'undefined' is not in graph at Constructor.BaseGraph._strictGetEdge (dagre-d3.js:3704) at Constructor.BaseGraph._delEdge (dagre-d3.js:3662) at Constructor.Digraph.delEdge (dagre-d3.js:4048) at Constructor.<anonymous> (dagre-d3.js:3633) at Array.forEach (<anonymous>) at Constructor.BaseGraph.delNode (dagre-d3.js:3633) at Constructor.Digraph.delNode (dagre-d3.js:4016) at Constructor.delNode (dagre-d3.js:4809) at dagre-d3.js:1551 at Constructor.BaseGraph.eachNode (dagre-d3.js:3585) BaseGraph._strictGetEdge @ dagre-d3.js:3704 BaseGraph._delEdge @ dagre-d3.js:3662 Digraph.delEdge @ dagre-d3.js:4048 (anonymous) @ dagre-d3.js:3633 BaseGraph.delNode @ dagre-d3.js:3633 Digraph.delNode @ dagre-d3.js:4016 Constructor.delNode @ dagre-d3.js:4809 (anonymous) @ dagre-d3.js:1551 BaseGraph.eachNode @ dagre-d3.js:3585 undoNormalize @ dagre-d3.js:1541 (anonymous) @ dagre-d3.js:3466 run @ dagre-d3.js:1477 (anonymous) @ dagre-d3.js:3466 runLayout @ dagre-d3.js:268 Renderer.run @ dagre-d3.js:202 (anonymous) @ graph?dag_id=my_dag&execution_date=:949{code}"
150,AIRFLOW-6825,AIRFLOW,1581943948000,1582530204000,Bug,Closed,Blocker,1,168,when build from source and use rbac=True UI is stuck When we build airflow 1.10.7 from source and run using rbac = True option UI is stuck and an't go to any view tabs.
151,AIRFLOW-6960,AIRFLOW,1582993152000,1583562209000,Bug,Open,Blocker,1,7193,"Airflow Celery worker : command returned non-zero exit status 2 I am getting below error and trying to fix it for hours and did get any luck. Below logs are from airflow celery worker. {code:java} airflow command error: argument subcommand: invalid choice: 'tasks' (choose from 'backfill', 'list_dag_runs', 'list_tasks', 'clear', 'pause', 'unpause', 'trigger_dag', 'delete_dag', 'show_dag', 'pool', 'variables', 'kerberos', 'render', 'run', 'initdb', 'list_dags', 'dag_state', 'task_failed_deps', 'task_state', 'serve_logs', 'test', 'webserver', 'resetdb', 'upgradedb', 'checkdb', 'shell', 'scheduler', 'worker', 'flower', 'version', 'connections', 'create_user', 'delete_user', 'list_users', 'sync_perm', 'next_execution', 'rotate_fernet_key'), see help above. usage: airflow [-h] {backfill,list_dag_runs,list_tasks,clear,pause,unpause,trigger_dag,delete_dag,show_dag,pool,variables,kerberos,render,run,initdb,list_dags,dag_state,task_failed_deps,task_state,serve_logs,test,webserver,resetdb,upgradedb,checkdb,shell,scheduler,worker,flower,version,connections,create_user,delete_user,list_users,sync_perm,next_execution,rotate_fernet_key} ... positional arguments: {backfill,list_dag_runs,list_tasks,clear,pause,unpause,trigger_dag,delete_dag,show_dag,pool,variables,kerberos,render,run,initdb,list_dags,dag_state,task_failed_deps,task_state,serve_logs,test,webserver,resetdb,upgradedb,checkdb,shell,scheduler,worker,flower,version,connections,create_user,delete_user,list_users,sync_perm,next_execution,rotate_fernet_key} sub-command help backfill Run subsections of a DAG for a specified date range. If reset_dag_run option is used, backfill will first prompt users whether airflow should clear all the previous dag_run and task_instances within the backfill date range. If rerun_failed_tasks is used, backfill will auto re-run the previous failed task instances within the backfill date range. list_dag_runs List dag runs given a DAG id. If state option is given, it will onlysearch for all the dagruns with the given state. If no_backfill option is given, it will filter outall backfill dagruns for given dag id. list_tasks List the tasks within a DAG clear Clear a set of task instance, as if they never ran pause Pause a DAG unpause Resume a paused DAG trigger_dag Trigger a DAG run delete_dag Delete all DB records related to the specified DAG show_dag Displays DAG's tasks with their dependencies pool CRUD operations on pools variables CRUD operations on variables kerberos Start a kerberos ticket renewer render Render a task instance's template(s) run Run a single task instance initdb Initialize the metadata database list_dags List all the DAGs dag_state Get the status of a dag run task_failed_deps Returns the unmet dependencies for a task instance from the perspective of the scheduler. In other words, why a task instance doesn't get scheduled and then queued by the scheduler, and then run by an executor). task_state Get the status of a task instance serve_logs Serve logs generate by worker test Test a task instance. This will run a task without checking for dependencies or recording its state in the database. webserver Start a Airflow webserver instance resetdb Burn down and rebuild the metadata database upgradedb Upgrade the metadata database to latest version checkdb Check if the database can be reached. shell Runs a shell to access the database scheduler Start a scheduler instance worker Start a Celery worker node flower Start a Celery Flower version Show the version connections List/Add/Delete connections create_user Create an account for the Web UI (FAB-based) delete_user Delete an account for the Web UI list_users List accounts for the Web UI sync_perm Update permissions for existing roles and DAGs. next_execution Get the next execution datetime of a DAG. rotate_fernet_key Rotate all encrypted connection credentials and variables; see https://airflow.readthedocs.io/en/stable/howto/secure- connections.html#rotating-encryption-keys. optional arguments: -h, --help show this help message and exit airflow command error: argument subcommand: invalid choice: 'tasks' (choose from 'backfill', 'list_dag_runs', 'list_tasks', 'clear', 'pause', 'unpause', 'trigger_dag', 'delete_dag', 'show_dag', 'pool', 'variables', 'kerberos', 'render', 'run', 'initdb', 'list_dags', 'dag_state', 'task_failed_deps', 'task_state', 'serve_logs', 'test', 'webserver', 'resetdb', 'upgradedb', 'checkdb', 'shell', 'scheduler', 'worker', 'flower', 'version', 'connections', 'create_user', 'delete_user', 'list_users', 'sync_perm', 'next_execution', 'rotate_fernet_key'), see help above. [2020-03-01 00:11:41,941: ERROR/ForkPoolWorker-8] execute_command encountered a CalledProcessError Traceback (most recent call last): File ""/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/airflow/executors/celery_executor.py"", line 69, in execute_command close_fds=True, env=env) File ""/opt/rh/rh-python36/root/usr/lib64/python3.6/subprocess.py"", line 311, in check_call raise CalledProcessError(retcode, cmd) subprocess.CalledProcessError: Command '['airflow', 'tasks', 'run', 'airflow_worker_check_pipeline', 'dev_couchbase_backup', '2020-02-29T14:47:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/root/airflow/dags/project1/airflow_worker_check_pipeline.py']' returned non-zero exit status 2. [2020-03-01 00:11:41,941: ERROR/ForkPoolWorker-8] None [2020-03-01 00:11:41,996: ERROR/ForkPoolWorker-8] Task airflow.executors.celery_executor.execute_command[0e0c3d02-bdb3-4d16-a863-cbb3bb7a7137] raised unexpected: AirflowException('Celery command failed',) Traceback (most recent call last): File ""/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/airflow/executors/celery_executor.py"", line 69, in execute_command close_fds=True, env=env) File ""/opt/rh/rh-python36/root/usr/lib64/python3.6/subprocess.py"", line 311, in check_call raise CalledProcessError(retcode, cmd) subprocess.CalledProcessError: Command '['airflow', 'tasks', 'run', 'airflow_worker_check_pipeline', 'dev_couchbase_backup', '2020-02-29T14:47:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/root/airflow/dags/project1/airflow_worker_check_pipeline.py']' returned non-zero exit status 2. During handling of the above exception, another exception occurred: {code} *_Airflow Scheduler & Mater versions : v2.0.0.dev0 docker platform (Image -->apache/airflow master-ci)_* *_Airflow Worker Versions : v1.10.9 (manual install/non docker platform)_* I suspect that the could be due to version mismatch and I tried to update the airflow worker version, but unfortunately I could not find that version {code:java} ERROR: Could not find a version that satisfies the requirement apache-airflow[celery]=={v2.0.0} (from versions: **1.10.9-bin, 1.8.1, 1.8.2rc1, 1.8.2, 1.9.0, 1.10.0, 1.10.1b1, 1.10.1rc2, 1.10.1, 1.10.2b2, 1.10.2rc1, 1.10.2rc2, 1.10.2rc3, 1.10.2, 1.10.3b1, 1.10.3b2, 1.10.3rc1, 1.10.3rc2, 1.10.3, 1.10.4b2, 1.10.4rc1, 1.10.4rc2, 1.10.4rc3, 1.10.4rc4, 1.10.4rc5, 1.10.4, 1.10.5rc1, 1.10.5, 1.10.6rc1, 1.10.6rc2, 1.10.6, 1.10.7rc1, 1.10.7rc2, 1.10.7rc3, 1.10.7, 1.10.8rc1, 1.10.8, 1.10.9rc1, 1.10.9**) ERROR: No matching distribution found for apache-airflow[celery]=={v2.0.0} {code}"
152,AIRFLOW-7027,AIRFLOW,1583837157000,1583983237000,Bug,Open,Blocker,1,1202,"The mirrored data folder for BigQuery_operators can't be accessed on manual runs Using Airflow through the Google Cloud Composer, I've placed a series of text files in the /data folder of the bucket as suggested in the documentation for where to store external data files and have written a BigQueryOperator of the following format: {{ with open('/home/airflow/gcs/data/{0}.txt'.format(models.Variable.get('tmpcreatives')),'r') as tmp_file: tmp_transfer = tmp_file.read() bq_sql_tmptransfer = bigquery_operator.BigQueryOperator( task_id = 'task1', sql = """""" {0} """""".format(tmp_transfer.format(tradata = dag.params[""ClientDatabase""]+dag.params[""bq_param1""],rawdata = dag.params[""ClientDatabase""]+dag.params[""bq_param2""])), use_legacy_sql = False ) }} On scheduled runs, the DAG run's fine and completes the task, however if I try to manually trigger the DAG or look at the run logs it comes up with the message 'DAG ""DataCreation_DAG_"" seems to be missing' This is only an issue when I use the open() function, if I replace that section with a hardcoded string then the DAG works fine even on manual runs, I think it's a bug with mounting the /data file from the cloud shell bucket but not entirely sure"
153,AIRFLOW-7043,AIRFLOW,1583983237000,1583983395000,Bug,In Progress,Blocker,1,1210,"CLONE - The mirrored data folder for BigQuery_operators can't be accessed on manual runs Using Airflow through the Google Cloud Composer, I've placed a series of text files in the /data folder of the bucket as suggested in the documentation for where to store external data files and have written a BigQueryOperator of the following format: {{ with open('/home/airflow/gcs/data/{0}.txt'.format(models.Variable.get('tmpcreatives')),'r') as tmp_file: tmp_transfer = tmp_file.read() bq_sql_tmptransfer = bigquery_operator.BigQueryOperator( task_id = 'task1', sql = """""" {0} """""".format(tmp_transfer.format(tradata = dag.params[""ClientDatabase""]+dag.params[""bq_param1""],rawdata = dag.params[""ClientDatabase""]+dag.params[""bq_param2""])), use_legacy_sql = False ) }} On scheduled runs, the DAG run's fine and completes the task, however if I try to manually trigger the DAG or look at the run logs it comes up with the message 'DAG ""DataCreation_DAG_"" seems to be missing' This is only an issue when I use the open() function, if I replace that section with a hardcoded string then the DAG works fine even on manual runs, I think it's a bug with mounting the /data file from the cloud shell bucket but not entirely sure"
154,AIRFLOW-7120,AIRFLOW,1588076064000,1588076106000,Bug,Open,Blocker,1,418,"WE HAVE MOVED TO GITHUB ISSUE Hello, We no longer use Jira for new tickets. Please, use Github Issue. https://github.com/apache/airflow/issues Jira is just an archive for old tickets. You can no longer create new tickets. You can still comment on old tickets, but you can't expect a response. If the ticket is important to you, please create a new ticket in Github Issue and give the link to the Jira ticket. Big love,"
155,AIRFLOW-18,AIRFLOW,1461868739000,1585329353000,Improvement,Closed,Critical,2,438,"Alembic's constraints and indexes are unnamed thus hard to drop or change Eg. in XXX_add_dagrun.py the constraint is added without a name: sa.UniqueConstraint('dag_id', 'execution_date'), This makes constraint naming database specific, ie. postgres' name for the constraint be different than mysql's and sqllite's. Best practice per http://alembic.readthedocs.io/en/latest/naming.html is to have naming conventions that are being applied."
156,AIRFLOW-37,AIRFLOW,1462236482000,1462392525000,Bug,Closed,Critical,2,2747,"""No such transport"" errors when using CeleryExecutor Airflow functions fine when using LocalExecutor on sample DAGs and ones I have made. When I switch to CeleryExecutor, I get the following stack trace from ""airflow scheduler"": {code:none} [2016-05-03 00:23:15,825] {celery_executor.py:62} INFO - [celery] queuing ('example_branch_operator', 'run_this_first', datetime.datetime(2016, 4, 27, 0, 0)) through c elery, queue=default [2016-05-03 00:23:15,827] {jobs.py:660} ERROR - 'No such transport: ' Traceback (most recent call last): File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs.py"", line 657, in _execute executor.heartbeat() File ""/usr/local/lib/python2.7/dist-packages/airflow/executors/base_executor.py"", line 86, in heartbeat self.execute_async(key, command=command, queue=queue) File ""/usr/local/lib/python2.7/dist-packages/airflow/executors/celery_executor.py"", line 64, in execute_async args=[command], queue=queue) File ""/usr/local/lib/python2.7/dist-packages/celery/app/task.py"", line 565, in apply_async **dict(self._get_exec_options(), **options) File ""/usr/local/lib/python2.7/dist-packages/celery/app/base.py"", line 348, in send_task with self.producer_or_acquire(producer) as P: File ""/usr/local/lib/python2.7/dist-packages/celery/app/base.py"", line 403, in producer_or_acquire producer, self.amqp.producer_pool.acquire, block=True, File ""/usr/local/lib/python2.7/dist-packages/celery/app/amqp.py"", line 502, in producer_pool self.app.pool, File ""/usr/local/lib/python2.7/dist-packages/celery/app/base.py"", line 609, in pool self._pool = self.connection().Pool(limit=limit) File ""/usr/local/lib/python2.7/dist-packages/celery/app/base.py"", line 386, in connection 'BROKER_CONNECTION_TIMEOUT', connect_timeout File ""/usr/local/lib/python2.7/dist-packages/kombu/connection.py"", line 165, in __init__ if not get_transport_cls(transport).can_parse_url: File ""/usr/local/lib/python2.7/dist-packages/kombu/transport/__init__.py"", line 109, in get_transport_cls _transport_cache[transport] = resolve_transport(transport) File ""/usr/local/lib/python2.7/dist-packages/kombu/transport/__init__.py"", line 89, in resolve_transport raise KeyError('No such transport: {0}'.format(transport)) KeyError: 'No such transport: ' {code} My Celery airflow.cfg is setup as follows: {code:none} [celery] celery_app_name = airflow.executors.celery_executor celeryd_concurrency = 16 worker_log_server_port = 8793 broker_url = 'redis://localhost:6379/0' celery_result_backend = 'redis://localhost:6379/0' flower_port = 5555 {code} Redis / Celery appear to be working on their own, as I can successfully execute the example Celery application here: http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html#application"
157,AIRFLOW-43,AIRFLOW,1462303858000,1534934640000,Bug,Closed,Critical,2,705,"Tasks never run in externally triggered DAGRun I have a DAG which should only be run as needed (no schedule interval): dag = DAG( dag_id='major_integ_cycle', default_args=args, schedule_interval=None, ) I attempted to start this using `airflow trigger_dag NAME` but the tasks never run. The scheduler has noted the fact that itthe DAG was triggered: INFO - Created <DagRun major_integ_cycle @ 2016-05-03 08:36:16.032474: manual__2016-05-03T08:36:16.032474, externally triggered: True> The tasks are currently very vanilla test cases, all look like this: task_mdc = op.BashOperator( task_id='master_data_cycle', bash_command='sleep 1', dag=dag, ) Have I misunderstood? Or is not running a bug in this case?"
158,AIRFLOW-92,AIRFLOW,1462878244000,1462900771000,Bug,Closed,Critical,2,3986,"Tasks not being retried at all due to a 'obj not bound to a Session' exception I have some tasks that are stuck in {{up_for_retry}} state, below is an extract from the database. (here it is in a [Google Drive spreadsheet|https://docs.google.com/spreadsheets/d/14dtb3zYa583V1SaLcpOq6hDM4ThCeN7JhHjftRwKxbI/edit?usp=sharing] with better formatting) {code} task_id dag_id execution_date start_date end_date duration state try_number hostname unixname job_id pool queue priority_weight operator queued_dttm id dag_id state job_type start_date end_date latest_heartbeat executor_class hostname unixname id dag_id execution_date state run_id external_trigger conf end_date start_date task_a dag_a1 2016-05-09 08:00:00.000000 2016-05-09 12:00:12.382775 2016-05-09 12:01:12.473914 60.091139 up_for_retry 1 d5593c115c22 root 46266 default 4 ExternalTaskSensor 46266 success LocalTaskJob 2016-05-09 12:00:08.195711 2016-05-09 12:01:13.261937 2016-05-09 12:00:08.195732 LocalExecutor d5593c115c22 root 17799 dag_a1 2016-05-09 08:00:00.000000 failed scheduled__2016-05-09T08:00:00 false 2016-05-09 12:00:04.406875 task_a dag_a2 2016-05-09 10:00:00.000000 2016-05-09 12:00:13.102094 2016-05-09 12:01:13.185960 60.083866 up_for_retry 1 d5593c115c22 root 46270 default 4 ExternalTaskSensor 46270 success LocalTaskJob 2016-05-09 12:00:08.896527 2016-05-09 12:01:13.960936 2016-05-09 12:00:08.896550 LocalExecutor d5593c115c22 root 17800 dag_a2 2016-05-09 10:00:00.000000 failed scheduled__2016-05-09T10:00:00 false 2016-05-09 12:00:04.531888 task_b dag_b 2016-04-07 18:00:00.000000 2016-05-09 12:53:59.990395 2016-05-09 12:54:00.393259 0.402864 up_for_retry 1 0a8613c2b5d2 root 46366 default 1 PostgresOperator 46366 success LocalTaskJob 2016-05-09 12:53:58.881987 2016-05-09 12:54:03.891450 2016-05-09 12:53:58.882006 LocalExecutor 0a8613c2b5d2 root 17836 dag_b 2016-04-07 18:00:00.000000 running scheduled__2016-04-07T18:00:00 false 2016-05-09 12:51:59.713718 task_c dag_b 2016-04-07 16:00:00.000000 2016-05-09 12:53:49.822634 2016-05-09 12:54:49.924291 60.101657 up_for_retry 1 0a8613c2b5d2 root 46359 default 2 ExternalTaskSensor 46359 success LocalTaskJob 2016-05-09 12:53:44.739355 2016-05-09 12:54:54.810579 2016-05-09 12:53:44.739575 LocalExecutor 0a8613c2b5d2 root 17831 dag_b 2016-04-07 16:00:00.000000 running scheduled__2016-04-07T16:00:00 false 2016-05-09 12:51:55.078050 {code} I'm getting the following exception which seems to be halting the scheduler just before it could queue the tasks for retrying: {code} [2016-05-10 09:42:33,562] {jobs.py:706} ERROR - Instance <DagRun at 0x7f48a6b87550> is not bound to a Session; attribute refresh operation cannot proceed Traceback (most recent call last): File ""/usr/local/lib/python3.5/site-packages/airflow/jobs.py"", line 703, in _do_dags self.process_dag(dag, tis_out) File ""/usr/local/lib/python3.5/site-packages/airflow/jobs.py"", line 507, in process_dag active_runs = dag.get_active_runs() File ""/usr/local/lib/python3.5/site-packages/airflow/models.py"", line 2731, in get_active_runs active_dates.append(run.execution_date) File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/attributes.py"", line 237, in __get__ return self.impl.get(instance_state(instance), dict_) File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/attributes.py"", line 578, in get value = state._load_expired(state, passive) File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/state.py"", line 474, in _load_expired self.manager.deferred_scalar_loader(self, toload) File ""/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/loading.py"", line 610, in load_scalar_attributes (state_str(state))) sqlalchemy.orm.exc.DetachedInstanceError: Instance <DagRun at 0x7f48a6b87550> is not bound to a Session; attribute refresh operation cannot proceed {code} I've managed to fix this by removing all {{ti.are_dependencies_met()}} calls which have a commit at the end; after doing this there's no exceptions and the tasks are getting retried correctly."
159,AIRFLOW-237,AIRFLOW,1465847698000,1465903543000,Bug,Resolved,Critical,2,2286,"AttributeError: type object 'TaskInstance' has no attribute 'log' After following the ""Quick Start"" instructions found at `http://pythonhosted.org/airflow/start.html`, executing the command `airflow webserver -p 8080` results in the following error: ``` [2016-06-13 19:13:33,536] {__init__.py:36} INFO - Using executor SequentialExecutor [2016-06-13 19:13:33,649] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt [2016-06-13 19:13:33,669] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ [2016-06-13 19:13:33,875] {models.py:154} INFO - Filling up the DagBag from /home/vagrant/airflow/dags Traceback (most recent call last): File ""/usr/local/bin/airflow"", line 15, in <module> args.func(args) File ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 393, in webserver app = cached_app(conf) File ""/usr/local/lib/python2.7/dist-packages/airflow/www/app.py"", line 133, in cached_app app = create_app(config) File ""/usr/local/lib/python2.7/dist-packages/airflow/www/app.py"", line 75, in create_app Session, name=""Task Instances"", category=""Browse"")) File ""/usr/local/lib/python2.7/dist-packages/flask_admin/contrib/sqla/view.py"", line 318, in __init__ menu_icon_value=menu_icon_value) File ""/usr/local/lib/python2.7/dist-packages/flask_admin/model/base.py"", line 771, in __init__ self._refresh_cache() File ""/usr/local/lib/python2.7/dist-packages/flask_admin/model/base.py"", line 847, in _refresh_cache self._list_columns = self.get_list_columns() File ""/usr/local/lib/python2.7/dist-packages/flask_admin/model/base.py"", line 980, in get_list_columns excluded_columns=self.column_exclude_list, File ""/usr/local/lib/python2.7/dist-packages/flask_admin/contrib/sqla/view.py"", line 517, in get_column_names column, path = tools.get_field_with_path(self.model, c) File ""/usr/local/lib/python2.7/dist-packages/flask_admin/contrib/sqla/tools.py"", line 144, in get_field_with_path value = getattr(current_model, attribute) AttributeError: type object 'TaskInstance' has no attribute 'log' ```"
160,AIRFLOW-257,AIRFLOW,1466211686000,1466440899000,Bug,Closed,Critical,2,1344,"airflow command fails with ""ImportError: No module named zope.deprecation"" After AIRFLOW-31 has been merged, airflow command fails as follows: {code} $ airflow webserver [2016-06-18 00:56:50,367] {__init__.py:36} INFO - Using executor SequentialExecutor [2016-06-18 00:56:50,492] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt [2016-06-18 00:56:50,529] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt Traceback (most recent call last): File ""/home/sekikn/.virtualenvs/e/bin/airflow"", line 6, in <module> exec(compile(open(__file__).read(), __file__, 'exec')) File ""/home/sekikn/dev/incubator-airflow/airflow/bin/airflow"", line 4, in <module> from airflow import configuration File ""/home/sekikn/dev/incubator-airflow/airflow/__init__.py"", line 76, in <module> from airflow import operators File ""/home/sekikn/dev/incubator-airflow/airflow/operators/__init__.py"", line 24, in <module> from .check_operator import ( File ""/home/sekikn/dev/incubator-airflow/airflow/operators/check_operator.py"", line 20, in <module> from airflow.hooks import BaseHook File ""/home/sekikn/dev/incubator-airflow/airflow/hooks/__init__.py"", line 66, in <module> from zope.deprecation import deprecated as _deprecated ImportError: No module named zope.deprecation {code}"
161,AIRFLOW-325,AIRFLOW,1468273294000,1561109286000,Task,Closed,Critical,2,459,"[Airflow] dependency on previous task Hi Maxime, I am using depends_on_past = True in a DAG. But I need a capability to run the task not only when previous task is succeeded but also failed. Could you please help me achieve this? Note: I need dependency on previous task, wanted to start current task only when succeeded or failed. But now depends_on_past = True helps me depend only on previous task's success state. I appreciate any help here. Thanks Vinod."
162,AIRFLOW-382,AIRFLOW,1469767031000,1585352346000,New Feature,Closed,Critical,2,126,YarnExecutor I am looking for YarnExecutor support. I wanted to see what support there is for this and if it's on the roadmap.
163,AIRFLOW-420,AIRFLOW,1470956909000,1579476987000,Improvement,Closed,Critical,2,1075,"Airflow allows passing of parameters to tasks in ""test"" mode. It should allow in ""run"" mode too Hey guys, I'm doing a POC on Airflow. Felt pretty excited initially about its capabilities after trying out Luigi. But I'm puzzled as to why passing parameters is available in ""test"" mode but not in ""run"" mode. As a background, I'm not evaluating Airflow for any ETL or other data related purposes. I need to setup a task that does a few different things based on parameters passed. I want to be able to queue multiple instances of this task (with different input params) to run on different dates/times e.g. sending out custom emails. But I also have a different task that needs to clear/skip certain instances of this task that may still be in the queue. They need to be cleared based on something that might happen between now and the execution time of those tasks. I was thinking earlier that it may be accomplished by assigning task ids dynamically and passing parameters. But looks like neither of those options are available at this time. Will appreciate any input. Thanks"
164,AIRFLOW-470,AIRFLOW,1472546104000,1559621552000,Bug,Resolved,Critical,2,1304,"Frequent multiple dispatching of the same task to celery We are seeing a lot of frequent dispatching of the same task to celery within a very short time frame (same task instance by Airflow conditions, but a different celery task uuid), which is causing a lot of unexpected behavior for us. Most of these are annoying but harmless  sometimes they clear xcom data and overwrite logs, but for the most part they are able to rely on the db metadata and not try to run itself multiple times. We are seeing this behavior frequent, some tasks are getting scheduled 5 times within the span of two minutes. The issue seems to be exacerbated by the use of pools. We have even seen the same task being dispatched twice within a second apart, causing real race conditions because the second try didn't see the task instance starting to run yet in the metadata db. It seems from other issues submitted here that people definitely see problems with the same tasks running multiple times, but this problem seems to be getting worse for us. Is it a known issue for the multiple dispatching to be so frequent/severe? (or maybe even the intentional design/side effect?) Are there things that we could be doing that might make this worse? (One of our primary suspect is the scheduler, which we have set its num_runs to 1)"
165,AIRFLOW-471,AIRFLOW,1472546119000,1559621552000,Bug,Open,Critical,2,1549,"Frequent multiple dispatching of the same task to celery We are seeing a lot of frequent dispatching of the same task to celery within a very short time frame (same task instance by Airflow conditions, but a different celery task uuid), which is causing a lot of unexpected behavior for us. Most of these are annoying but harmless  sometimes they clear xcom data and overwrite logs, but for the most part they are able to rely on the db metadata and not try to run itself multiple times. We are seeing this behavior frequently, with some tasks getting scheduled 5 times within the span of two minutes. The issue seems to be exacerbated by the use of pools though we don't have fully corroborated evidence of that yet. We have even seen the same task being dispatched twice within a second apart, causing real race conditions because the second try didn't see the task instance starting to run yet in the metadata db. It seems from other issues submitted here that people definitely see problems with the same tasks running multiple times, but this problem seems to be getting worse/particular bad for our usage. Is it a known issue for the multiple dispatching to be so frequent/severe? (or maybe even the intentional design/side effect?) Are there things that we could be doing that might make this worse? (One of our primary suspect is the scheduler, which we have set its num_runs to 1) [I blurred out the task names, because its potentially sensitive to what we do, but please trust that its the same ""airflow task instance"" being triggered :)]"
166,AIRFLOW-492,AIRFLOW,1473282451000,1494351356000,Bug,Closed,Critical,2,4873,"Insert into dag_stats table results into failed task while task itself succeeded In some occasions there seem to be a duplicate key being inserted in dag_stats that results in a task/dag run being marked failed while the task itself has succeeded. [2016-09-07 18:44:16,940] {models.py:3912} INFO - Marking run <DagRun hanging_subdags_n16_sqe.level_2_14 @ 2016-04-21 00:00:00: backfill_2016-04-21T00:00:00, externally triggered: False> successful [2016-09-07 18:44:17,671] {models.py:1450} ERROR - (_mysql_exceptions.IntegrityError) (1062, ""Duplicate entry 'hanging_subdags_n16_sqe.level_2_14-success' for key 'PRIMARY'"") [SQL: u'INSERT INTO dag_stats (dag_id, state, count, dirty) VALUES (%s, %s, %s, %s)'] [parameters: ('hanging_subdags_n16_sqe.level_2_14', 'success', 3L, 0)] Traceback (most recent call last): File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.1.3-py2.7.egg/airflow/models.py"", line 1409, in run result = task_copy.execute(context=context) File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.1.3-py2.7.egg/airflow/operators/subdag_operator.py"", line 88, in execute executor=self.executor) File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.1.3-py2.7.egg/airflow/models.py"", line 3244, in run job.run() File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.1.3-py2.7.egg/airflow/jobs.py"", line 189, in run self._execute() File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.1.3-py2.7.egg/airflow/jobs.py"", line 1855, in _execute models.DagStat.clean_dirty([run.dag_id], session=session) File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.1.3-py2.7.egg/airflow/utils/db.py"", line 54, in wrapper result = func(*args, **kwargs) File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.1.3-py2.7.egg/airflow/models.py"", line 3695, in clean_dirty session.commit() File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 801, in commit self.transaction.commit() File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 392, in commit self._prepare_impl() File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 372, in _prepare_impl self.session.flush() File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 2019, in flush self._flush(objects) File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 2137, in _flush transaction.rollback(_capture_exception=True) File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/util/langhelpers.py"", line 60, in __exit__ compat.reraise(exc_type, exc_value, exc_tb) File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 2101, in _flush flush_context.execute() File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/unitofwork.py"", line 373, in execute rec.execute(self) File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/unitofwork.py"", line 532, in execute uow File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/persistence.py"", line 174, in save_obj mapper, table, insert) File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/persistence.py"", line 767, in _emit_insert_statements execute(statement, multiparams) File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 914, in execute return meth(self, multiparams, params) File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/sql/elements.py"", line 323, in _execute_on_connection return connection._execute_clauseelement(self, multiparams, params) File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1010, in _execute_clauseelement compiled_sql, distilled_params File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1146, in _execute_context context) File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1341, in _handle_dbapi_exception exc_info File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/util/compat.py"", line 200, in raise_from_cause reraise(type(exception), exception, tb=exc_tb, cause=cause) File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1139, in _execute_context context) File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/default.py"", line 450, in do_execute cursor.execute(statement, parameters) File ""/usr/local/lib/python2.7/dist-packages/MySQLdb/cursors.py"", line 226, in execute self.errorhandler(self, exc, value) File ""/usr/local/lib/python2.7/dist-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler raise errorvalue IntegrityError: (_mysql_exceptions.IntegrityError) (1062, ""Duplicate entry 'hanging_subdags_n16_sqe.level_2_14-success' for key 'PRIMARY'"") [SQL: u'INSERT INTO dag_stats (dag_id, state, count, dirty) VALUES (%s, %s, %s, %s)'] [parameters: ('hanging_subdags_n16_sqe.level_2_14', 'success', 3L, 0)] [2016-09-07 18:44:17,787] {models.py:1473} INFO - Marking task as FAILED."
167,AIRFLOW-503,AIRFLOW,1473584542000,1559208243000,Bug,Closed,Critical,2,3938,"ExternalTaskSensor causes runtime exception I just created a new task using ExternalTaskSensor between weekly dag and daily dag (named 'services_daily_sensor') . When I tried to test it, i ran the command: 'airflow test weekly_agg services_daily_sensor 2016-09-11T06:00:00'. The command failed with the following error: ervices_daily_sensor> on 2016-09-11 06:00:00 [2016-09-11 08:59:09,602] {sensors.py:195} INFO - Poking for daily_agg.services_daily_task on 2016-09-11 02:00:00 ... [2016-09-11 08:59:09,614] {models.py:1286} ERROR - (psycopg2.ProgrammingError) can't adapt type 'builtin_function_or_method' [SQL: 'SELECT count(*) AS count_1 \nFROM (SELECT task_instance.task_id AS task_instance_task_id, task_instance.dag_id AS task_instance_dag_id, task_instance.execution_date AS task_instance_execution_date, task_instance.start_date AS task_instance_start_date, task_instance.end_date AS task_instance_end_date, task_instance.duration AS task_instance_duration, task_instance.state AS task_instance_state, task_instance.try_number AS task_instance_try_number, task_instance.hostname AS task_instance_hostname, task_instance.unixname AS task_instance_unixname, task_instance.job_id AS task_instance_job_id, task_instance.pool AS task_instance_pool, task_instance.queue AS task_instance_queue, task_instance.priority_weight AS task_instance_priority_weight, task_instance.operator AS task_instance_operator, task_instance.queued_dttm AS task_instance_queued_dttm \nFROM task_instance \nWHERE task_instance.dag_id = %(dag_id_1)s AND task_instance.task_id = %(task_id_1)s AND task_instance.state IN (%(state_1)s) AND task_instance.execution_date = %(execution_date_1)s) AS anon_1'] [parameters: {'state_1': <built-in function all>, 'execution_date_1': datetime.datetime(2016, 9, 11, 2, 0), 'dag_id_1': 'daily_agg', 'task_id_1': 'services_daily_task'}] Traceback (most recent call last): File ""/usr/lib/python2.7/site-packages/airflow/models.py"", line 1242, in run result = task_copy.execute(context=context) File ""/usr/lib/python2.7/site-packages/airflow/operators/sensors.py"", line 56, in execute while not self.poke(context): File ""/usr/lib/python2.7/site-packages/airflow/operators/sensors.py"", line 203, in poke TI.execution_date == dttm, File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2980, in count return self.from_self(col).scalar() File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2749, in scalar ret = self.one() File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2718, in one ret = list(self) File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2761, in __iter__ return self._execute_and_instances(context) File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2776, in _execute_and_instances result = conn.execute(querycontext.statement, self._params) File ""/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py"", line 914, in execute return meth(self, multiparams, params) File ""/usr/lib64/python2.7/site-packages/sqlalchemy/sql/elements.py"", line 323, in _execute_on_connection return connection._execute_clauseelement(self, multiparams, params) File ""/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1010, in _execute_clauseelement compiled_sql, distilled_params File ""/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1146, in _execute_context context) File ""/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1341, in _handle_dbapi_exception exc_info File ""/usr/lib64/python2.7/site-packages/sqlalchemy/util/compat.py"", line 202, in raise_from_cause reraise(type(exception), exception, tb=exc_tb, cause=cause) File ""/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1139, in _execute_context context) File ""/usr/lib64/python2.7/site-packages/sqlalchemy/engine/default.py"", line 450, in do_execute cursor.execute(statement, parameters)"
168,AIRFLOW-504,AIRFLOW,1473596636000,1537640836000,Bug,Closed,Critical,2,413,"datetime.now() has microseconds Mysql does not store these resulting in issues >>> datetime.datetime.utcnow() datetime.datetime(2016, 9, 11, 12, 22, 22, 483998) >>> datetime.datetime.now() datetime.datetime(2016, 9, 11, 14, 22, 31, 363392) >>> Mysql rounds those to ""2016-05-13 00:00:00"". If sqlalchemy now tries to find the the record by primary key (eg. session.merge) it cannot find them due to lost precision."
169,AIRFLOW-550,AIRFLOW,1475695037000,1483005365000,Bug,Resolved,Critical,2,336,SSL config has the potential of not starting up the webserver Although it's not consistent but [AIRFLOW-91] does brake some configurations of airflow. It seems that the default for cert/key are empty string so when checking against None it still returns True and throws and error. Change the config check with the standard falsly check.
170,AIRFLOW-563,AIRFLOW,1476171459000,1484623429000,Bug,Closed,Critical,2,442,"Dag stuck in ""Filling up the DagBag from .."" state Hello, I have scheduled a dag hourly but the task doesn't run at all, it stays at ""Filling up the DagBag from .."" step and each hour I have one more Dag id in running state (stuck aswell). Do you have any idea where does the problem comes from ? You'll find attached screens, log of the dag for 1 run and the dag file. If you need anything else feel free to ask. Thanks Best regards, Vincent"
171,AIRFLOW-587,AIRFLOW,1477210528000,1535980915000,Bug,Closed,Critical,2,216,"Google Authentication is broken due to broken scope The Google Authenticator is broken, wondering if it ever worked. The scope is incorrect due to (probably incorrect formatting). Action split the string up in array."
172,AIRFLOW-598,AIRFLOW,1477488687000,1518289398000,Bug,Closed,Critical,2,4849,"Can't install Airlfow Hi, i get this error when trying to install airflow 1.7.1.3 on ubuntu 14.04.5 with kernel 4.4.0-45-generic and python version 2.7.6. Some Logs: ----------------- Installed /tmp/pip-build-1KTFCl/pandas/.eggs/numpy-1.11.2-py2.7-linux-x86_64.egg running egg_info creating pip-egg-info/pandas.egg-info writing requirements to pip-egg-info/pandas.egg-info/requires.txt writing pip-egg-info/pandas.egg-info/PKG-INFO writing top-level names to pip-egg-info/pandas.egg-info/top_level.txt writing dependency_links to pip-egg-info/pandas.egg-info/dependency_links.txt writing manifest file 'pip-egg-info/pandas.egg-info/SOURCES.txt' warning: manifest_maker: standard file '-c' not found package init file 'pandas/io/tests/sas/__init__.py' not found (or not a regular file) pandas/index.pyx: cannot find cimported module 'datetime' pandas/index.pyx: cannot find cimported module 'util' ./pandas/hashtable.pxd: cannot find cimported module 'khash' Traceback (most recent call last): File ""<string>"", line 17, in <module> File ""/tmp/pip-build-1KTFCl/pandas/setup.py"", line 680, in <module> **setuptools_kwargs) File ""/usr/lib/python2.7/distutils/core.py"", line 151, in setup dist.run_commands() File ""/usr/lib/python2.7/distutils/dist.py"", line 953, in run_commands self.run_command(cmd) File ""/usr/lib/python2.7/distutils/dist.py"", line 972, in run_command cmd_obj.run() File ""<string>"", line 15, in replacement_run File ""/usr/local/lib/python2.7/dist-packages/setuptools/command/egg_info.py"", line 306, in find_sources mm.run() File ""/usr/local/lib/python2.7/dist-packages/setuptools/command/egg_info.py"", line 533, in run self.add_defaults() File ""/usr/local/lib/python2.7/dist-packages/setuptools/command/egg_info.py"", line 562, in add_defaults sdist.add_defaults(self) File ""/usr/local/lib/python2.7/dist-packages/setuptools/command/py36compat.py"", line 36, in add_defaults self._add_defaults_ext() File ""/usr/local/lib/python2.7/dist-packages/setuptools/command/py36compat.py"", line 119, in _add_defaults_ext build_ext = self.get_finalized_command('build_ext') File ""/usr/lib/python2.7/distutils/cmd.py"", line 312, in get_finalized_command cmd_obj.ensure_finalized() File ""/usr/lib/python2.7/distutils/cmd.py"", line 109, in ensure_finalized self.finalize_options() File ""/usr/local/lib/python2.7/dist-packages/Cython/Distutils/build_ext.py"", line 19, in finalize_options self.distribution.ext_modules) File ""/usr/local/lib/python2.7/dist-packages/Cython/Build/Dependencies.py"", line 809, in cythonize aliases=aliases) File ""/usr/local/lib/python2.7/dist-packages/Cython/Build/Dependencies.py"", line 714, in create_extension_list kwds = deps.distutils_info(file, aliases, base).values File ""/usr/local/lib/python2.7/dist-packages/Cython/Build/Dependencies.py"", line 590, in distutils_info return (self.transitive_merge(filename, self.distutils_info0, DistutilsInfo.merge) File ""/usr/local/lib/python2.7/dist-packages/Cython/Build/Dependencies.py"", line 600, in transitive_merge node, extract, merge, seen, {}, self.cimported_files)[0] File ""/usr/local/lib/python2.7/dist-packages/Cython/Build/Dependencies.py"", line 605, in transitive_merge_helper deps = extract(node) File ""/usr/local/lib/python2.7/dist-packages/Cython/Build/Dependencies.py"", line 581, in distutils_info0 externs = self.cimports_and_externs(filename)[1] File ""/usr/local/lib/python2.7/dist-packages/Cython/Utils.py"", line 44, in wrapper res = cache[args] = f(self, *args) File ""/usr/local/lib/python2.7/dist-packages/Cython/Build/Dependencies.py"", line 491, in cimports_and_externs for include in self.included_files(filename): File ""/usr/local/lib/python2.7/dist-packages/Cython/Utils.py"", line 44, in wrapper res = cache[args] = f(self, *args) File ""/usr/local/lib/python2.7/dist-packages/Cython/Build/Dependencies.py"", line 474, in included_files include_path = self.context.find_include_file(include, None) File ""/usr/local/lib/python2.7/dist-packages/Cython/Compiler/Main.py"", line 274, in find_include_file error(pos, ""'%s' not found"" % filename) File ""/usr/local/lib/python2.7/dist-packages/Cython/Compiler/Errors.py"", line 177, in error raise InternalError(message) Cython.Compiler.Errors.InternalError: Internal compiler error: 'algos_common_helper.pxi' not found ---------------------------------------- Cleaning up... Command python setup.py egg_info failed with error code 1 in /tmp/pip-build-1KTFCl/pandas Traceback (most recent call last): File ""/usr/bin/pip"", line 9, in <module> load_entry_point('pip==1.5.6', 'console_scripts', 'pip')() File ""/usr/lib/python2.7/dist-packages/pip/__init__.py"", line 248, in main return command.main(cmd_args) File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 161, in main text = '\n'.join(complete_log) UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 72: ordinal not in range(128)"
173,AIRFLOW-608,AIRFLOW,1477942799000,1561109761000,Bug,Resolved,Critical,2,2602,"DAG.max_active_runs should consider runs with active/pending tasks as active I expected dag runs that have active or pending tasks to be counted towards DAG.max_active_runs but that doesn't happen if the dag run has a failed task. The code works as it is described, the issue is that the feature isn't as useful as it could be due to the way it accounts for failed tasks. There should be some way to limit concurrent dag runs, where the definition of running means a dag run has any active or pending tasks. max_active_runs is described as {code} :param max_active_runs: maximum number of active DAG runs, beyond this number of DAG runs in a running state, the scheduler won't create new active DAG runs {code} https://github.com/apache/incubator-airflow/blob/527e3ecdb8adf9d7a2f40c3ce2c30f2ccaf1924a/airflow/models.py#L2521 and it's usage is {code} if dag.schedule_interval: active_runs = DagRun.find( dag_id=dag.dag_id, state=State.RUNNING, external_trigger=False, session=session ) # return if already reached maximum active runs and no timeout setting if len(active_runs) >= dag.max_active_runs and not dag.dagrun_timeout: return {code} https://github.com/apache/incubator-airflow/blob/527e3ecdb8adf9d7a2f40c3ce2c30f2ccaf1924a/airflow/jobs.py#L689 but the state is not considered RUNNING if any of the tasks have failed though some other tasks could still be running. {code} def update_state(self, session=None): """""" Determines the overall state of the DagRun based on the state of its TaskInstances. :returns State: """""" dag = self.get_dag() tis = self.get_task_instances(session=session) [...omitted...] # future: remove the check on adhoc tasks (=active_tasks) if len(tis) == len(dag.active_tasks): # if any roots failed, the run failed root_ids = [t.task_id for t in dag.roots] roots = [t for t in tis if t.task_id in root_ids] if any(r.state in (State.FAILED, State.UPSTREAM_FAILED) for r in roots): logging.info('Marking run {} failed'.format(self)) self.state = State.FAILED # if all roots succeeded, the run succeeded elif all(r.state in (State.SUCCESS, State.SKIPPED) for r in roots): logging.info('Marking run {} successful'.format(self)) self.state = State.SUCCESS # if *all tasks* are deadlocked, the run failed elif unfinished_tasks and none_depends_on_past and no_dependencies_met: logging.info( 'Deadlock; marking run {} failed'.format(self)) self.state = State.FAILED # finally, if the roots aren't done, the dag is still running else: self.state = State.RUNNING {code} https://github.com/apache/incubator-airflow/blob/527e3ecdb8adf9d7a2f40c3ce2c30f2ccaf1924a/airflow/models.py#L3800"
174,AIRFLOW-622,AIRFLOW,1478778544000,1585163752000,Bug,Closed,Critical,2,2219,"airflow cannot recover itself after mysql connection was once gone away and not trigger an error from email alert This issue occurs once the connections with airflow mysql gone away, even it recovered soon, but airflow hang there and print the following errors. Base on the suggestion of mysql, Session rollback method is needed. Also not alert mail for this case, and no retry, just silently wrap the log files. 2016-11-10 16:04:30,361 [ERROR] file:jobs.py Line:582 - This Session's transaction has been rolled back by a nested rollback() call. To begin a new transaction, issue Session.rollback() first. Traceback (most recent call last): File ""/datayes/mercury/anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 580, in _execute self.prioritize_queued(executor=executor, dagbag=dagbag) File ""/datayes/mercury/anaconda/lib/python2.7/site-packages/airflow/utils.py"", line 122, in wrapper result = func(*args, **kwargs) File ""/datayes/mercury/anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 482, in prioritize_queued pools = {p.pool: p for p in session.query(models.Pool).all()} File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/query.py"", line 2584, in all return list(self) File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/query.py"", line 2732, in __iter__ return self._execute_and_instances(context) File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/query.py"", line 2745, in _execute_and_instances close_with_result=True) File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/query.py"", line 2736, in _connection_from_session **kw) File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/session.py"", line 905, in connection execution_options=execution_options) File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/session.py"", line 910, in _connection_for_bind engine, execution_options) File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/session.py"", line 313, in _connection_for_bind self._assert_active() File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/session.py"", line 218, in _assert_active ""This Session's transaction has been rolled back "" InvalidRequestError: This Session's transaction has been rolled back by a nested rollback() call. To begin a new transaction, issue Session.rollback() first."
175,AIRFLOW-739,AIRFLOW,1483812109000,1484927066000,Bug,Resolved,Critical,2,630,"test_pickle_info has an error message on travis while pickling On Travis this test logs and error message but doesn't fail the test. This is because: {code} @provide_session def pickle_info(self, session=None): d = {} d['is_picklable'] = True try: dttm = datetime.now() pickled = pickle.dumps(self) d['pickle_len'] = len(pickled) d['pickling_duration'] = ""{}"".format(datetime.now() - dttm) except Exception as e: logging.exception(e) d['is_picklable'] = False d['stacktrace'] = traceback.format_exc() return d {code} Logs to error, while no propagation happens and the error is non fatal. Therefore it should log to debug instead."
176,AIRFLOW-742,AIRFLOW,1483921131000,1483950178000,Bug,Closed,Critical,2,3038,"Fresh Airflow install throwing ""duplicate argument 'l_1_caller'"" error from GET /admin/airflow/graph Fresh install of AF giving the following error when nagivating to `GET /admin/airflow/graph?dag_id=example_bash_operator`: {noformat} [2017-01-09 00:04:38,521] {app.py:1423} ERROR - Exception on /admin/airflow/graph [GET] Traceback (most recent call last): File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/flask/app.py"", line 1817, in wsgi_app response = self.full_dispatch_request() File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/flask/app.py"", line 1477, in full_dispatch_request rv = self.handle_user_exception(e) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/flask/app.py"", line 1381, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/flask/app.py"", line 1475, in full_dispatch_request rv = self.dispatch_request() File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/flask/app.py"", line 1461, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/flask_admin/base.py"", line 68, in inner return self._run_view(f, *args, **kwargs) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/flask_admin/base.py"", line 367, in _run_view return fn(self, *args, **kwargs) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/flask_login.py"", line 755, in decorated_view return func(*args, **kwargs) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/airflow/www/utils.py"", line 213, in view_func return f(*args, **kwargs) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/airflow/www/utils.py"", line 118, in wrapper return f(*args, **kwargs) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/airflow/www/views.py"", line 1412, in graph edges=json.dumps(edges, indent=2),) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/flask_admin/base.py"", line 307, in render return render_template(template, **kwargs) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/flask/templating.py"", line 128, in render_template context, ctx.app) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/flask/templating.py"", line 110, in _render rv = template.render(context) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/jinja2/environment.py"", line 1008, in render return self.environment.handle_exception(exc_info, True) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/jinja2/environment.py"", line 780, in handle_exception reraise(exc_type, exc_value, tb) File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/airflow/www/templates/airflow/graph.html"", line 2, in top-level template code {% import 'admin/lib.html' as lib with context %} File ""/opt/virtualenvs/airflow/lib/python2.7/site-packages/jinja2/environment.py"", line 551, in _compile return compile(source, filename, 'exec') SyntaxError: duplicate argument 'l_1_caller' in function definition (lib.html, line 255) {noformat}"
177,AIRFLOW-780,AIRFLOW,1484952316000,1486061669000,Bug,Resolved,Critical,2,194,The UI no longer shows broken DAGs When a faulty dag is placed in the dags folder the UI would report a parsing error. Now it doesnt due to the separate parising (but not reporting back errors)
178,AIRFLOW-831,AIRFLOW,1486052851000,1487511081000,Bug,Resolved,Critical,2,390,"Fix broken unit tests AIRFLOW-794 (https://github.com/apache/incubator-airflow/pull/2013) removed an import statement that was required by the PR for AIRFLOW-780 (https://github.com/apache/incubator-airflow/pull/2018). At the time AIRFLOW-794 was tested, the tests passed, but AIRFLOW-780 was merged prior to merging AIRFLOW-794. Restoring the import statement should fix the failing tests."
179,AIRFLOW-874,AIRFLOW,1487047096000,1562163957000,Bug,Closed,Critical,2,654,"Backfill dates spanning multiple years fail to parse In following output, note that 2017-01-01 is a valid iso date, but airflow says otherwise. My guess is that this has to do with the range spanning years? I could be wrong. ------------- airflow backfill internet_honey_spark_workflow -t spark_postgres_to_pyspark -s 2016-10-01 -e 201701-02 [2017-02-14 04:36:46,927] {__init__.py:36} INFO - Using executor SequentialExecutor usage: airflow backfill [-h] [-t TASK_REGEX] [-s START_DATE] [-e END_DATE] [-m] [-l] [-x] [-a] [-i] [-I] [-sd SUBDIR] [--pool POOL] [-dr] dag_id airflow backfill: error: argument -e/--end_date: invalid parse value: '201701-02'"
180,AIRFLOW-877,AIRFLOW,1487106418000,1535981029000,Bug,Closed,Critical,2,296,"GoogleCloudStorageDownloadOperator: template_ext causes TemplateNotFound Error Since a file is to be downloaded by this operator, having .sql in template_ext results in TemplateNotFound exception to be thrown (since the file doesn't exist). The solution is to remove .sql template file extension."
181,AIRFLOW-892,AIRFLOW,1487755282000,1535980998000,Bug,Resolved,Critical,2,4982,"HIvePartitionSensor and HiveNamedPartitionSensor raise AttributeError All HiveParititionSensor and friends using the Meatastore hook throw this exception in 2 installs I've done, one on a clean docker to rule out other causes. install has been done using pip install git+https://github.com/apache/incubator-airflow.git@v1-8-stable {noformat} [2017-02-22 08:44:49,079] {base_task_runner.py:95} INFO - Subtask: -------------------------------------------------------------------------------- [2017-02-22 08:44:49,079] {base_task_runner.py:95} INFO - Subtask: Starting attempt 1 of 1 [2017-02-22 08:44:49,079] {base_task_runner.py:95} INFO - Subtask: -------------------------------------------------------------------------------- [2017-02-22 08:44:49,079] {base_task_runner.py:95} INFO - Subtask: [2017-02-22 08:44:49,111] {base_task_runner.py:95} INFO - Subtask: [2017-02-22 08:44:49,110] {models.py:1342} INFO - Executing <Task(HivePartitionSensor): wait_for_***> on 2017-02-18 00:00:00 [2017-02-22 08:44:49,124] {base_task_runner.py:95} INFO - Subtask: [2017-02-22 08:44:49,123] {sensors.py:373} INFO - Poking for table ****, partition d_date='2017-02-18' [2017-02-22 08:44:49,125] {base_task_runner.py:95} INFO - Subtask: [2017-02-22 08:44:49,124] {models.py:1417} ERROR - [2017-02-22 08:44:49,125] {base_task_runner.py:95} INFO - Subtask: Traceback (most recent call last): [2017-02-22 08:44:49,125] {base_task_runner.py:95} INFO - Subtask: File ""/home/airflow/airflow2/lib/python2.7/site-packages/airflow/models.py"", line 1374, in run [2017-02-22 08:44:49,125] {base_task_runner.py:95} INFO - Subtask: result = task_copy.execute(context=context) [2017-02-22 08:44:49,125] {base_task_runner.py:95} INFO - Subtask: File ""/home/airflow/airflow2/lib/python2.7/site-packages/airflow/operators/sensors.py"", line 76, in execute [2017-02-22 08:44:49,125] {base_task_runner.py:95} INFO - Subtask: while not self.poke(context): [2017-02-22 08:44:49,125] {base_task_runner.py:95} INFO - Subtask: File ""/home/airflow/airflow2/lib/python2.7/site-packages/airflow/operators/sensors.py"", line 375, in poke [2017-02-22 08:44:49,126] {base_task_runner.py:95} INFO - Subtask: self.hook = airflow.hooks.hive_hooks.HiveMetastoreHook( [2017-02-22 08:44:49,126] {base_task_runner.py:95} INFO - Subtask: File ""/home/airflow/airflow2/lib/python2.7/site-packages/airflow/utils/helpers.py"", line 436, in __getattr__ [2017-02-22 08:44:49,126] {base_task_runner.py:95} INFO - Subtask: raise AttributeError [2017-02-22 08:44:49,126] {base_task_runner.py:95} INFO - Subtask: AttributeError [2017-02-22 08:44:49,126] {base_task_runner.py:95} INFO - Subtask: [2017-02-22 08:44:49,125] {models.py:1441} INFO - Marking task as FAILED. [2017-02-22 08:44:49,163] {base_task_runner.py:95} INFO - Subtask: [2017-02-22 08:44:49,163] {models.py:1462} ERROR - [2017-02-22 08:44:49,163] {base_task_runner.py:95} INFO - Subtask: Traceback (most recent call last): [2017-02-22 08:44:49,164] {base_task_runner.py:95} INFO - Subtask: File ""/home/airflow/airflow2/bin/airflow"", line 28, in <module> [2017-02-22 08:44:49,164] {base_task_runner.py:95} INFO - Subtask: args.func(args) [2017-02-22 08:44:49,164] {base_task_runner.py:95} INFO - Subtask: File ""/home/airflow/airflow2/lib/python2.7/site-packages/airflow/bin/cli.py"", line 427, in run [2017-02-22 08:44:49,164] {base_task_runner.py:95} INFO - Subtask: pool=args.pool, [2017-02-22 08:44:49,164] {base_task_runner.py:95} INFO - Subtask: File ""/home/airflow/airflow2/lib/python2.7/site-packages/airflow/utils/db.py"", line 54, in wrapper [2017-02-22 08:44:49,164] {base_task_runner.py:95} INFO - Subtask: result = func(*args, **kwargs) [2017-02-22 08:44:49,164] {base_task_runner.py:95} INFO - Subtask: File ""/home/airflow/airflow2/lib/python2.7/site-packages/airflow/models.py"", line 1374, in run [2017-02-22 08:44:49,165] {base_task_runner.py:95} INFO - Subtask: result = task_copy.execute(context=context) [2017-02-22 08:44:49,165] {base_task_runner.py:95} INFO - Subtask: File ""/home/airflow/airflow2/lib/python2.7/site-packages/airflow/operators/sensors.py"", line 76, in execute [2017-02-22 08:44:49,165] {base_task_runner.py:95} INFO - Subtask: while not self.poke(context): [2017-02-22 08:44:49,165] {base_task_runner.py:95} INFO - Subtask: File ""/home/airflow/airflow2/lib/python2.7/site-packages/airflow/operators/sensors.py"", line 375, in poke [2017-02-22 08:44:49,165] {base_task_runner.py:95} INFO - Subtask: self.hook = airflow.hooks.hive_hooks.HiveMetastoreHook( [2017-02-22 08:44:49,165] {base_task_runner.py:95} INFO - Subtask: File ""/home/airflow/airflow2/lib/python2.7/site-packages/airflow/utils/helpers.py"", line 436, in __getattr__ [2017-02-22 08:44:49,165] {base_task_runner.py:95} INFO - Subtask: raise AttributeError {noformat} I've worked around the issue adding a 'import airflow.hooks.hive_hooks' in class HiveParititionSensor (because I don't really know what this is about, there surely is a real fix to be done). It fixes the issue for me."
182,AIRFLOW-981,AIRFLOW,1489516384000,1539693082000,Bug,Closed,Critical,2,378,TreeView date axis shows dates into the future Freshly installed AirFlow example_twitter_dag Tree View shows date scale from Mar 13 (yesterday when that job didn't even run) till March 27th and further (2+ weeks into the future) So the tasks below that date scale are totally misaligned to the time dimension. See screenshot below. !Airflow - TreeView-2 weeks in the future.png!
183,AIRFLOW-989,AIRFLOW,1489610877000,1489621191000,Bug,Resolved,Critical,2,1696,"Clear Task Regression There is a regression in the current 1.8 rc (e.g. rc5) related to Clear Task behavior. Consider the following test DAG : 1. Code : https://gist.github.com/r39132/b44f7d791e11f882cde28a219df97c29 2. Graph : https://www.dropbox.com/s/1e9rfnq6cy4hh45/Screenshot%202017-03-15%2013.48.26.png?dl=0 The test DAG has 4 dummy tasks chained together as d1->d2->d3->d4. d1 is the first task and d4 is the last task. Prior to 1.8, if any of d1..d4 were cleared individually. the scheduler would pick up and rerun the cleared tasks. In 1.8. unless the last task in a DAG is cleared, none of the tasks in the DAG run are rerun. In order for a task that is not the last task in the DAG to be rerun after being cleared, its terminal downstream task needs to be cleared. Another workaround is to use the CLI to rerun the cleared task. Here are some screenshots to illustrate the regressed behavior: Use Case 1 : Clear d1, d2, d3, and d4 in 4 separate DAG runs. In a 5th separate DAG run, clear the entire DAG Run. After Clearing : https://www.dropbox.com/s/mgiwoyaxf5f2pb2/Screenshot%202017-03-15%2010.12.02.png?dl=0 After the Scheduler Runs : https://www.dropbox.com/s/7btwzydv87v3iz0/Screenshot%202017-03-15%2010.15.16.png?dl=0 You'll notice that only the DAG runs with the last task cleared completed by actually running cleared tasks. These are shown as the 1st and 5th DAG runs from the left. Use Case 2 : Clear d1 and d4 in the same DAG Run After Clearing (c.f. 2nd from right DAG run): https://www.dropbox.com/s/2a6by6k28eb7geh/Screenshot%202017-03-15%2013.34.11.png?dl=0 After the Scheduler Runs : https://www.dropbox.com/s/19cg6qr2oqi1ps7/Screenshot%202017-03-15%2013.34.51.png?dl=0"
184,AIRFLOW-992,AIRFLOW,1489627360000,1490042246000,Bug,Closed,Critical,2,1240,"Skipped tasks do not propagate correctly We are seeing skipped tasks not being propagated correctly: E.g. A->B `-->C Task A depends on task B and C If B gets skipped and C gets run then: Expected: A will get skipped EDIT: Upon further investigation this was caused by a change in the semantics of ALL_SUCCESS, which I have these feelings about: Intuitively you would expect to skip any task that has dependencies that weren't run by default, i.e. the trigger rule is called ALL_SUCCESS and skipped tasks are not successful ones, and that was also the old behavior in 1.7.3. This is going to break some use cases which could be alright, but I feel these new semantics make less sense than before so it's a bad reason to break existing use cases. I will get started on a PR for a new ALL_SUCCESS_NOT_SKIPPED trigger rule but again I feel this is hacky and really we should have the old ALL_SUCCESS (default) and a new ALL_SUCCESS_OR_SKIPPED trigger rule if desired. Actual: A gets run It's possible the reason for the change in semantics (which was to fix a bug) is fixed already by https://github.com/apache/incubator-airflow/pull/2154 and the only thing we need to do is revert https://github.com/apache/incubator-airflow/pull/2125 [~bolke]"
185,AIRFLOW-1013,AIRFLOW,1489976523000,1556721672000,Bug,Closed,Critical,2,1999,"airflow/jobs.py:manage_slas() exception for @once dag Getting following exception {noformat} [2017-03-19 20:16:25,786] {jobs.py:354} DagFileProcessor2638 ERROR - Got an exception! Propagating... Traceback (most recent call last): File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper pickle_dags) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 1581, in process_file self._process_dags(dagbag, dags, ti_keys_to_schedule) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 1175, in _process_dags self.manage_slas(dag) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 595, in manage_slas while dttm < datetime.now(): TypeError: can't compare datetime.datetime to NoneType {noformat} Exception is in airflow/jobs.py:manage_slas() : https://github.com/apache/incubator-airflow/blob/v1-8-stable/airflow/jobs.py#L595 {code} ts = datetime.now() SlaMiss = models.SlaMiss for ti in max_tis: task = dag.get_task(ti.task_id) dttm = ti.execution_date if task.sla: dttm = dag.following_schedule(dttm) >>> while dttm < datetime.now(): <<< here following_schedule = dag.following_schedule(dttm) if following_schedule + task.sla < datetime.now(): session.merge(models.SlaMiss( task_id=ti.task_id, {code} It seems that dag.following_schedule() returns None for @once dag? Here's how dag is defined: {code} main_dag = DAG( dag_id = 'DISCOVER-Oracle-Load', default_args = default_args, user_defined_macros = dag_macros, start_date = datetime.now(), catchup = False, schedule_interval = '@once', concurrency = 2, max_active_runs = 1, dagrun_timeout = timedelta(days=4), ) {code}"
186,AIRFLOW-1017,AIRFLOW,1490044346000,1490822117000,Bug,Resolved,Critical,2,311,"get_task_instance should return None instead of throw an exception for non-existent TIs We were seeing errors in our scheduler like the following due to this issue: File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2796, in one raise orm_exc.NoResultFound(""No row was found for one()"")"
187,AIRFLOW-1019,AIRFLOW,1490057243000,1516027276000,Improvement,Resolved,Critical,2,512,active_dagruns shouldn't include paused DAGs Since 1.8.0 Airflow resets orphaned tasks (tasks that are in the DB but not in the executor's memory). The problem is that Airflow counts dagruns in paused DAGs as running as long as the dagruns state is running. Instead we should join against non-paused DAGs everywhere we calculate active dagruns (e.g. in _process_task_instances in the Scheduler class in jobs.py). If there are enough paused DAGs it brings the scheduler to a halt especially on scheduler restarts.
188,AIRFLOW-1024,AIRFLOW,1490131066000,1497368168000,Bug,Resolved,Critical,2,453,"Handle CeleryExecutor errors gracefully If the Airflow celery executor receives a bad response from a worker (e.g. unpickled response), then it will crash the scheduler and cause it to restart. We should code defensively around the interactions with celery so that we just log errors instead of crashing the scheduler. It might makes sense to make the try catches one level higher (to catch errors from all executors), but this needs some investigation."
189,AIRFLOW-1026,AIRFLOW,1490158006000,1544454057000,Bug,Resolved,Critical,2,846,"connection string using _cmd tin airflow.cfg is broken sql_alchemy_conn_cmd = python ./pipeline/dags/configure.py I am expectiing configure.py to be invoked. But it just throws: ""cannot use sqlite with the LocalExecutor"" The connection string that my script ""configure.py"" would return is something like this: mysql+mysqldb://username:**@mysqlhostname:3306/airflowdbname But after debugging, I found that, my script is not getting invoked at all. This is my airflow.cfg: executor = LocalExecutor sql_alchemy_conn_cmd = python ./pipeline/dags/configure.py sql_alchemy_pool_size = 5 sql_alchemy_pool_recycle = 3600 I tried not using the script and directly hardcoding the conn_url sql_alchemy_conn = mysql+mysqldb://username:**@mysqlhostname:3306/airflowdbname It works. But there is a regression bug if somebody wants to use ""sql_alchemy_conn_cmd"""
190,AIRFLOW-1055,AIRFLOW,1490855447000,1535980874000,Bug,Closed,Critical,2,2588,"airflow/jobs.py:create_dag_run() exception for @once dag when catchup = False Getting following exception {noformat} [2017-03-19 20:16:25,786] {jobs.py:354} DagFileProcessor2638 ERROR - Got an exception! Propagating... Traceback (most recent call last): File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper pickle_dags) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 1581, in process_file self._process_dags(dagbag, dags, ti_keys_to_schedule) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 1175, in _process_dags self.manage_slas(dag) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 595, in manage_slas while dttm < datetime.now(): TypeError: can't compare datetime.datetime to NoneType {noformat} Exception is in airflow/jobs.py:manage_slas() : https://github.com/apache/incubator-airflow/blob/v1-8-stable/airflow/jobs.py#L595 {code} ts = datetime.now() SlaMiss = models.SlaMiss for ti in max_tis: task = dag.get_task(ti.task_id) dttm = ti.execution_date if task.sla: dttm = dag.following_schedule(dttm) >>> while dttm < datetime.now(): <<< here following_schedule = dag.following_schedule(dttm) if following_schedule + task.sla < datetime.now(): session.merge(models.SlaMiss( task_id=ti.task_id, {code} It seems that dag.following_schedule() returns None for @once dag? Here's how dag is defined: {code} import datetime as dt import airflow from airflow.models import DAG from airflow.operators.dummy_operator import DummyOperator from datetime import datetime, timedelta def sla_alert_func(dag, task_list, blocking_task_list, slas, blocking_tis): print('Executing SLA miss callback') now = datetime.now() now_to_the_hour = now.replace(hour=now.time().hour, minute=0, second=0, microsecond=0) START_DATE = now_to_the_hour + timedelta(hours=-3) DAG_NAME = 'manage_sla_once_dag' default_args = { 'owner': 'sanand', 'depends_on_past': False, 'start_date': START_DATE, 'sla': timedelta(hours=2) } dag = DAG( dag_id = 'manage_sla_once_dag', default_args = default_args, catchup = False, schedule_interval = '@once', sla_miss_callback = sla_alert_func ) task1 = DummyOperator(task_id='task1', dag=dag) {code} This issue works if catchup = True."
191,AIRFLOW-1092,AIRFLOW,1491810593000,1543412278000,Bug,Closed,Critical,2,302,"{{execution_date}} not matching '%Y-%m-%d %H:%M:%S.%f' Hi there, I was trying to use datetime.strptime({{ execution_date }}, '%Y-%m-%d %H:%M:%S.%f') to format some files names, but an error was returned saying {{ execution_date }} format is not matching the provided one. Could anyone take a look? Thx!"
192,AIRFLOW-1147,AIRFLOW,1493149338000,1535980947000,Bug,Closed,Critical,2,884,"airflow scheduler not working I've created some `DAG`s, and I tried to put it on scheduler. I want to run all the tasks in the DAG after exact 24 hours. I tried to do something like this. {code} DEFAULT_ARGS = { 'owner' : 'mubin', 'depends_on_past' : False, 'start_date' : datetime(2017, 4, 24, 14, 30), 'retries' : 5, 'retry_delay' : timedetla(1), } SCHEDULE_INTERVAL = timedelta(minutes=1440) # SCHEDULE_INTERVAL = timedelta(hours=24) # SCHEDULE_INTERVAL = timedelta(days=1) dag = DAG('StandardizeDataDag', default_args = DEFAULT_ARGS, schedule_interval = SCHEDULE_INTERVAL ) {code} I tried to put different intervals, but not any working. However if I try to reset db {code} airflow resetdb -y {code} and then run {code} airflow initdb {code} , it works for once. then after that, scheduler isn't able to run it. PS. {code} airflow scheduler {code} executed from {code} root {code}"
193,AIRFLOW-1153,AIRFLOW,1493262183000,1562842659000,Bug,Resolved,Critical,2,618,"params in HiveOperator constructor can't be passed into Hive execution context params parameter in HiveOperator can't be imported into Hive executation context. so the following centence won't work, because 'mynumber' doesn't work for sql sentence. test_hiveoperator = HiveOperator( task_id='hive_test', hiveconf_jinja_translate=True, hql = ''' use myDB; INSERT OVERWRITE TABLE t2 select * from t1 where t1.x > ' ${hiveconf:mynumber}' ''', params={'mynumber': 2}, dag=dag ) this modification pass the 'params' in HiveOperator construction to Hive sql execution context. The the variable definition can pass to hive sql"
194,AIRFLOW-1157,AIRFLOW,1493321648000,1535980835000,Bug,Resolved,Critical,2,4601,"Assigning a task to a pool that doesn't exist crashes the scheduler If a dag is run that contains a task using a pool that doesn't exist, the scheduler will crash. Manually triggering the run of this dag on an environment without a pool named 'a_non_existent_pool' will crash the scheduler: {code} from datetime import datetime from airflow.models import DAG from airflow.operators.dummy_operator import DummyOperator dag = DAG(dag_id='crash_scheduler', start_date=datetime(2017,1,1), schedule_interval=None) t1 = DummyOperator(task_id='crash', pool='a_non_existent_pool', dag=dag) {code} Here is the relevant log output on the scheduler: {noformat} [2017-04-27 19:31:24,816] {dag_processing.py:559} INFO - Processor for /opt/airflow/dags/test-3.py finished [2017-04-27 19:31:24,817] {dag_processing.py:559} INFO - Processor for /opt/airflow/dags/test_s3_file_move.py finished [2017-04-27 19:31:24,819] {dag_processing.py:627} INFO - Started a process (PID: 124) to generate tasks for /opt/airflow/dags/crash_scheduler.py - logging into /tmp/airflow/scheduler/logs/2017-04-27/crash_scheduler.py.log [2017-04-27 19:31:24,822] {dag_processing.py:627} INFO - Started a process (PID: 125) to generate tasks for /opt/airflow/dags/configuration/constants.py - logging into /tmp/airflow/scheduler/logs/2017-04-27/configuration/constants.py.log [2017-04-27 19:31:24,847] {jobs.py:1007} INFO - Tasks up for execution: <TaskInstance: move_s3_file_test.move_files 2017-04-27 19:31:22.298893 [scheduled]> [2017-04-27 19:31:24,849] {jobs.py:1030} INFO - Figuring out tasks to run in Pool(name=None) with 128 open slots and 1 task instances in queue [2017-04-27 19:31:24,856] {jobs.py:1078} INFO - DAG move_s3_file_test has 0/16 running tasks [2017-04-27 19:31:24,856] {jobs.py:1105} INFO - Sending to executor (u'move_s3_file_test', u'move_files', datetime.datetime(2017, 4, 27, 19, 31, 22, 298893)) with priority 1 and queue MVSANDBOX-airflow-DEV-dev [2017-04-27 19:31:24,859] {jobs.py:1116} INFO - Setting state of (u'move_s3_file_test', u'move_files', datetime.datetime(2017, 4, 27, 19, 31, 22, 298893)) to queued [2017-04-27 19:31:24,867] {base_executor.py:50} INFO - Adding to queue: airflow run move_s3_file_test move_files 2017-04-27T19:31:22.298893 --local -sd /opt/airflow/dags/test_s3_file_move.py [2017-04-27 19:31:24,867] {jobs.py:1440} INFO - Heartbeating the executor [2017-04-27 19:31:24,872] {celery_executor.py:78} INFO - [celery] queuing (u'move_s3_file_test', u'move_files', datetime.datetime(2017, 4, 27, 19, 31, 22, 298893)) through celery, queue=MVSANDBOX-airflow-DEV-dev [2017-04-27 19:31:25,974] {jobs.py:1404} INFO - Heartbeating the process manager [2017-04-27 19:31:25,975] {dag_processing.py:559} INFO - Processor for /opt/airflow/dags/crash_scheduler.py finished [2017-04-27 19:31:25,975] {dag_processing.py:559} INFO - Processor for /opt/airflow/dags/configuration/constants.py finished [2017-04-27 19:31:25,977] {dag_processing.py:627} INFO - Started a process (PID: 128) to generate tasks for /opt/airflow/dags/example_s3_sensor.py - logging into /tmp/airflow/scheduler/logs/2017-04-27/example_s3_sensor.py.log [2017-04-27 19:31:25,980] {dag_processing.py:627} INFO - Started a process (PID: 129) to generate tasks for /opt/airflow/dags/test-4.py - logging into /tmp/airflow/scheduler/logs/2017-04-27/test-4.py.log [2017-04-27 19:31:26,004] {jobs.py:1007} INFO - Tasks up for execution: <TaskInstance: crash_scheduler.crash 2017-04-27 19:30:51.948542 [scheduled]> [2017-04-27 19:31:26,006] {jobs.py:1311} INFO - Exited execute loop [2017-04-27 19:31:26,008] {jobs.py:1325} INFO - Terminating child PID: 128 [2017-04-27 19:31:26,008] {jobs.py:1325} INFO - Terminating child PID: 129 [2017-04-27 19:31:26,008] {jobs.py:1329} INFO - Waiting up to 5s for processes to exit... Traceback (most recent call last): File ""/usr/bin/airflow"", line 28, in <module> args.func(args) File ""/usr/lib/python2.7/site-packages/airflow/bin/cli.py"", line 839, in scheduler job.run() File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 200, in run self._execute() File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1309, in _execute self._execute_helper(processor_manager) File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1437, in _execute_helper (State.SCHEDULED,)) File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1025, in _execute_task_instances open_slots = pools[pool].open_slots(session=session) KeyError: u'a_non_existant_pool' {noformat}"
195,AIRFLOW-1183,AIRFLOW,1494342970000,1535980901000,Bug,Closed,Critical,2,742,"How to pass Spark URL for standalone cluster? How can I pass my Spark URL? When I look in the logs I see `--master` is pointed to ""yarn."" Also, the same thing for `cluster-mode`. I tried passing it within the function but I'm getting an error. *Error* {code} PendingDeprecationWarning: Invalid arguments were passed to SparkSubmitOperator. Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were: *args: () **kwargs: {'deploy_mode': 'cluster', 'java_class': 'SimpleApp'} {code} *Code* {code} testSpark = SparkSubmitOperator( task_id='test-spark', deploy_mode='cluster', application='src/main/scala/target/scala-2.11/simple-project_2.11-1.0.jar', java_class='SimpleApp', deploy_mode='cluster', dag=dag) {code}"
196,AIRFLOW-1253,AIRFLOW,1496123979000,1567581578000,Bug,Open,Critical,2,814,Jobs are not scheduled Jobs are not scheduled for me in airflow when using 24 hour cron (example: 0 7 * * *) I tried to debug jobs.py and it seems the following is causing it not to run (line 857): {code} if next_run_date and period_end and period_end <= datetime.now(): {code} I don't understand the logic here. This is what's in the above variables: {noformat} next_run_date=2017-05-30 07:00:00 period_end=2017-05-31 07:00:00 datetime.now()=2017-05-30 07:01:22 {noformat} next_run_date reflects the next run date of the DAG which is correct (today at 07:00) period_end reflects when will be the next-next run which is also correct (tomorrow) The DAG needs to run in between. So the IF above should be: {code} if next_run_date and period_end and period_end >= datetime.now(): {code} Am I missing something? Thanks
197,AIRFLOW-1260,AIRFLOW,1496306126000,1558149419000,Bug,Resolved,Critical,2,233,FLOWER XSS Vulnerability The affected functions are WorkerQueueAddConsumer() and WorkerQueueCancelConsumer() from the fichier flower/static/js/flower.js file. The use of the .html() function instead of .text() allows script execution
198,AIRFLOW-1267,AIRFLOW,1496417856000,1558123382000,Bug,Resolved,Critical,2,320,"BigQueryCheckOperator is not configurable on the use_legacy_sql flag The BigqueryCheckOperator is not configurable to use standard sql, which is causing our DAGs to fail. It might be due to the BigQuery hook configs using legacy sql as default, and the check operator class never has access to change that configuration."
199,AIRFLOW-1268,AIRFLOW,1496435144000,1559208512000,Bug,Closed,Critical,2,196,"Celery bug can cause tasks to be delayed indefinitely With celery, tasks can get delayed indefinitely (or default 1 hour) due to a bug with celery, see https://github.com/celery/celery/issues/3765"
200,AIRFLOW-1298,AIRFLOW,1497033888000,1547914724000,Bug,Resolved,Critical,2,771,"Airflow Clear Command does not clear tasks in UPSTREAM_FAILED state We are unable to clear airflow tasks that are in the UPSTREAM_FAILED state using the command line. The fix is simple - just change `clear` function in models.py to also clear tasks in UPSTREAM_FAILED state, not just FAILED. Diff: {noformat} diff --git a/airflow/models.py b/airflow/models.py index 30e18a44..e60d2918 100755 --- a/airflow/models.py +++ b/airflow/models.py @@ -3180,7 +3180,7 @@ class DAG(BaseDag, LoggingMixin): if end_date: tis = tis.filter(TI.execution_date <= end_date) if only_failed: - tis = tis.filter(TI.state == State.FAILED) + tis = tis.filter(TI.state == State.FAILED or TI.state == State.UPSTREAM_FAILED) if only_running: tis = tis.filter(TI.state == State.RUNNING) {noformat}"
201,AIRFLOW-1305,AIRFLOW,1497427941000,1550133684000,Bug,Open,Critical,2,216,"This DAG seems to be exists locally. All DAGs I created after tutorial show like this, and I can not make it on or off in UI. Restart webserver, no use to this. Also, the dags are not scheduled. Hope for useful help."
202,AIRFLOW-1346,AIRFLOW,1498448497000,1498457575000,Bug,Open,Critical,2,550,"How to repeat the backfill task ? How do I repeat the backfill task? For example, my first backfill task has made an error and I want to backfill again. But always the following tips, how can I solve it? [root@client23 ~]# airflow backfill platformusertags_day -s 20170202 -e 20170202 [2017-06-26 11:38:12,662] {__init__.py:57} INFO - Using executor LocalExecutor [2017-06-26 11:38:13,145] {models.py:167} INFO - Filling up the DagBag from /root/airflow/dags [2017-06-26 11:38:13,369] {jobs.py:2023} INFO - *{color:red}Backfill done. Exiting.{color}*"
203,AIRFLOW-1355,AIRFLOW,1498646060000,1535980968000,Bug,Closed,Critical,2,412,"Unable to launch jobs : DAGs not being executed. Steps to re-produce: 1. Create new installation 2. Launch Airflow 3. Enable a DAG and trigger it manually DAG/Job won't get executed. Will stay in Running status, but no execution starts and continues to stay at same status. Same issues are there with Sequential, Local and Celeri executors. Happening in 1.8 version. Tried on multiple Mac machines and on Ubuntu."
204,AIRFLOW-1357,AIRFLOW,1498726945000,1535980463000,Bug,Resolved,Critical,2,452,"Scheduler does not support zip files Users from 1.7 series of airflow using zipped DAGs are unable to migrate to 1.8+ series because the scheduler does not detect/run them. The zipfile support is still present in the models.py file so the zipped DAGs do show up on `airflow list_dags` but they are never executed by the scheduler since it only cares for .py files. I'm attaching the fix for this, will also open a GH PR. Tested ok & in production here."
205,AIRFLOW-1397,AIRFLOW,1499724763000,1506702563000,Bug,Resolved,Critical,2,304,"Airflow 1.8.1 - No data displays in Last Run Column in Airflow UI Recently upgraded Airflow version from 1.8.0 to 1.8.1. After upgrading, the Last Run column in Airflow UI started showing as Blank for all the existing dags. Created a pr for this bug: https://github.com/apache/incubator-airflow/pull/2430"
206,AIRFLOW-1428,AIRFLOW,1500455000000,1500469478000,Bug,Open,Critical,2,2290,"DagRun deadlocks when all tasks' dependencies have skipped state In controlling which tasks are executed in a DagRun, it's common for tasks to skip themselves, e.g., by raising an AirflowSkipException. One controls how skips propagate using trigger rules. It is currently unclear to me how to propagate skipped states without causing the DagRun to deadlock. Consider the following simple example {code} def raise_skip(): raise AirflowSkipException skip_op = PythonOperator( task_id='skip_op', python_callable=raise_skip, dag=dag) skipped_child_op = DummyOperator(task_id='skipped_child_op', dag=dag) skip_op.set_downstream(skipped_child_op) {code} When I run the code above, the DagRun deadlocks. I have dug into why: * The deadlock is detected by DagRun.update_state [here|https://github.com/apache/incubator-airflow/blob/master/airflow/models.py#L4290-L4293]. * That's raised because {{no_dependencies_met}} is {{True}}, when it should be {{False}} * no_dependencies_met is True because when you call {{skipped_child_op.get_failed_dep_statuses()}}, it returns [TIDepStatus(dep_name='Trigger Rule', passed=False, reason=""Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_tasks_state={'successes': 0, 'skipped': 1, 'failed': 0, 'upstream_failed': 0, 'done': 1}, upstream_task_ids=['metas_update_skipper']"")] So basically, because {{skipped_child_op}}'s trigger rule is {{all_success}} and its parent is skipped, the scheduler can't find any tasks that are ready for execution. The Dag is then marked as failed (due to deadlock). I have looked for a trigger rule that would cause skipped parents to be not failed, but that doesn't seem to exist. I don't want to use {{ALL_DONE}} because I want {{skipped_child_op}} to be skipped. Thus, there seems to be no way for me to properly implement this very simple DAG. It seems that the airflow community has gone back and forth on how to handle skipped tasks--see [AIRFLOW-983|https://issues.apache.org/jira/browse/AIRFLOW-983], [AIRFLOW-992|https://issues.apache.org/jira/browse/AIRFLOW-992], and [AIRFLOW-719|https://issues.apache.org/jira/browse/AIRFLOW-719]. There seems to be support for adding a trigger rule called `ALL_SUCCESS_OR_SKIPPED`, but nobody has implemented it."
207,AIRFLOW-1430,AIRFLOW,1500468984000,1522160194000,Bug,Resolved,Critical,2,477,"GPL licensing issues The current requirements require the install of the GPLv2 licensed package {{Unidecode}} . This is a violation to the ASF licensing conditions, c.f. https://www.apache.org/legal/resolved.html#category-x https://www.apache.org/legal/resolved.html#prohibited The requirement comes in via: {{python-nvd3}} https://github.com/areski/python-nvd3 -> {{python-sluggify}} https://github.com/un33k/python-slugify -> {{Unidecode}} https://github.com/avian2/unidecode"
208,AIRFLOW-1473,AIRFLOW,1501235650000,1553681231000,Bug,Resolved,Critical,2,427,"DAG state set failed, no failed task *Next Scheduled task which never gets queued or run:* Task Instance Details: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but *found 1 non-success*(es). upstream_tasks_state={'successes': 54L, 'failed': 0L, 'upstream_failed': 0L, 'skipped': 0L, 'done': 54L} Dagrun Running: Task instance's dagrun was not in the 'running' state but in the state 'failed'."
209,AIRFLOW-1540,AIRFLOW,1503959380000,1541105057000,New Feature,Closed,Critical,2,390,Airflow 1.8.1 - Add proxies to slack operator We are trying to use slack operator from the cloud server that use proxies to connect open internet including slack. Currently the connection to Slack APIs fail everytime with connection timed out. Need to include proxies option in Slack operator to resolve the issue. Pull request created: https://github.com/apache/incubator-airflow/pull/2548
210,AIRFLOW-1607,AIRFLOW,1505316261000,1562365669000,Bug,Open,Critical,2,236,"oauth with google and 2FA We try to use oauth with 2FA . Each times or almost , we use a Firefox in regular mode , that does not work . see file s.txt But if we switch to private mode/incognito mode it's working . List of packages p.txt"
211,AIRFLOW-1617,AIRFLOW,1505590877000,1533653782000,Bug,Closed,Critical,2,224,"XSS Vulnerability in Variable endpoint Variable view has an XSS vulnerability when the Variable template does not exist. The input is returned to the user as is, without escaping. Original report by Seth Long. CVE is pending"
212,AIRFLOW-1756,AIRFLOW,1508886107000,1510261284000,Bug,Resolved,Critical,2,482,"S3 Task Handler Cannot Read Logs With New S3Hook With the changes to the S3Hook, it seems like it cannot read the S3 task logs. In the `s3_read` in the S3TaskHandler.py: {code} s3_key = self.hook.get_key(remote_log_location) if s3_key: return s3_key.get_contents_as_string().decode() {code} Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook's `read_key()` method to read the contents of the task logs now."
213,AIRFLOW-1776,AIRFLOW,1509545715000,1509576030000,Bug,Resolved,Critical,2,34,stdout/stderr logging not captured
214,AIRFLOW-1784,AIRFLOW,1509820706000,1580114488000,Bug,Resolved,Critical,2,468,SKIPPED status is being cascading wrongly After implementation of AIRFLOW-1296 within 1.8.2 there is an wrong behavior of LatestOnlyOperator which is forcing SKIPPED status cascading despite of TriggerRule='all_done' set Which is opposite to documented [here|https://airflow.incubator.apache.org/concepts.html#latest-run-only] *Expected Behavior:* dummy task and all downstreams (update_*) should not be skipped Full listings are attached 1.8.1 did not have such issue
215,AIRFLOW-1829,AIRFLOW,1510944251000,1513032045000,Improvement,Resolved,Critical,2,437,"Support BigQuery schema updates as a side effect of a query job BigQuery hook supports schema updates as a side effect of a load job but not for query jobs. Respectively GCS to BQ operator (which executes a load job) supports such possibility unlike its 'sister' operator, BQ operator, when running a query with a table as destination. Both operations, load and query, should support such feature (experimental as of this writing though)"
216,AIRFLOW-1927,AIRFLOW,1513347996000,1535910926000,Bug,Resolved,Critical,2,250,"TaskInstance should also convert naive dates Task Instances cannot be saved without time zone information anymore. Although airflow itself is fine, some people like to instantiate TaskInstances themselves. Conversion should therefore be as with Dags."
217,AIRFLOW-1947,AIRFLOW,1513858656000,1581633543000,Bug,Closed,Critical,2,3531,"airflow json file created i /tmp get wrong permission when using run_as_user We are using run_as_user on two specific task, to make sure that the resulting files are assigned to the correct user. If we are running the task as the Airflow user the task get done as expected. *DAG START* from airflow import DAG from airflow.operators.bash_operator import BashOperator from datetime import datetime, timedelta default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2015, 6, 1), 'email': ['sln@fcoo.dk'], 'email_on_failure': False, 'email_on_retry': False, 'retries': 1, 'retry_delay': timedelta(minutes=5), 'queue': 'storage-arch03', 'dagrun_timeout' : timedelta(minutes=60) # 'pool': 'backfill', # 'priority_weight': 10, # 'end_date': datetime(2016, 1, 1), } dag = DAG('Archive_Sentinel-1_data_from_FCOO_ftp_server', default_args=default_args, schedule_interval=timedelta(1)) archivingTodaysData = BashOperator( task_id='Archive_todays_data', bash_command='/home/airflow/airflowScripts/archive-Sentinel-1-data.sh 0 ', dag=dag) archivingYesterdaysData = BashOperator( task_id='Archive_yesterdays_data', bash_command='/home/airflow/airflowScripts/archive-Sentinel-1-data.sh 1 ', dag=dag) # First archive the newest data, then the data from yesterday. archivingYesterdaysData.set_upstream( archivingTodaysData ) *DAG END* When we run the tast with a user called prod by using the run_as_user, the file(s) are generated In the /tmp -rw------- 1 airflow airflow 2205 dec 19 11:46 tmpicu87_au But the prod user cannot read the file. From the log file we have: [2017-12-19 11:46:31,803] {base_task_runner.py:112} INFO - Running: ['bash', '-c', 'sudo -H -u prod airflow run Archive_Sentinel-1_data_from_FCOO_ftp_server Archive_yesterdays_data 2017-12-19T00:00:00 --job_id 1047 --raw -sd DAGS_FOLDER/archive-Sentinel-1-data-from-ftp-server.py --cfg_path /tmp/tmpicu87_au'] [2017-12-19 11:46:32,463] {base_task_runner.py:95} INFO - Subtask: [2017-12-19 11:46:32,462] {__init__.py:57} INFO - Using executor SequentialExecutor [2017-12-19 11:46:32,587] {base_task_runner.py:95} INFO - Subtask: [2017-12-19 11:46:32,587] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python3.5/lib2to3/Grammar.txt [2017-12-19 11:46:32,630] {base_task_runner.py:95} INFO - Subtask: [2017-12-19 11:46:32,630] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python3.5/lib2to3/PatternGrammar.txt [2017-12-19 11:46:33,124] {base_task_runner.py:95} INFO - Subtask: /usr/local/lib/python3.5/dist-packages/airflow/www/app.py:23: FlaskWTFDeprecationWarning: ""flask_wtf.CsrfProtect"" has been renamed to ""CSRFProtect"" and will be removed in 1.0. [2017-12-19 11:46:33,124] {base_task_runner.py:95} INFO - Subtask: csrf = CsrfProtect() [2017-12-19 11:46:33,344] {base_task_runner.py:95} INFO - Subtask: Traceback (most recent call last): [2017-12-19 11:46:33,344] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/bin/airflow"", line 28, in <module> [2017-12-19 11:46:33,344] {base_task_runner.py:95} INFO - Subtask: args.func(args) [2017-12-19 11:46:33,344] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 329, in run [2017-12-19 11:46:33,344] {base_task_runner.py:95} INFO - Subtask: with open(args.cfg_path, 'r') as conf_file: [2017-12-19 11:46:33,344] {base_task_runner.py:95} INFO - Subtask: PermissionError: [Errno 13] Permission denied: '/tmp/tmpicu87_au' [2017-12-19 11:46:36,770] {jobs.py:2125} INFO - Task exited with return code 1"
218,AIRFLOW-2047,AIRFLOW,1517334034000,1594641606000,Bug,Resolved,Critical,2,452,"airflow is insecure by default # [API Documentation|https://airflow.apache.org/api.html#authentication] states that the API is open/insecure by default. # [Security Documentation|https://airflow.apache.org/security.html#security] does not mention the API. Can we either: a) Disable the API by default, and have instructions on how to enable the experimental API? or b) Place a warning on the security documentation page that the API is open by default."
219,AIRFLOW-2059,AIRFLOW,1517607875000,1558123377000,Bug,Resolved,Critical,2,3759,"taskinstance query is awful, un-indexed, and does not scale The page at /admin/taskinstance/ can reach a point where it blocks loading the page and crushes the database. It appears this is because the task_instance.job_id column is unindexed. On our database, getting the results for this query took over four minutes, locking the table for the duration. 500 rows in set (4 min 8.93 sec) Query: {code:java} SELECT task_instance.task_id AS task_instance_task_id, task_instance.dag_id AS task_instance_dag_id, task_instance.execution_date AS task_instance_execution_date, task_instance.start_date AS task_instance_start_date, task_instance.end_date AS task_instance_end_date, task_instance.duration AS task_instance_duration, task_instance.state AS task_instance_state, task_instance.try_number AS task_instance_try_number, task_instance.hostname AS task_instance_hostname, task_instance.unixname AS task_instance_unixname, task_instance.job_id AS task_instance_job_id, task_instance.pool AS task_instance_pool, task_instance.queue AS task_instance_queue, task_instance.priority_weight AS task_instance_priority_weight, task_instance.operator AS task_instance_operator, task_instance.queued_dttm AS task_instance_queued_dttm, task_instance.pid AS task_instance_pid FROM task_instance ORDER BY task_instance.job_id DESC LIMIT 500; {code} Profile, explain: {code:java} <redacted>:airflow> EXPLAIN SELECT task_instance.task_id AS task_instance_task_id, task_instance.dag_id AS task_instance_dag_id, task_instance.execution_date AS task_instance_execution_date, task_instance.start_date AS task_instance_start_date, task_instance.end_date AS task_instance_end_date, task_instance.duration AS task_instance_duration, task_instance.state AS task_instance_state, task_instance.try_number AS task_instance_try_number, task_instance.hostname AS task_instance_hostname, task_instance.unixname AS task_instance_unixname, task_instance.job_id AS task_instance_job_id, task_instance.pool AS task_instance_pool, task_instance.queue AS task_instance_queue, task_instance.priority_weight AS task_instance_priority_weight, task_instance.operator AS task_instance_operator, task_instance.queued_dttm AS task_instance_queued_dttm, task_instance.pid AS task_instance_pid -> FROM task_instance ORDER BY task_instance.job_id DESC -> LIMIT 500; +----+-------------+---------------+------+---------------+------+---------+------+---------+----------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+---------------+------+---------------+------+---------+------+---------+----------------+ | 1 | SIMPLE | task_instance | ALL | NULL | NULL | NULL | NULL | 2542776 | Using filesort | +----+-------------+---------------+------+---------------+------+---------+------+---------+----------------+ 1 row in set (0.00 sec) <redacted>:airflow> select count(*) from task_instance; +----------+ | count(*) | +----------+ | 2984749 | +----------+ 1 row in set (1.67 sec) <redacted>:airflow> show profile for query 2; +----------------------+------------+ | Status | Duration | +----------------------+------------+ | starting | 0.000157 | | checking permissions | 0.000017 | | Opening tables | 0.000033 | | init | 0.000046 | | System lock | 0.000017 | | optimizing | 0.000010 | | statistics | 0.000022 | | preparing | 0.000020 | | Sorting result | 0.000010 | | executing | 0.000008 | | Sending data | 0.000151 | | Creating sort index | 248.955841 | | end | 0.015358 | | query end | 0.000012 | | closing tables | 0.000019 | | freeing items | 0.000549 | | logging slow query | 0.000007 | | logging slow query | 0.000901 | | cleaning up | 0.000030 | +----------------------+------------+ 19 rows in set, 1 warning (0.00 sec) {code}"
220,AIRFLOW-2065,AIRFLOW,1517823101000,1537526663000,Bug,Resolved,Critical,2,3107,"Worker logging can raise FileExistsError when more than one process execute concurrently Hello, We started observing random failing during the execution of our dags after upgrading to 1.9.0. After careful debugging, we noticing the following exception in the worker logs: {noformat} Traceback (most recent call last): File ""/projects/airflow-hadoop/anaconda3/lib/python3.6/logging/config.py"", line 558, in configure handler = self.configure_handler(handlers[name]) File ""/projects/airflow-hadoop/anaconda3/lib/python3.6/logging/config.py"", line 731, in configure_handler result = factory(**kwargs) File ""/projects/airflow-hadoop/anaconda3/lib/python3.6/site-packages/airflow/utils/log/file_processor_handler.py"", line 48, in __init__ os.makedirs(self._get_log_directory()) File ""/projects/airflow-hadoop/anaconda3/lib/python3.6/os.py"", line 220, in makedirs mkdir(name, mode) FileExistsError: [Errno 17] File exists: '/projects/airflow-hadoop/airflow/logs/scheduler/2018-02-05' During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/projects/airflow-hadoop/anaconda3/bin/airflow"", line 16, in <module> from airflow import configuration File ""/projects/airflow-hadoop/anaconda3/lib/python3.6/site-packages/airflow/__init__.py"", line 31, in <module> from airflow import settings File ""/projects/airflow-hadoop/anaconda3/lib/python3.6/site-packages/airflow/settings.py"", line 148, in <module> configure_logging() File ""/projects/airflow-hadoop/anaconda3/lib/python3.6/site-packages/airflow/logging_config.py"", line 75, in configure_logging raise e File ""/projects/airflow-hadoop/anaconda3/lib/python3.6/site-packages/airflow/logging_config.py"", line 70, in configure_logging dictConfig(logging_config) File ""/projects/airflow-hadoop/anaconda3/lib/python3.6/logging/config.py"", line 795, in dictConfig dictConfigClass(config).configure() File ""/projects/airflow-hadoop/anaconda3/lib/python3.6/logging/config.py"", line 566, in configure '%r: %s' % (name, e)) ValueError: Unable to configure handler 'file.processor': [Errno 17] File exists: '/projects/airflow-hadoop/airflow/logs/scheduler/2018-02-05 {noformat} As you can see, an exception is raised when trying to create the directory where to store the executor logs. This can happen if two tasks are scheduled are the exact same time on the same worker. It appears to be the case here : {noformat} [2018-02-05 02:10:07,886] \{celery_executor.py:50} INFO - Executing command in Celery: airflow run xxxx pairing_sensor_check 2018-02-04T02:10:00 --local --pool sensor -sd /projects/airflow-hadoop/airflow/dags/flow.py [2018-02-05 02:10:07,908] \{celery_executor.py:50} INFO - Executing command in Celery: airflow run yyy pairing_sensor_check 2018-02-04T02:10:00 --local --pool sensor -sd /projects/airflow-hadoop/airflow/dags/flow.py {noformat} Culprits is here: [https://github.com/apache/incubator-airflow/blob/v1-9-stable/airflow/utils/log/file_processor_handler.py#L47-L48] (not fixed in master) A simple fix would be to wrap the {{makedirs}} command into a {{try}} / {{catch}} block. Thanks, Sbastien"
221,AIRFLOW-2076,AIRFLOW,1517990255000,1550053529000,Bug,Closed,Critical,2,273,"[2017-11-21 16:12:40,961] {jobs.py:187} DEBUG - [heart] Boom. I used airflow submit spark application, the airflow DAG always print DEBUG - [heart] Boom. When my spark application is finished, the print is still onand the DAG cannot finished! I was used the LocalExecutor."
222,AIRFLOW-2103,AIRFLOW,1518465347000,1536501548000,Bug,Resolved,Critical,2,1918,"Authentication using password_auth backend prevents webserver from running airflow webserver fails to run with config {code:java} [webserver] authenticate = True auth_backend = airflow.contrib.auth.backends.password_auth {code} and errors out with following traceback {code:java} Traceback (most recent call last): File ""/usr/local/bin/airflow"", line 4, in <module> __import__('pkg_resources').run_script('apache-airflow==1.10.0.dev0+incubating', 'airflow') File ""/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 750, in run_script self.require(requires)[0].run_script(script_name, ns) File ""/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 1527, in run_script exec(code, namespace, namespace) File ""/usr/local/lib/python3.6/site-packages/apache_airflow-1.10.0.dev0+incubating-py3.6.egg/EGG-INFO/scripts/airflow"", line 27, in <module> args.func(args) File ""/usr/local/lib/python3.6/site-packages/apache_airflow-1.10.0.dev0+incubating-py3.6.egg/airflow/bin/cli.py"", line 696, in webserver app = cached_app(conf) File ""/usr/local/lib/python3.6/site-packages/apache_airflow-1.10.0.dev0+incubating-py3.6.egg/airflow/www/app.py"", line 176, in cached_app app = create_app(config, testing) File ""/usr/local/lib/python3.6/site-packages/apache_airflow-1.10.0.dev0+incubating-py3.6.egg/airflow/www/app.py"", line 63, in create_app from airflow.www import views File ""/usr/local/lib/python3.6/site-packages/apache_airflow-1.10.0.dev0+incubating-py3.6.egg/airflow/www/views.py"", line 97, in <module> login_required = airflow.login.login_required AttributeError: module 'airflow.contrib.auth.backends.password_auth' has no attribute 'login_required' {code} Broke at [https://github.com/apache/incubator-airflow/pull/2730] with the changing of {code:java} from flask_login import login_required, current_user, logout_user {code} to {code:java} from flask_login import current_user {code}"
223,AIRFLOW-2113,AIRFLOW,1518727820000,1518774582000,Bug,Resolved,Critical,2,3220,"Address missing DagRun callbacks This originally arose from the missing notification from the on_failure and on_success callback at the dag level. The stack trace is as follows: {code:java} [2018-02-07 07:00:08,145] \{models.py:2984} DagFileProcessor172 INFO - Executing dag callback function: <pandora_airflow_plugin.hooks.general_notification_hook .GeneralNotifyFailed instance at 0x7fec9d8ad368> [2018-02-07 07:00:08,161] \{models.py:168} DagFileProcessor172 INFO - Filling up the DagBag from /home/charon/.virtualenvs/airflow/airflow_home/dags Dag: <DAG: hive_migration_dag>, paused: False Dag: <DAG: continuously_refresh_ui>, paused: False Dag: <DAG: consul_ttl_check>, paused: False Dag: <DAG: example_analytics_dag>, paused: False Dag: <DAG: postgres_vacuum>, paused: False [2018-02-07 07:00:12,103] \{jobs.py:354} DagFileProcessor172 ERROR - Got an exception! Propagating... Traceback (most recent call last): File ""/home/charon/.virtualenvs/airflow/local/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper pickle_dags) File ""/home/charon/.virtualenvs/airflow/local/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/home/charon/.virtualenvs/airflow/local/lib/python2.7/site-packages/airflow/jobs.py"", line 1586, in process_file self._process_dags(dagbag, dags, ti_keys_to_schedule) File ""/home/charon/.virtualenvs/airflow/local/lib/python2.7/site-packages/airflow/jobs.py"", line 1175, in _process_dags dag_run = self.create_dag_run(dag) File ""/home/charon/.virtualenvs/airflow/local/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/home/charon/.virtualenvs/airflow/local/lib/python2.7/site-packages/airflow/jobs.py"", line 747, in create_dag_run dag.handle_callback(dr, success=False, reason='dagrun_timeout', session=session) File ""/home/charon/.virtualenvs/airflow/local/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/home/charon/.virtualenvs/airflow/local/lib/python2.7/site-packages/airflow/models.py"", line 2990, in handle_callback d = dagrun.dag or DagBag().get_dag(dag_id=dagrun.dag_id) File ""/home/charon/.virtualenvs/airflow/local/lib/python2.7/site-packages/sqlalchemy/orm/attributes.py"", line 237, in __get__ return self.impl.get(instance_state(instance), dict_) File ""/home/charon/.virtualenvs/airflow/local/lib/python2.7/site-packages/sqlalchemy/orm/attributes.py"", line 579, in get value = state._load_expired(state, passive) File ""/home/charon/.virtualenvs/airflow/local/lib/python2.7/site-packages/sqlalchemy/orm/state.py"", line 592, in _load_expired self.manager.deferred_scalar_loader(self, toload) File ""/home/charon/.virtualenvs/airflow/local/lib/python2.7/site-packages/sqlalchemy/orm/loading.py"", line 644, in load_scalar_attributes (state_str(state))) DetachedInstanceError: Instance <DagRun at 0x7fec9d8c8e90> is not bound to a Session; attribute refresh operation cannot proceed [2018-02-07 07:00:31,003] \{jobs.py:343} DagFileProcessor208 INFO - Started process (PID=7813) to work on /home/charon/.virtualenvs/airflow/airflow_home/dags/c haron-airflow/dags/inapp_vendor_sku_breakdown.py\ {code}"
224,AIRFLOW-2115,AIRFLOW,1518764850000,1555353766000,Task,Resolved,Critical,2,369,Default Airflow.cfg & Airflow Web UI contains links to pythonhosted The default `airflow.cfg` file points to pythonhosted docs which should be fixed https://github.com/apache/incubator-airflow/blob/1e36b37b68ab354d1d7d1d1d3abd151ce2a7cac7/airflow/config_templates/default_airflow.cfg#L227 Also Airflow WebUI contains links to old documentations located at PythonHosted.
225,AIRFLOW-2130,AIRFLOW,1519173952000,1555353765000,Bug,Resolved,Critical,2,2421,"Many Operators are missing from the docs * BaseSensorOperator references the wrong import path, so the autodoc fails * In the core operators, these are missing: ** airflow.operators.check_operator.CheckOperator ** airflow.operators.check_operator.IntervalCheckOperator ** airflow.operators.check_operator.ValueCheckOperator ** airflow.operators.hive_stats_operator.HiveStatsCollectionOperator ** airflow.operators.jdbc_operator.JdbcOperator ** airflow.operators.latest_only_operator.LatestOnlyOperator ** airflow.operators.mysql_operator.MySqlOperator ** airflow.operators.oracle_operator.OracleOperator ** airflow.operators.pig_operator.PigOperator ** airflow.operators.s3_file_transform_operator.S3FileTransformOperator ** airflow.operators.sqlite_operator.SqliteOperator ** airflow.operators.mysql_to_hive.MySqlToHiveTransfer ** airflow.operators.presto_to_mysql.PrestoToMySqlTransfer ** airflow.operators.redshift_to_s3_operator.RedshiftToS3Transfer * In contrib.operators, these are missing: ** airflow.contrib.operators.awsbatch_operator.AWSBatchOperator ** airflow.contrib.operators.druid_operator.DruidOperator ** airflow.contrib.operators.emr_add_steps_operator.EmrAddStepsOperator ** airflow.contrib.operators.emr_create_job_flow_operator.EmrCreateJobFlowOperator ** airflow.contrib.operators.emr_terminate_job_flow_operator.EmrTerminateJobFlowOperator ** airflow.contrib.operators.gcs_to_bq.GoogleCloudStorageToBigQueryOperator ** airflow.contrib.operators.jira_operator.JiraOperator ** airflow.contrib.operators.kubernetes_pod_operator.KubernetesPodOperator ** airflow.contrib.operators.mlengine_operator.MLEngineBatchPredictionOperator ** airflow.contrib.operators.mlengine_operator.MLEngineModelOperator ** airflow.contrib.operators.mlengine_operator.MLEngineVersionOperator ** airflow.contrib.operators.mlengine_operator.MLEngineTrainingOperator ** airflow.contrib.operators.mysql_to_gcs.MySqlToGoogleCloudStorageOperator ** airflow.contrib.operators.postgres_to_gcs_operator.PostgresToGoogleCloudStorageOperator ** airflow.contrib.operators.sftp_operator.SFTPOperator ** airflow.contrib.operators.spark_jdbc_operator.SparkJDBCOperator ** airflow.contrib.operators.spark_sql_operator.SparkSqlOperator ** airflow.contrib.operators.spark_submit_operator.SparkSubmitOperator ** airflow.contrib.operators.sqoop_operator.SqoopOperator ** airflow.contrib.operators.hive_to_dynamodb.HiveToDynamoDBTransferOperator"
226,AIRFLOW-2131,AIRFLOW,1519174420000,1555353765000,Bug,Resolved,Critical,2,496,"API Reference includes confusing docs from airflow.utils.AirflowImporter The generated API documentation includes {{automodule}} declarations for several modules (hooks and operators) that end up pulling in docs from {{airflow.utils.helpers.AirflowImporter}}. This leads to the confusing situation for new users who think they're reading docs about what Hooks are, but are instead reading unlabeled docs about the seemingly-deprecated AirflowImporter. Like so: !image-2018-02-20-16-53-04-572.png!"
227,AIRFLOW-2148,AIRFLOW,1519626928000,1559099777000,Bug,Open,Critical,2,2117,"getting error ""Field 'execution_date' doesn't have a default value"" with airflow using celery/local executor [2018-02-26 05:55:07,702] \{base_task_runner.py:98} INFO - Subtask: sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (pymysql.err.InternalError) (1364, ""Field 'execution_date' doesn't have a default value"") [SQL: 'INSERT INTO task_instance (task_id, dag_id, start_date, end_date, duration, state, try_number, max_tries, hostname, unixname, job_id, pool, queue, priority_weight, operator, queued_dttm, pid) VALUES (%(task_id)s, %(dag_id)s, %(start_date)s, %(end_date)s, %(duration)s, %(state)s, %(try_number)s, %(max_tries)s, %(hostname)s, %(unixname)s, %(job_id)s, %(pool)s, %(queue)s, %(priority_weight)s, %(operator)s, %(queued_dttm)s, %(pid)s)'] [parameters: \{'job_id': None, 'queue': 'default', 'pid': None, 'max_tries': 0, 'dag_id': 'example_trigger_target_dag', 'try_number': 0, 'priority_weight': 4, 'duration': None, 'task_id': 'validation_this', 'operator': None, 'pool': None, 'state': None, 'hostname': '', 'start_date': None, 'unixname': 'root', 'queued_dttm': None, 'end_date': None}] (Background on this error at: http://sqlalche.me/e/2j85) [2018-02-26 05:55:07,655] \{base_task_runner.py:98} INFO - Subtask: sqlalchemy.exc.IntegrityError: (pymysql.err.IntegrityError) (1062, ""Duplicate entry 'example_trigger_target_dag-2018-02-26 05:55:07.000000' for key 'dag_id'"") [SQL: 'INSERT INTO dag_run (dag_id, execution_date, start_date, end_date, state, run_id, external_trigger, conf) VALUES (%(dag_id)s, now(), now(), %(end_date)s, %(state)s, %(run_id)s, %(external_trigger)s, %(conf)s)'] [parameters: \{'dag_id': 'example_trigger_target_dag', 'end_date': None, 'state': 'running', 'conf': b'\x80\x04\x95\x1f\x00\x00\x00\x00\x00\x00\x00}\x94\x8c\x07message\x94\x8c\x0eHello World 98\x94s.', 'run_id': 'trig__2018-02-26T05:55:07.485541', 'external_trigger': 1}] (Background on this error at: http://sqlalche.me/e/gkpj)"
228,AIRFLOW-2175,AIRFLOW,1520256127000,1558123388000,Bug,Resolved,Critical,2,2481,"Failed to upgradedb 1.8.2 -> 1.9.0 We've got airflow installation with hundreds of DAGs and thousands of tasks. During upgrade (1.8.2 -> 1.9.0) we've got following error. After analyzing stacktrace i've found that it's most likely caused by None value in 'fileloc' field of Dag column. I checked database and indeed we've got one record with such value: {code:java} SELECT COUNT(*) FROM dag WHERE fileloc IS NULL; 1 SELECT COUNT(*) FROM dag; 343 {code} {code:java} Traceback (most recent call last): File ""/usr/local/bin/airflow"", line 27, in <module> args.func(args) File ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 913, in upgradedb db_utils.upgradedb() File ""/usr/local/lib/python2.7/dist-packages/airflow/utils/db.py"", line 320, in upgradedb command.upgrade(config, 'heads') File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 174, in upgrade script.run_env() File ""/usr/local/lib/python2.7/dist-packages/alembic/script/base.py"", line 416, in run_env util.load_python_file(self.dir, 'env.py') File ""/usr/local/lib/python2.7/dist-packages/alembic/util/pyfiles.py"", line 93, in load_python_file module = load_module_py(module_id, path) File ""/usr/local/lib/python2.7/dist-packages/alembic/util/compat.py"", line 79, in load_module_py mod = imp.load_source(module_id, path, fp) File ""/usr/local/lib/python2.7/dist-packages/airflow/migrations/env.py"", line 86, in <module> run_migrations_online() File ""/usr/local/lib/python2.7/dist-packages/airflow/migrations/env.py"", line 81, in run_migrations_online context.run_migrations() File ""<string>"", line 8, in run_migrations File ""/usr/local/lib/python2.7/dist-packages/alembic/runtime/environment.py"", line 807, in run_migrations self.get_context().run_migrations(**kw) File ""/usr/local/lib/python2.7/dist-packages/alembic/runtime/migration.py"", line 321, in run_migrations step.migration_fn(**kw) File ""/usr/local/lib/python2.7/dist-packages/airflow/migrations/versions/cc1e65623dc7_add_max_tries_column_to_task_instance.py"", line 63, in upgrade dag = dagbag.get_dag(ti.dag_id) File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 232, in get_dag filepath=orm_dag.fileloc, only_if_updated=False) File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 249, in process_file if not os.path.isfile(filepath): File ""/usr/lib/python2.7/genericpath.py"", line 29, in isfile st = os.stat(path) TypeError: coercing to Unicode: need string or buffer, NoneType found{code}"
229,AIRFLOW-2192,AIRFLOW,1520461529000,1547301054000,Bug,Resolved,Critical,2,446,"Google Authentication issue with non-latin1 username with mysql backend It's a weird, I tried to login using Google Authentication and Airflow returned ""_UnicodeEncodeError: 'latin-1' codec can't encode character u'\u200b' in position 8: ordinal not in range(256)_"". So, my google profile was: _First Name: Fernando_ _Last Name: Ike_ I changed my profile just ""_Ike""_ in the ""First Name"" and now I can login. In the attachment is the log related:"
230,AIRFLOW-2196,AIRFLOW,1520506801000,1561729949000,Bug,Closed,Critical,2,1192,"DAG are not executing with sudo command DAGS are queued but not starting, below is the log when the workers are executed with sudo ##########:~ # airflow worker [2018-03-08 05:40:17,204] \{configuration.py:206} WARNING - section/key [celery/celery_ssl_active] not found in config [2018-03-08 05:40:17,204] \{default_celery.py:41} WARNING - Celery Executor will run without SSL [2018-03-08 05:40:17,205] \{__init__.py:45} INFO - Using executor CeleryExecutor Running a worker with superuser privileges when the worker accepts messages serialized with pickle is a very bad idea! If you really want to continue then you have to set the C_FORCE_ROOT environment variable (but please think about this before you do). User information: uid=0 euid=0 gid=0 egid=0 ###########:~ # [2018-03-08 05:40:17,997] \{configuration.py:206} WARNING - section/key [celery/celery_ssl_active] not found in config [2018-03-08 05:40:17,997] \{default_celery.py:41} WARNING - Celery Executor will run without SSL [2018-03-08 05:40:17,998] \{__init__.py:45} INFO - Using executor CeleryExecutor Starting flask [2018-03-08 05:40:18,081] \{_internal.py:88} INFO - * Running on http://0.0.0.0:8082/ (Press CTRL+C to quit)"
231,AIRFLOW-2201,AIRFLOW,1520552311000,1620203832000,Bug,Resolved,Critical,2,1859,"Scheduler processes dags every second Building latest master. The scheduler appears to be running continuously, not respecting scheduler_heartbeat_sec = 30 Logs are filled up with messages like: Mar8 23:20:36 netdocker1-eastus2 INFO :[2018-03-08 23:20:36,826] \{dag_processing.py:468} INFO - Processor for /var/lib/airflow/dags/test_nt.py finished Mar8 23:20:36 netdocker1-eastus2 INFO :[2018-03-08 23:20:36,831] \{dag_processing.py:543} INFO - Started a process (PID: 64) to generate tasks for /var/lib/airflow/dags/test_nt.py Mar8 23:20:37 netdocker1-eastus2 INFO :[2018-03-08 23:20:37,834] \{dag_processing.py:468} INFO - Processor for /var/lib/airflow/dags/test_nt.py finished Mar8 23:20:37 netdocker1-eastus2 INFO :[2018-03-08 23:20:37,838] \{dag_processing.py:543} INFO - Started a process (PID: 66) to generate tasks for /var/lib/airflow/dags/test_nt.py Mar8 23:20:38 netdocker1-eastus2 INFO :[2018-03-08 23:20:38,842] \{dag_processing.py:468} INFO - Processor for /var/lib/airflow/dags/test_nt.py finished Mar8 23:20:38 netdocker1-eastus2 INFO :[2018-03-08 23:20:38,846] \{dag_processing.py:543} INFO - Started a process (PID: 68) to generate tasks for /var/lib/airflow/dags/test_nt.py Mar8 23:20:39 netdocker1-eastus2 INFO :[2018-03-08 23:20:39,850] \{dag_processing.py:468} INFO - Processor for /var/lib/airflow/dags/test_nt.py finished Mar8 23:20:39 netdocker1-eastus2 INFO :[2018-03-08 23:20:39,856] \{dag_processing.py:543} INFO - Started a process (PID: 70) to generate tasks for /var/lib/airflow/dags/test_nt.py Mar8 23:20:40 netdocker1-eastus2 INFO :[2018-03-08 23:20:40,863] \{dag_processing.py:468} INFO - Processor for /var/lib/airflow/dags/test_nt.py finished Mar8 23:20:40 netdocker1-eastus2 INFO :[2018-03-08 23:20:40,867] \{dag_processing.py:543} INFO - Started a process (PID: 72) to generate tasks for /var/lib/airflow/dags/test_nt.py"
232,AIRFLOW-2230,AIRFLOW,1521572706000,1555353764000,Bug,Closed,Critical,2,515,"[possible dup] tutorial does not specify initdb/upgradedb prerequisite command, although quick start does Quick start specifies `initdb` but full tutorial docs afaict do not specify this prerequisite command. If this is not run before everything else you end up with: sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: connection [SQL: 'SELECT connection.conn_id AS connection_conn_id \nFROM connection GROUP BY connection.conn_id'] (Background on this error at: [http://sqlalche.me/e/e3q8])"
233,AIRFLOW-2232,AIRFLOW,1521591826000,1521597815000,Bug,Resolved,Critical,2,351,DAG must be imported for airflow dag discovery repro: put the following in the dags/ directory ----------------------------- from my_dags import MyDag d = MyDag() . # this is an airflow.DAG ````````````````````````````` Expected: airflow list_dags lists the dag Actual: airflow does not list the dag unless an unused `from airflow import DAG` is added
234,AIRFLOW-2272,AIRFLOW,1522443303000,1525723094000,Bug,Closed,Critical,2,3070,"Travis CI is failing builds due to oracle-java8-installer failing to install via apt-get All the PR builds in TravisCI are failing with the following apt-get error: {code:java} --2018-03-30 17:56:23-- (try: 5) http://download.oracle.com/otn-pub/java/jdk/8u161-b12/2f38c3b165be4555a1fa6e98c45e0808/jdk-8u161-linux-x64.tar.gz Connecting to download.oracle.com (download.oracle.com)|23.45.144.164|:80... failed: Connection timed out. Giving up. apt-get install failed $ cat ~/apt-get-update.log Ign:1 http://us-central1.gce.archive.ubuntu.com/ubuntu trusty InRelease Hit:2 http://us-central1.gce.archive.ubuntu.com/ubuntu trusty-updates InRelease Hit:3 http://us-central1.gce.archive.ubuntu.com/ubuntu trusty-backports InRelease Hit:4 http://us-central1.gce.archive.ubuntu.com/ubuntu trusty Release Ign:5 http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.4 InRelease Ign:6 http://dl.google.com/linux/chrome/deb stable InRelease Hit:7 http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.4 Release Hit:8 http://security.ubuntu.com/ubuntu trusty-security InRelease Ign:9 http://toolbelt.heroku.com/ubuntu ./ InRelease Get:10 http://dl.bintray.com/apache/cassandra 39x InRelease [3,168 B] Hit:11 http://ppa.launchpad.net/chris-lea/redis-server/ubuntu trusty InRelease Hit:13 https://download.docker.com/linux/ubuntu trusty InRelease Hit:12 http://toolbelt.heroku.com/ubuntu ./ Release Hit:15 http://dl.google.com/linux/chrome/deb stable Release Hit:16 http://apt.postgresql.org/pub/repos/apt trusty-pgdg InRelease Hit:18 https://dl.hhvm.com/ubuntu trusty InRelease Ign:19 http://ppa.launchpad.net/couchdb/stable/ubuntu trusty InRelease Hit:22 https://packagecloud.io/computology/apt-backport/ubuntu trusty InRelease Hit:23 https://packagecloud.io/github/git-lfs/ubuntu trusty InRelease Hit:24 https://packagecloud.io/rabbitmq/rabbitmq-server/ubuntu trusty InRelease Hit:25 http://ppa.launchpad.net/git-core/ppa/ubuntu trusty InRelease Hit:26 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu trusty InRelease Hit:27 http://ppa.launchpad.net/pollinate/ppa/ubuntu trusty InRelease Hit:28 http://ppa.launchpad.net/webupd8team/java/ubuntu trusty InRelease Hit:29 http://ppa.launchpad.net/couchdb/stable/ubuntu trusty Release Fetched 3,168 B in 2s (1,102 B/s) Reading package lists... W: http://ppa.launchpad.net/couchdb/stable/ubuntu/dists/trusty/Release.gpg: Signature by key 15866BAFD9BCC4F3C1E0DFC7D69548E1C17EAB57 uses weak digest algorithm (SHA1) The command ""sudo -E apt-get -yq --no-install-suggests --no-install-recommends --force-yes install slapd ldap-utils openssh-server mysql-server-5.6 mysql-client-core-5.6 mysql-client-5.6 krb5-user krb5-kdc krb5-admin-server oracle-java8-installer python-selinux"" failed and exited with 100 during .{code} It looks like this is due to the configuration in the .travis.yml installing {{oracle-java8-installer}}: {code} apt: packages: - slapd - ldap-utils - openssh-server - mysql-server-5.6 - mysql-client-core-5.6 - mysql-client-5.6 - krb5-user - krb5-kdc - krb5-admin-server - oracle-java8-installer - python-selinux {code}"
235,AIRFLOW-2305,AIRFLOW,1523297769000,1523300871000,Bug,Resolved,Critical,2,37,Fix CI failure caused by AIRFLOW-2027
236,AIRFLOW-2328,AIRFLOW,1523914974000,1524292692000,Bug,Resolved,Critical,2,168,S3ToGoogleCloudStorageOperator doesn't accept empty GCS blobs The aforementioned operator refuses syncing if the GCS path is simply a bucket (the object path is empty).
237,AIRFLOW-2340,AIRFLOW,1524080561000,1556721639000,Bug,Resolved,Critical,2,571,"SQLalchemy pessimistic connection handling not working Our scheduler keeps crashing, about once a day. It seems to be triggered by a failure to connect to the postgresql database, but then it doesn't recover and crashes the scheduler over and over. The scheduler runs in a container in our environment, so after several container restarts, docker gives up and the container stays down. Airflow should be able to recover from a connection failure without blowing up the container altogether. Perhaps some exponential backoff is needed? See attached log from the scheduler."
238,AIRFLOW-2362,AIRFLOW,1524498528000,1535911397000,Bug,Resolved,Critical,2,226,DockerOperator imports Client should import APIClient The docker-py package changed its API over a year ago to rename the Client to APIClient: https://github.com/docker/docker-py/commit/f5ac10c469fca252e69ae780749f4ec6fe369789
239,AIRFLOW-2384,AIRFLOW,1524780710000,1524940027000,Bug,Resolved,Critical,2,2073,"Flask 0.12.3+ breaks Airflow webserver Flask 0.12.3 and 1.0.0 were released about an hour ago with breaking changes to Airflows webserver {code:java} File ""/usr/local/lib/python3.6/site-packages/gunicorn/workers/sync.py"", line 135, in handle self.handle_request(listener, req, client, addr) File ""/usr/local/lib/python3.6/site-packages/gunicorn/workers/sync.py"", line 176, in handle_request respiter = self.wsgi(environ, resp.start_response) File ""/usr/local/lib/python3.6/site-packages/werkzeug/wsgi.py"", line 826, in __call__ return app(environ, start_response) File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1997, in __call__ return self.wsgi_app(environ, start_response) File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1978, in wsgi_app ctx.push() File ""/usr/local/lib/python3.6/site-packages/flask/ctx.py"", line 332, in push self.session = self.app.open_session(self.request) File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 912, in open_session return self.session_interface.open_session(self, request) File ""/usr/local/lib/python3.6/site-packages/flask/sessions.py"", line 324, in open_session s = self.get_signing_serializer(app) File ""/usr/local/lib/python3.6/site-packages/flask/sessions.py"", line 321, in get_signing_serializer signer_kwargs=signer_kwargs) File ""/usr/local/lib/python3.6/site-packages/itsdangerous.py"", line 519, in __init__ self.is_text_serializer = is_text_serializer(serializer) File ""/usr/local/lib/python3.6/site-packages/itsdangerous.py"", line 69, in is_text_serializer return isinstance(serializer.dumps({}), text_type) File ""/usr/local/lib/python3.6/site-packages/flask/sessions.py"", line 85, in dumps return json.dumps(_tag(value), separators=(',', ':')) File ""/usr/local/lib/python3.6/site-packages/flask/json/__init__.py"", line 177, in dumps _dump_arg_defaults(kwargs) File ""/usr/local/lib/python3.6/site-packages/flask/json/__init__.py"", line 98, in _dump_arg_defaults bp.json_encoder if bp and bp.json_encoder AttributeError: 'Blueprint' object has no attribute 'json_encoder'{code}"
240,AIRFLOW-2426,AIRFLOW,1525662478000,1572292946000,Test,Resolved,Critical,2,83,Add Google Cloud Storage Hook tests The Google Cloud Storage Hook is missing tests.
241,AIRFLOW-2435,AIRFLOW,1525766044000,1526323997000,Improvement,Resolved,Critical,2,226,"Add launch_type to ECSOperator to allow FARGATE Currently we can start ECS tasks via the task definition on EC2. It would be cool if we could also supply an extra parameter, launch_type, to launch the airflow tasks on FARGATE."
242,AIRFLOW-2469,AIRFLOW,1526437428000,1558124651000,Bug,Resolved,Critical,2,777,"example task in documentation causes dataflow operator to fail https://github.com/apache/incubator-airflow/blob/c7a472ed6b0d8a4720f57ba1140c8cf665757167/airflow/contrib/operators/dataflow_operator.py#L176 {noformat} t1 = DataflowTemplateOperator( task_id='datapflow_example', template='{{var.value.gcp_dataflow_base}}', parameters={ 'inputFile': ""gs://bucket/input/my_input.txt"", 'outputFile': ""gs://bucket/output/my_output.txt"" }, gcp_conn_id='gcp-airflow-service-account', dag=my-dag){noformat} If you actually name a dataflow task 'datapflow_example', the Google dataflow service will not accept the job because it is not named correctly. Dataflow job names can't have '_' in them. Strictly speaking, apache beam jobnames must adhere to the regex [a-z]([-a-z0-9]*[a-z0-9])?."
243,AIRFLOW-2470,AIRFLOW,1526450801000,1530108082000,Bug,Open,Critical,2,553,"Retry Time is not honored correctly The retry time that is mentioned in the Dag is not honored correctly. For example, we posted a Dag with retry count=2 and with a delay of 5 seconds and following are the observations : 1) We observed that when there was no load on scheduler, it took about 30 seconds to retry each time so time of 5 seconds was not being honored here 2)We tried this again where there was existing load on scheduler. In this case, it took 25 minutes for each retry. Expected : It should have retried in 5 seconds which is not the case"
244,AIRFLOW-2509,AIRFLOW,1527005971000,1555353762000,Improvement,Resolved,Critical,2,1060,"Separate Configuration page into separate how-to guides The existing ""Configuration"" page is attempting to be both a tutorial (teaching the basics of Airflow configuration & a minimal production deployment) as well as how-tos for specific tasks. I propose we separate Configuration into separate how-to guides, keeping the current sequence so that it can still maintain the tutorial properties (at least until a ""Deploying a Production Airflow Environment"" tutorial is written). There's a principle that the [distinct kinds of documentation|http://www.writethedocs.org/videos/eu/2017/the-four-kinds-of-documentation-and-why-you-need-to-understand-what-they-are-daniele-procida/] should be organized separately. The Django project does this [https://docs.djangoproject.com/en/2.0/] by splitting into * Tutorials * Topic guides (what Airflow calls Concepts) * Reference guides * How-to guides I think the same could apply well here. (This issue covers only How-to for Configuration. More work would be required to separate other docs into proper document types.)"
245,AIRFLOW-2561,AIRFLOW,1528133543000,1535911294000,Bug,Resolved,Critical,2,136,Typo in Email operator call to send_email There's a typo in the params in send_email. It should be mime_charset instead of mine_charset.
246,AIRFLOW-2620,AIRFLOW,1528987936000,1558131081000,Wish,Resolved,Critical,2,944,"New property called ""RetainSameConnection"" Some ETL stuff relies on the -high- reusability of the connections in the middle of the data flow, and part of this is achieved using temporary tables. I performed a test [1] in Airflow and the behaviour is the for each command that we use the ""PostgresOperator"" hits the database again, and if the DAG had a lot of connections it can be a nightmare. As I said in SO, for several cases where you have cases of processing some tables in stages the reuse a temporary table across multiple tasks can be very useful and plus: can remove a lot of pressure in the RDBMS cause you won't need to hit several times. That's why a purpose a feature of the connection called ""RetainSameConnection"" that will keep the same connection during the execution of the DAG. Reference: [1] - [My question in Stack Overflow|https://stackoverflow.com/questions/50858770/airflow-retain-the-same-database-connection/] about it"
247,AIRFLOW-2631,AIRFLOW,1529080146000,1559100848000,Bug,Open,Critical,2,815,"Worker fails to connect with MySQL DB I have containerized Airflow webserver, scheduler, flower and worker. I'm using MySQL for the backend and Redis for queuing. Everytime I spin up the worker node it throws the error below and all the Dags are stuck in running status. Logs from the Worker Node: File ""/usr/lib64/python2.7/site-packages/MySQLdb/connections.py"", line 193, in __init__ super(Connection, self).__init__(*args, **kwargs2) OperationalError: (_mysql_exceptions.OperationalError) (2002, ""Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2)"") [2018-06-15 10:21:56,005: ERROR/MainProcess] Pool callback raised exception: OperationalError('(_mysql_exceptions.OperationalError) (2002, ""Can\'t connect to local MySQL server through socket \'/var/lib/mysql/mysql.sock\' (2)"")',)"
248,AIRFLOW-2715,AIRFLOW,1530730956000,1553039530000,Improvement,Resolved,Critical,2,693,"Dataflow template operator dosenot support region parameter The DataflowTemplateOperator uses dataflow.projects.templates.launch which has a region parameter but only supports execution of the dataflow job in the us-central1 region. Alternatively there is another api, dataflow.projects.locations.templates.launch which supports execution of the template in all regional endpoints provided by google cloud. It would be great if, # The base REST API of this operator could be changed from ""dataflow.projects.templates.launch"" to ""dataflow.projects.locations.templates.launch"" # A templated region paramter was included in the operator to run the dataflow job in the requested regional endpoint."
249,AIRFLOW-2770,AIRFLOW,1531994108000,1545030330000,Improvement,Resolved,Critical,2,529,"kubernetes: add support for dag folder in the docker image currently the kube executor need to provider dag_volume_chain or git repo in the config file, but if the user has build dag into their docker image, they doesn't need to provider these two options, and they can manager their dag version by manager the docker image version. So I suppose we can add the a new configuration as kube.config.dag_folder_path along with dag_volume_chain and git repo. with this config, we can run the worker just from the dags in docker image."
250,AIRFLOW-2787,AIRFLOW,1532368811000,1550766705000,Bug,Resolved,Critical,2,1040,"Airflow scheduler dies on DAGs with NULL DagRun run_id When a DagRun is created with NULL run_id, the scheduler subprocess will crash when checking `is_backfill`: {noformat} Got an exception! Propagating... Traceback (most recent call last): File ""/usr/local/lib/python3.6/site-packages/airflow/jobs.py"", line 347, in helper pickle_dags) File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/usr/local/lib/python3.6/site-packages/airflow/jobs.py"", line 1583, in process_file self._process_dags(dagbag, dags, ti_keys_to_schedule) File ""/usr/local/lib/python3.6/site-packages/airflow/jobs.py"", line 1175, in _process_dags self._process_task_instances(dag, tis_out) File ""/usr/local/lib/python3.6/site-packages/airflow/jobs.py"", line 873, in _process_task_instances if run.is_backfill: File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 4257, in is_backfill if ""backfill"" in self.run_id: TypeError: argument of type 'NoneType' is not iterable {noformat}"
251,AIRFLOW-2814,AIRFLOW,1532747736000,1547068933000,Bug,Resolved,Critical,2,906,"Default Arg ""file_process_interval"" for class SchedulerJob is inconsistent with doc h2. Backgrond In [https://github.com/XD-DENG/incubator-airflow/blob/master/airflow/jobs.py#L592] , it was mentioned the default value of argument *file_process_interval* should be 3 minutes (*file_process_interval:* Parse and schedule each file no faster than this interval). The value is normally parsed from the default configuration. However, in the default config_template, its value is 0 rather than 180 seconds ([https://github.com/XD-DENG/incubator-airflow/blob/master/airflow/config_templates/default_airflow.cfg#L432] ). h2. Issue This means that actually that each file is parsed and scheduled without letting Airflow ""rest"". This conflicts with the design purpose (by default let it be 180 seconds) and may affect performance significantly. h2. My Proposal Change the value in the config template from 0 to 180."
252,AIRFLOW-2825,AIRFLOW,1532964298000,1536575089000,Bug,Resolved,Critical,2,268,"S3ToHiveTransfer operator may not may able to handle GZIP file with uppercase ext in S3 Because upper/lower case was not considered in the extension check, S3ToHiveTransfer operator may think a GZIP file with uppercase ext `.GZ` is not a GZIP file and raise exception."
253,AIRFLOW-2830,AIRFLOW,1533046507000,1535980404000,Bug,Open,Critical,2,1554,"Worker subprocess crash results in tasks failing without retry We ran across this fixed bug in production: [https://github.com/apache/incubator-airflow/pull/3040] Fair enough, it's fixed. However, that task had `retries=3` which never kicked in - that's a bug in its own right! I do see this in the documentation: {quote}Zombies & Undeads Task instances die all the time, usually as part of their normal life cycle, but sometimes unexpectedly. Zombie tasks are characterized by the absence of an heartbeat (emitted by the job periodically) and a running status in the database. {quote} I was not on call at the time so I don't have a full log of what happened with the task states. However, I am wondering if what happened looked something like this: * Scheduler detects that process needs to run * Scheduler changes state to ""queued"" * Scheduler adds to Celery queue * Worker pulls message off queue * Worker starts subprocess * Worker subprocess dies to bug when trying to load logging config, before changing task state to running * Worker never tries to actually run task, so it never sets task to ""up_for_retry"" * Message no longer exists in queue so worker won't grab task again * Scheduler never retries because the task wasn't ""up_for_retry"" * Scheduler never checks heartbeat because it's ""queued"", not ""running"" In general it's been disappointing to see so many ugly race conditions in Airflow. I'd love to see an Airflow enhancement proposal for converting the codebase to use a reliable state machine and better distributed system primitives."
254,AIRFLOW-2838,AIRFLOW,1533167785000,1549882145000,Bug,Closed,Critical,2,7930,"Installtion issues: Could not install packages due to an EnvironmentError Hello, Am trying to install airflow (sudo pip install apache-airflow) on my mac. Getting below error. Can someone help me with that issue? Could not install packages due to an EnvironmentError: [('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/parser.pyc', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/parser.pyc', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/parser.pyc'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/relativedelta.py', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/relativedelta.py', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/relativedelta.py'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/zoneinfo/zoneinfo-2010g.tar.gz', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/zoneinfo/zoneinfo-2010g.tar.gz', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/zoneinfo/zoneinfo-2010g.tar.gz'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/zoneinfo/__init__.py', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/zoneinfo/__init__.py', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/zoneinfo/__init__.py'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/zoneinfo/__init__.pyc', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/zoneinfo/__init__.pyc', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/zoneinfo/__init__.pyc'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/zoneinfo', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/zoneinfo', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/zoneinfo'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/tz.py', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/tz.py', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/tz.py'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/relativedelta.pyc', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/relativedelta.pyc', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/relativedelta.pyc'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/rrule.pyc', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/rrule.pyc', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/rrule.pyc'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/__init__.py', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/__init__.py', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/__init__.py'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/parser.py', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/parser.py', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/parser.py'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/tzwin.py', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/tzwin.py', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/tzwin.py'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/rrule.py', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/rrule.py', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/rrule.py'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/__init__.pyc', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/__init__.pyc', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/__init__.pyc'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/easter.py', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/easter.py', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/easter.py'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/easter.pyc', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/easter.pyc', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/easter.pyc'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/tz.pyc', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/tz.pyc', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/tz.pyc'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/tzwin.pyc', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/tzwin.pyc', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil/tzwin.pyc'""), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil', '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil', ""[Errno 1] Operation not permitted: '/private/tmp/pip-uninstall-bXYiIZ/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/dateutil'"")]"
255,AIRFLOW-2839,AIRFLOW,1533174752000,1555353759000,Improvement,Resolved,Critical,2,791,"Answer a question in doc: what if I have connections with the same conn_id in both metadata and environment variables? We can define connections either in metadata database or environment variables. However, how about there are connections with the same conn_id in both metadata database and environment variables? Will one of them be picked? Which one? Or a random one will be picked? The answer is: Airflow will check environment variables first.[https://github.com/apache/incubator-airflow/blob/master/airflow/hooks/base_hook.py#L70] The ""random picking"" mechanism will only be applicable for multiple connections with the same conn_id in metadata database (if there is no connection with the same conn_id defined in environment variables). This should be made clear in the documentation."
256,AIRFLOW-2844,AIRFLOW,1533311552000,1610532644000,Bug,Open,Critical,2,32653,"Airflow Logs BrokenPipeException I'm using a clustered Airflow environment where I have four AWS ec2-instances for the servers. *ec2-instances* - Server 1: Webserver, Scheduler, Redis Queue, PostgreSQL Database - Server 2: Webserver - Server 3: Worker - Server 4: Worker My setup has been working perfectly fine for three months now but sporadically about once a week I get a Broken Pipe Exception when Airflow is attempting to log something. {code:java} Log file isn't local. Fetching here: http://ip-1-2-3-4:8793/log/foobar/task_1/2018-07-13T00:00:00/1.log [2018-07-16 00:00:15,521] {cli.py:374} INFO - Running on host ip-1-2-3-4 [2018-07-16 00:00:15,698] {models.py:1197} INFO - Dependencies all met for <TaskInstance: foobar.task_1 2018-07-13 00:00:00 [queued]> [2018-07-16 00:00:15,710] {models.py:1197} INFO - Dependencies all met for <TaskInstance: foobar.task_1 2018-07-13 00:00:00 [queued]> [2018-07-16 00:00:15,710] {models.py:1407} INFO - -------------------------------------------------------------------------------- Starting attempt 1 of 1 -------------------------------------------------------------------------------- [2018-07-16 00:00:15,719] {models.py:1428} INFO - Executing <Task(OmegaFileSensor): task_1> on 2018-07-13 00:00:00 [2018-07-16 00:00:15,720] {base_task_runner.py:115} INFO - Running: ['bash', '-c', 'airflow run foobar task_1 2018-07-13T00:00:00 --job_id 1320 --raw -sd DAGS_FOLDER/datalake_digitalplatform_arl_workflow_schedule_test_2.py'] [2018-07-16 00:00:16,532] {base_task_runner.py:98} INFO - Subtask: [2018-07-16 00:00:16,532] {configuration.py:206} WARNING - section/key [celery/celery_ssl_active] not found in config [2018-07-16 00:00:16,532] {base_task_runner.py:98} INFO - Subtask: [2018-07-16 00:00:16,532] {default_celery.py:41} WARNING - Celery Executor will run without SSL [2018-07-16 00:00:16,534] {base_task_runner.py:98} INFO - Subtask: [2018-07-16 00:00:16,533] {_init_.py:45} INFO - Using executor CeleryExecutor [2018-07-16 00:00:16,597] {base_task_runner.py:98} INFO - Subtask: [2018-07-16 00:00:16,597] {models.py:189} INFO - Filling up the DagBag from /home/ec2-user/airflow/dags/datalake_digitalplatform_arl_workflow_schedule_test_2.py [2018-07-16 00:00:16,768] {cli.py:374} INFO - Running on host ip-1-2-3-4 [2018-07-16 00:16:24,931] {logging_mixin.py:84} WARNING -  Logging error  [2018-07-16 00:16:24,931] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-07-16 00:16:24,931] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/_init_.py"", line 996, in emit self.flush() [2018-07-16 00:16:24,932] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/_init_.py"", line 976, in flush self.stream.flush() [2018-07-16 00:16:24,932] {logging_mixin.py:84} WARNING - BrokenPipeError: [Errno 32] Broken pipe [2018-07-16 00:16:24,932] {logging_mixin.py:84} WARNING - Call stack: [2018-07-16 00:16:24,933] {logging_mixin.py:84} WARNING - File ""/usr/bin/airflow"", line 27, in <module> args.func(args) [2018-07-16 00:16:24,934] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/bin/cli.py"", line 392, in run pool=args.pool, [2018-07-16 00:16:24,934] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 50, in wrapper result = func(*args, **kwargs) [2018-07-16 00:16:24,934] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1488, in _run_raw_task result = task_copy.execute(context=context) [2018-07-16 00:16:24,934] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/sensors.py"", line 78, in execute while not self.poke(context): [2018-07-16 00:16:24,934] {logging_mixin.py:84} WARNING - File ""/home/ec2-user/airflow/plugins/custom_plugins.py"", line 35, in poke directory = os.listdir(full_path) [2018-07-16 00:16:24,934] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/utils/timeout.py"", line 36, in handle_timeout self.log.error(""Process timed out"") [2018-07-16 00:16:24,934] {logging_mixin.py:84} WARNING - Message: 'Process timed out' Arguments: () [2018-07-16 00:16:24,942] {models.py:1595} ERROR - Timeout Traceback (most recent call last): File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1488, in _run_raw_task result = task_copy.execute(context=context) File ""/usr/local/lib/python3.6/site-packages/airflow/operators/sensors.py"", line 78, in execute while not self.poke(context): File ""/home/ec2-user/airflow/plugins/custom_plugins.py"", line 35, in poke directory = os.listdir(full_path) File ""/usr/local/lib/python3.6/site-packages/airflow/utils/timeout.py"", line 37, in handle_timeout raise AirflowTaskTimeout(self.error_message) airflow.exceptions.AirflowTaskTimeout: Timeout [2018-07-16 00:16:24,942] {models.py:1624} INFO - Marking task as FAILED. [2018-07-16 00:16:24,956] {models.py:1644} ERROR - Timeout {code} Sometimes the error will also say {code:java} Log file isn't local. Fetching here: http://ip-1-2-3-4:8793/log/foobar/task_1/2018-07-12T00:00:00/1.log Failed to fetch log file from worker. 404 Client Error: NOT FOUND for url: http://ip-1-2-3-4:8793/log/foobar/task_1/2018-07-12T00:00:00/1.log {code} I'm not sure why the logs are working ~95% of the time but are randomly failing at other times. Here are my log settings in my Airflow.cfg file, {code:java} # The folder where airflow should store its log files # This path must be absolute base_log_folder = /home/ec2-user/airflow/logs # Airflow can store logs remotely in AWS S3 or Google Cloud Storage. Users # must supply an Airflow connection id that provides access to the storage # location. remote_log_conn_id = encrypt_s3_logs = False # Logging level logging_level = INFO # Logging class # Specify the class that will specify the logging configuration # This class has to be on the python classpath # logging_config_class = my.path.default_local_settings.LOGGING_CONFIG logging_config_class = # Log format log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s # Name of handler to read task instance logs. # Default to use file task handler. task_log_reader = file.task # Log files for the gunicorn webserver. '-' means log to stderr. access_logfile = - error_logfile = # The amount of time (in secs) webserver will wait for initial handshake # while fetching logs from other worker machine log_fetch_timeout_sec = 5 # When you start an airflow worker, airflow starts a tiny web server # subprocess to serve the workers local log files to the airflow main # web server, who then builds pages and sends them to users. This defines # the port on which the logs are served. It needs to be unused, and open # visible from the main web server to connect into the workers. worker_log_server_port = 8793 # How often should stats be printed to the logs print_stats_interval = 30 child_process_log_directory = /home/ec2-user/airflow/logs/scheduler{code} I'm wondering if maybe I should try a different technique for my logging such as writing to an S3 Bucket or if there is something else I can do to fix this issue. *Update:* Writing the logs to S3 did not resolve this issue. Also, the error is more consistent now (still sporadic). It's happening more like 50% of the time now. One thing I noticed is that the task it's happening on is my AWS EMR creation task. Starting an AWS EMR cluster takes about 20 minutes and then the task has to wait for the Spark commands to run on the EMR cluster. So the single task is running for about 30 minutes. I'm wondering if this is too long for an Airflow task to be running and if that's why it starts to fail writing to the logs. If this is the case then I could breakup the EMR task so that there is one task for the EMR creation, then another task for the Spark commands on the EMR cluster. *Update 2:* Sometimes I see slightly different details in the output of the error such as this {code:java} *** Log file isn't local. *** Fetching here: http://ip-1-2-3-4:1234/log/dag_name/task_name/2018-07-30T00:00:00/9.log [2018-08-01 15:24:52,002] {cli.py:374} INFO - Running on host ip-1-2-3-4 [2018-08-01 15:24:52,383] {models.py:1197} INFO - Dependencies all met for <TaskInstance: dag_name.task_name 2018-07-30 00:00:00 [queued]> [2018-08-01 15:24:52,538] {models.py:1197} INFO - Dependencies all met for <TaskInstance: dag_name.task_name 2018-07-30 00:00:00 [queued]> [2018-08-01 15:24:52,538] {models.py:1407} INFO - -------------------------------------------------------------------------------- Starting attempt 9 of 9 -------------------------------------------------------------------------------- [2018-08-01 15:24:52,618] {models.py:1428} INFO - Executing <Task(OmegaFileSensor): task_name> on 2018-07-30 00:00:00 [2018-08-01 15:24:52,619] {base_task_runner.py:115} INFO - Running: ['bash', '-c', 'airflow run dag_name task_name 2018-07-30T00:00:00 --job_id 410 --raw -sd DAGS_FOLDER/dag_name.py'] [2018-08-01 15:25:00,650] {base_task_runner.py:98} INFO - Subtask: [2018-08-01 15:25:00,649] {configuration.py:206} WARNING - section/key [celery/celery_ssl_active] not found in config [2018-08-01 15:25:00,682] {base_task_runner.py:98} INFO - Subtask: [2018-08-01 15:25:00,682] {default_celery.py:41} WARNING - Celery Executor will run without SSL [2018-08-01 15:25:00,684] {base_task_runner.py:98} INFO - Subtask: [2018-08-01 15:25:00,683] {__init__.py:45} INFO - Using executor CeleryExecutor [2018-08-01 15:25:01,151] {base_task_runner.py:98} INFO - Subtask: [2018-08-01 15:25:01,151] {models.py:189} INFO - Filling up the DagBag from /var/lib/airflow/dags/dag_name.py [2018-08-01 15:25:03,909] {cli.py:374} INFO - Running on host ip-1-2-3-4 [2018-08-02 13:13:47,130] {models.py:1470} ERROR - Killing subprocess [2018-08-02 13:13:47,560] {models.py:1595} ERROR - Task received SIGTERM signal Traceback (most recent call last): File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1488, in _run_raw_task result = task_copy.execute(context=context) File ""/usr/local/lib/python3.6/site-packages/airflow/operators/sensors.py"", line 84, in execute sleep(self.poke_interval) File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1472, in signal_handler raise AirflowException(""Task received SIGTERM signal"") airflow.exceptions.AirflowException: Task received SIGTERM signal [2018-08-02 13:13:47,561] {models.py:1624} INFO - Marking task as FAILED. [2018-08-02 13:13:47,639] {logging_mixin.py:84} WARNING - --- Logging error --- [2018-08-02 13:13:47,639] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:47,639] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1488, in _run_raw_task result = task_copy.execute(context=context) [2018-08-02 13:13:47,639] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/sensors.py"", line 84, in execute sleep(self.poke_interval) [2018-08-02 13:13:47,639] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1472, in signal_handler raise AirflowException(""Task received SIGTERM signal"") [2018-08-02 13:13:47,639] {logging_mixin.py:84} WARNING - airflow.exceptions.AirflowException: Task received SIGTERM signal [2018-08-02 13:13:47,639] {logging_mixin.py:84} WARNING - During handling of the above exception, another exception occurred: [2018-08-02 13:13:47,639] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:47,640] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 996, in emit self.flush() [2018-08-02 13:13:47,640] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 976, in flush self.stream.flush() [2018-08-02 13:13:47,640] {logging_mixin.py:84} WARNING - BrokenPipeError: [Errno 32] Broken pipe [2018-08-02 13:13:47,640] {logging_mixin.py:84} WARNING - Call stack: [2018-08-02 13:13:47,641] {logging_mixin.py:84} WARNING - File ""/usr/bin/airflow"", line 27, in <module> args.func(args) [2018-08-02 13:13:47,641] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/bin/cli.py"", line 392, in run pool=args.pool, [2018-08-02 13:13:47,641] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 50, in wrapper result = func(*args, **kwargs) [2018-08-02 13:13:47,641] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1530, in _run_raw_task self.handle_failure(e, test_mode, context) [2018-08-02 13:13:47,641] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1636, in handle_failure task.on_failure_callback(context) [2018-08-02 13:13:47,641] {logging_mixin.py:84} WARNING - File ""/var/lib/airflow/dags/dag_name.py"", line 36, in on_failure_callback return failure_task_notification.execute(context=context) [2018-08-02 13:13:47,641] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/bash_operator.py"", line 70, in execute self.log.info(""Tmp dir root location: \n %s"", gettempdir()) [2018-08-02 13:13:47,641] {logging_mixin.py:84} WARNING - Message: 'Tmp dir root location: \n %s' Arguments: ('/tmp',) [2018-08-02 13:13:47,641] {logging_mixin.py:84} WARNING - --- Logging error --- [2018-08-02 13:13:47,641] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:47,641] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1488, in _run_raw_task result = task_copy.execute(context=context) [2018-08-02 13:13:47,642] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/sensors.py"", line 84, in execute sleep(self.poke_interval) [2018-08-02 13:13:47,642] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1472, in signal_handler raise AirflowException(""Task received SIGTERM signal"") [2018-08-02 13:13:47,642] {logging_mixin.py:84} WARNING - airflow.exceptions.AirflowException: Task received SIGTERM signal [2018-08-02 13:13:47,642] {logging_mixin.py:84} WARNING - During handling of the above exception, another exception occurred: [2018-08-02 13:13:47,642] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:47,719] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 996, in emit self.flush() [2018-08-02 13:13:47,720] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 976, in flush self.stream.flush() [2018-08-02 13:13:47,720] {logging_mixin.py:84} WARNING - BrokenPipeError: [Errno 32] Broken pipe [2018-08-02 13:13:47,720] {logging_mixin.py:84} WARNING - Call stack: [2018-08-02 13:13:47,720] {logging_mixin.py:84} WARNING - File ""/usr/bin/airflow"", line 27, in <module> args.func(args) [2018-08-02 13:13:47,720] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/bin/cli.py"", line 392, in run pool=args.pool, [2018-08-02 13:13:47,720] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 50, in wrapper result = func(*args, **kwargs) [2018-08-02 13:13:47,720] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1530, in _run_raw_task self.handle_failure(e, test_mode, context) [2018-08-02 13:13:47,720] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1636, in handle_failure task.on_failure_callback(context) [2018-08-02 13:13:47,720] {logging_mixin.py:84} WARNING - File ""/var/lib/airflow/dags/dag_name.py"", line 36, in on_failure_callback return failure_task_notification.execute(context=context) [2018-08-02 13:13:47,720] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/bash_operator.py"", line 80, in execute script_location [2018-08-02 13:13:47,720] {logging_mixin.py:84} WARNING - Message: 'Temporary script location: %s' Arguments: ('/tmp/airflowtmp81ztdhnm//tmp/airflowtmp81ztdhnm/failure_task_notificationlzixk4ar',) [2018-08-02 13:13:47,720] {logging_mixin.py:84} WARNING - --- Logging error --- [2018-08-02 13:13:47,720] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1488, in _run_raw_task result = task_copy.execute(context=context) [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/sensors.py"", line 84, in execute sleep(self.poke_interval) [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1472, in signal_handler raise AirflowException(""Task received SIGTERM signal"") [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - airflow.exceptions.AirflowException: Task received SIGTERM signal [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - During handling of the above exception, another exception occurred: [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 996, in emit self.flush() [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 976, in flush self.stream.flush() [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - BrokenPipeError: [Errno 32] Broken pipe [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - Call stack: [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - File ""/usr/bin/airflow"", line 27, in <module> args.func(args) [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/bin/cli.py"", line 392, in run pool=args.pool, [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 50, in wrapper result = func(*args, **kwargs) [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1530, in _run_raw_task self.handle_failure(e, test_mode, context) [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1636, in handle_failure task.on_failure_callback(context) [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - File ""/var/lib/airflow/dags/dag_name.py"", line 36, in on_failure_callback return failure_task_notification.execute(context=context) [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/bash_operator.py"", line 88, in execute self.log.info(""Running command: %s"", bash_command) [2018-08-02 13:13:47,721] {logging_mixin.py:84} WARNING - Message: 'Running command: %s' Arguments: ('aws sns publish --topic-arn arn:aws:sns:us-east-1:464816863426:datalake_digital_platform_arl_airflow_workflow --message ""The ARL Airflow Workflow <TaskInstance: dag_name.task_name 2018-07-30 00:00:00 [failed]> task has failed."" --region us-east-1',) [2018-08-02 13:13:47,835] {logging_mixin.py:84} WARNING - --- Logging error --- [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1488, in _run_raw_task result = task_copy.execute(context=context) [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/sensors.py"", line 84, in execute sleep(self.poke_interval) [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1472, in signal_handler raise AirflowException(""Task received SIGTERM signal"") [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - airflow.exceptions.AirflowException: Task received SIGTERM signal [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - During handling of the above exception, another exception occurred: [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 996, in emit self.flush() [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 976, in flush self.stream.flush() [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - BrokenPipeError: [Errno 32] Broken pipe [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - Call stack: [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - File ""/usr/bin/airflow"", line 27, in <module> args.func(args) [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/bin/cli.py"", line 392, in run pool=args.pool, [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 50, in wrapper result = func(*args, **kwargs) [2018-08-02 13:13:47,836] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1530, in _run_raw_task self.handle_failure(e, test_mode, context) [2018-08-02 13:13:47,837] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1636, in handle_failure task.on_failure_callback(context) [2018-08-02 13:13:47,837] {logging_mixin.py:84} WARNING - File ""/var/lib/airflow/dags/dag_name.py"", line 36, in on_failure_callback return failure_task_notification.execute(context=context) [2018-08-02 13:13:47,837] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/bash_operator.py"", line 97, in execute self.log.info(""Output:"") [2018-08-02 13:13:47,837] {logging_mixin.py:84} WARNING - Message: 'Output:' Arguments: () [2018-08-02 13:13:57,731] {logging_mixin.py:84} WARNING - --- Logging error --- [2018-08-02 13:13:57,731] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:57,731] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1488, in _run_raw_task result = task_copy.execute(context=context) [2018-08-02 13:13:57,731] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/sensors.py"", line 84, in execute sleep(self.poke_interval) [2018-08-02 13:13:57,731] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1472, in signal_handler raise AirflowException(""Task received SIGTERM signal"") [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - airflow.exceptions.AirflowException: Task received SIGTERM signal [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - During handling of the above exception, another exception occurred: [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 996, in emit self.flush() [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 976, in flush self.stream.flush() [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - BrokenPipeError: [Errno 32] Broken pipe [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - Call stack: [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - File ""/usr/bin/airflow"", line 27, in <module> args.func(args) [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/bin/cli.py"", line 392, in run pool=args.pool, [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 50, in wrapper result = func(*args, **kwargs) [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1530, in _run_raw_task self.handle_failure(e, test_mode, context) [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1636, in handle_failure task.on_failure_callback(context) [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - File ""/var/lib/airflow/dags/dag_name.py"", line 36, in on_failure_callback return failure_task_notification.execute(context=context) [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/bash_operator.py"", line 101, in execute self.log.info(line) [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - Message: '{' Arguments: () [2018-08-02 13:13:57,732] {logging_mixin.py:84} WARNING - --- Logging error --- [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1488, in _run_raw_task result = task_copy.execute(context=context) [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/sensors.py"", line 84, in execute sleep(self.poke_interval) [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1472, in signal_handler raise AirflowException(""Task received SIGTERM signal"") [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - airflow.exceptions.AirflowException: Task received SIGTERM signal [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - During handling of the above exception, another exception occurred: [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 996, in emit self.flush() [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 976, in flush self.stream.flush() [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - BrokenPipeError: [Errno 32] Broken pipe [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - Call stack: [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - File ""/usr/bin/airflow"", line 27, in <module> args.func(args) [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/bin/cli.py"", line 392, in run pool=args.pool, [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 50, in wrapper result = func(*args, **kwargs) [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1530, in _run_raw_task self.handle_failure(e, test_mode, context) [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1636, in handle_failure task.on_failure_callback(context) [2018-08-02 13:13:57,733] {logging_mixin.py:84} WARNING - File ""/var/lib/airflow/dags/dag_name.py"", line 36, in on_failure_callback return failure_task_notification.execute(context=context) [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/bash_operator.py"", line 101, in execute self.log.info(line) [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - Message: '""MessageId"": ""7c284092-dc0c-539b-97b9-134fe219aacd""' Arguments: () [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - --- Logging error --- [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1488, in _run_raw_task result = task_copy.execute(context=context) [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/sensors.py"", line 84, in execute sleep(self.poke_interval) [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1472, in signal_handler raise AirflowException(""Task received SIGTERM signal"") [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - airflow.exceptions.AirflowException: Task received SIGTERM signal [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - During handling of the above exception, another exception occurred: [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - Traceback (most recent call last): [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 996, in emit self.flush() [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - File ""/usr/lib64/python3.6/logging/__init__.py"", line 976, in flush self.stream.flush() [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - BrokenPipeError: [Errno 32] Broken pipe [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - Call stack: [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - File ""/usr/bin/airflow"", line 27, in <module> args.func(args) [2018-08-02 13:13:57,734] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/bin/cli.py"", line 392, in run pool=args.pool, [2018-08-02 13:13:57,735] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 50, in wrapper result = func(*args, **kwargs) [2018-08-02 13:13:57,735] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1530, in _run_raw_task self.handle_failure(e, test_mode, context) [2018-08-02 13:13:57,735] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1636, in handle_failure task.on_failure_callback(context) [2018-08-02 13:13:57,735] {logging_mixin.py:84} WARNING - File ""/var/lib/airflow/dags/dag_name.py"", line 36, in on_failure_callback return failure_task_notification.execute(context=context) [2018-08-02 13:13:57,735] {logging_mixin.py:84} WARNING - File ""/usr/local/lib/python3.6/site-packages/airflow/operators/bash_operator.py"", line 101, in execute self.log.info(line) [2018-08-02 13:13:57,735] {logging_mixin.py:84} WARNING - Message: '}' Arguments: () [2018-08-02 13:13:58,639] {models.py:1470} ERROR - Killing subprocess [2018-08-02 13:13:58,640] {models.py:1638} ERROR - Failed at executing callback [2018-08-02 13:13:58,640] {models.py:1639} ERROR - Task received SIGTERM signal Traceback (most recent call last): File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1488, in _run_raw_task result = task_copy.execute(context=context) File ""/usr/local/lib/python3.6/site-packages/airflow/operators/sensors.py"", line 84, in execute sleep(self.poke_interval) File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1472, in signal_handler raise AirflowException(""Task received SIGTERM signal"") airflow.exceptions.AirflowException: Task received SIGTERM signal During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1636, in handle_failure task.on_failure_callback(context) File ""/var/lib/airflow/dags/dag_name.py"", line 36, in on_failure_callback return failure_task_notification.execute(context=context) File ""/usr/local/lib/python3.6/site-packages/airflow/operators/bash_operator.py"", line 99, in execute for line in iter(sp.stdout.readline, b''): File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 1472, in signal_handler raise AirflowException(""Task received SIGTERM signal"") airflow.exceptions.AirflowException: Task received SIGTERM signal [2018-08-02 13:13:59,126] {models.py:1644} ERROR - Task received SIGTERM signal{code} *Note:* I've also created a Stackoverflow question for this issue here [https://stackoverflow.com/questions/51365911/airflow-logs-brokenpipeexception]"
257,AIRFLOW-2848,AIRFLOW,1533361291000,1558123382000,Bug,Resolved,Critical,2,218,"dag_id is missing in metadata table ""job"" for LocalTaskJob dag_id is missing for all entries in metadata table ""job"" with job_type ""LocalTaskJob"". This is due to that dag_id was not specified within class LocalTaskJob."
258,AIRFLOW-2855,AIRFLOW,1533479244000,1536592054000,Improvement,Resolved,Critical,2,783,"Need to Check Validity of Cron Expression When Process DAG File/Zip File *schedule_interval* of DAGs can either be *timedelta* or a *Cron expression*. When it's a Cron expression, there is no mechanism to check its validity at this moment. If there is anything wrong with the Cron expression itself, it will cause issues when methods _*following_schedule(**)*_ and _*previous_schedule()*_ are invoked (will affect scheduling). However, exceptions will only be written into logs. From Web UI, its hard for users to identify this issue & the source while no new task can be initiated (especially for users whore not very familiar with Cron). It may be good to show error messages in web UI when a DAG's Cron expression (as schedule_interval) can not be parsed by *croniter* properly."
259,AIRFLOW-2872,AIRFLOW,1533714893000,1534310198000,Improvement,Closed,Critical,2,810,"Implement ""Ad Hoc Query"" in /www_rbac, and refine existing QueryView() To implement ""Ad Hoc Query"" in for RBAC in /www_rbac, based on the existing implementation in /www. In addition, refine the existing QueryView(): # The "".csv"" button in *Ad Hoc Query* view is responding with a plain text file, rather than a CSV file (even though users can manually change the extension). # Argument 'has_data' passed to the template is not used by the template 'airflow/query.html'. # Sometimes get errors 'UnboundLocalError: local variable 'df' referenced before assignment' # 'result = df.to_html()' should only be invoked when user doesn NOT choose '.csv'. Otherwise it's a waste of resource to invoke 'df.to_html()' since the result it returns will not be used if user askes for CSV downloading instead of a html page."
260,AIRFLOW-2878,AIRFLOW,1533817111000,1576628156000,Improvement,Resolved,Critical,2,186,"""/www_rbac"" Layout/UI Display Issue The new RBAC UI has some issues about layout/UI display. The header (<h2>) is not shown (""hidden"" by the Nav Bar), or tables are not shown completely."
261,AIRFLOW-2884,AIRFLOW,1533865191000,1607040015000,Bug,Resolved,Critical,2,348,"Fix Flask SECRET_KEY security issue in www_rbac Flask secret key should be as random as possible, while it's not in Airflow Flask App. This issue was fixed for *www* in ticket https://issues.apache.org/jira/browse/AIRFLOW-2809 (merged in PR [https://github.com/apache/incubator-airflow/pull/3651)] . But this issue was not fixed for *www_rbac* yet."
262,AIRFLOW-2885,AIRFLOW,1533889615000,1537575214000,Bug,Closed,Critical,2,278,"A Bug in www_rbac.utils.get_params *get_params(page=0, search=""abc"",showPaused=False)* returns ""_search=abc&showPaused=False_"", while it's supposed to return ""page=0&search=abc&showPaused=False"". This is because Python takes 0 as False when it's used in a conditional statement."
263,AIRFLOW-2886,AIRFLOW,1533920576000,1607040018000,Bug,Resolved,Critical,2,570,"Secure Flask SECRET_KEY In my earlier PRs, [https://github.com/apache/incubator-airflow/pull/3651] and [https://github.com/apache/incubator-airflow/pull/3729] , I proposed to generate random SECRET_KEY for Flask App. If we have multiple workers for the Flask webserver, we may encounter CSRF error {{The CSRF session token is missing}} . On the other hand, it's still very important to have as random SECRET_KEY as possible for security reasons. We can deal with it like how we dealt with FERNET_KEY (i.e. generate a random value when the airflow.cfg file is initiated)."
264,AIRFLOW-2913,AIRFLOW,1534516080000,1534531633000,Improvement,Resolved,Critical,2,582,"Check bucket_key and bucket_name combination in S3KeySensor() 1. When *bucket_name* is provided, and *bucket_key* is also provided as a full S3:// url, the full_url obtained eventually will be wrong. It will be like ""_s3://bucket_name/s3://bucket_name/object_key_"". This should be avoided by adding checking and raise exception in such case. 2. Given the documentation is not clear enough, this may happen to new users of S3KeySensor() (like myself). I have improved the Documentation as well. 3. Unit tests are added for the bucket_key and bucket_name combination checking as well."
265,AIRFLOW-2935,AIRFLOW,1534934640000,1585163835000,Bug,Closed,Critical,2,713,"CLONE - Tasks never run in externally triggered DAGRun I have a DAG which should only be run as needed (no schedule interval): dag = DAG( dag_id='major_integ_cycle', default_args=args, schedule_interval=None, ) I attempted to start this using `airflow trigger_dag NAME` but the tasks never run. The scheduler has noted the fact that itthe DAG was triggered: INFO - Created <DagRun major_integ_cycle @ 2016-05-03 08:36:16.032474: manual__2016-05-03T08:36:16.032474, externally triggered: True> The tasks are currently very vanilla test cases, all look like this: task_mdc = op.BashOperator( task_id='master_data_cycle', bash_command='sleep 1', dag=dag, ) Have I misunderstood? Or is not running a bug in this case?"
266,AIRFLOW-2943,AIRFLOW,1535019320000,1555590173000,Bug,Open,Critical,2,819,"example_trigger_target_dag.py Dag is stuck in running state Hi, I am trying to implement example_trigger_controller_dag.py and example_trigger_target_dag.py (according to my requirements) but the execution time of target dag is approximately 7 hours more than the triggered time i.e Triggered time is in PDT but the execution time of dag is showing in UTC. These sample examples are giving same issue. Because of this Dag run after 7 hours of the triggered time. In Issue - Apache AirflowAIRFLOW-43 , it is mentioned - If you see the dag run created, but not being scheduled, it is likely because you are not running in UTC everywhere. Airflow dags are running in PDT timezone , so to convert that into UTC, i updated cfg file with default_timezone = utc and restarted airflow but still getting same issue. Please help."
267,AIRFLOW-2946,AIRFLOW,1535032960000,1606137112000,Bug,Open,Critical,2,4888,"Connection times out on airflow worker Hi I have Airflow cluster setup running Celery executors with Postgresql installed on same machine as webserver and scheduler. After sometime, remote worker shows error 'Connection timed out' and Airflow queues number of configured tasks in pool in queue and flow hungs up there until queue tasks are deleted manually after stopping the scheduler service. Logs: [2018-08-23 13:44:03,954: ERROR/MainProcess] Pool callback raised exception: OperationalError('(psycopg2.OperationalError) could not connect to server: Connection timed out\n\tIs the server running on host ""<host>"" and accepting\n\tTCP/IP connections on port 5432?\n',) Traceback (most recent call last): File ""/home/ubuntu/.local/lib/python2.7/site-packages/billiard/pool.py"", line 1747, in safe_apply_callback fun(*args, **kwargs) File ""/home/ubuntu/.local/lib/python2.7/site-packages/celery/worker/request.py"", line 367, in on_failure self.id, exc, request=self, store_result=self.store_errors, File ""/home/ubuntu/.local/lib/python2.7/site-packages/celery/backends/base.py"", line 157, in mark_as_failure traceback=traceback, request=request) File ""/home/ubuntu/.local/lib/python2.7/site-packages/celery/backends/base.py"", line 322, in store_result request=request, **kwargs) File ""/home/ubuntu/.local/lib/python2.7/site-packages/celery/backends/database/__init__.py"", line 53, in _inner return fun(*args, **kwargs) File ""/home/ubuntu/.local/lib/python2.7/site-packages/celery/backends/database/__init__.py"", line 105, in _store_result session = self.ResultSession() File ""/home/ubuntu/.local/lib/python2.7/site-packages/celery/backends/database/__init__.py"", line 99, in ResultSession **self.engine_options) File ""/home/ubuntu/.local/lib/python2.7/site-packages/celery/backends/database/session.py"", line 60, in session_factory self.prepare_models(engine) File ""/home/ubuntu/.local/lib/python2.7/site-packages/celery/backends/database/session.py"", line 55, in prepare_models ResultModelBase.metadata.create_all(engine) File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/sql/schema.py"", line 4005, in create_all tables=tables) File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1939, in _run_visitor with self._optional_conn_ctx_manager(connection) as conn: File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__ return self.gen.next() File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1932, in _optional_conn_ctx_manager with self.contextual_connect() as conn: File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2123, in contextual_connect self._wrap_pool_connect(self.pool.connect, None), File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2162, in _wrap_pool_connect e, dialect, self) File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1476, in _handle_dbapi_exception_noconnection exc_info File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/util/compat.py"", line 265, in raise_from_cause reraise(type(exception), exception, tb=exc_tb, cause=cause) File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2158, in _wrap_pool_connect return fn() File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 403, in connect return _ConnectionFairy._checkout(self) File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 791, in _checkout fairy = _ConnectionRecord.checkout(pool) File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 532, in checkout rec = pool._do_get() File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 1287, in _do_get return self._create_connection() File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 350, in _create_connection return _ConnectionRecord(self) File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 477, in __init__ self.__connect(first_connect_check=True) File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 674, in __connect connection = pool._invoke_creator(self) File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py"", line 106, in connect return dialect.connect(*cargs, **cparams) File ""/home/ubuntu/.local/lib/python2.7/site-packages/sqlalchemy/engine/default.py"", line 412, in connect return self.dbapi.connect(*cargs, **cparams) File ""/home/ubuntu/.local/lib/python2.7/site-packages/psycopg2/__init__.py"", line 130, in connect conn = _connect(dsn, connection_factory=connection_factory, **kwasync) OperationalError: (psycopg2.OperationalError) could not connect to server: Connection timed out Is the server running on host ""<host>"" and accepting TCP/IP connections on port 5432?"
268,AIRFLOW-2979,AIRFLOW,1535571332000,1537955365000,Bug,Resolved,Critical,2,965,"Deprecated Celery Option not in Options list References AIRFLOW-1840 In airflow/configuration.py {code:java} # A two-level mapping of (section -> new_name -> old_name). When reading # new_name, the old_name will be checked to see if it exists. If it does a # DeprecationWarning will be issued and the old name will be used instead deprecated_options = { 'celery': { # Remove these keys in Airflow 1.11 'worker_concurrency': 'celeryd_concurrency', 'broker_url': 'celery_broker_url', 'ssl_active': 'celery_ssl_active', 'ssl_cert': 'celery_ssl_cert', 'ssl_key': 'celery_ssl_key', } } {code} This block is missing the renaming of celery_result_backend to just result_backed. When setting this through an environment variable, the deprecated config name is not being used and instead the default value in the file is being used. This is obviously remedied by the reading the UPDATING and setting the new name, but this change has broken back compat as far as I can tell."
269,AIRFLOW-3024,AIRFLOW,1536277365000,1536277365000,Bug,Open,Critical,2,6205,"Airflow 10 Gunicorn exception with ssl I am runninug Gunicorn 19.9.0 and when accessing the webpage I receive an exception. This causes the browser the gunicorn to freeze for a few minutes until it is able to start serving again. These are my pip packages and I am running python 2.7 adal==1.0.2 alabaster==0.7.11 alembic==0.8.10 amqp==2.3.2 analytics-python==1.2.9 apache-airflow==1.10.0 asn1crypto==0.24.0 atlasclient==0.1.2 awscli==1.16.1 azure-common==1.1.14 azure-datalake-store==0.0.19 azure-mgmt-datalake-nspkg==2.0.0 azure-mgmt-datalake-store==0.4.0 azure-mgmt-nspkg==2.0.0 azure-mgmt-resource==1.2.2 azure-nspkg==2.0.0 azure-storage==0.36.0 Babel==2.6.0 backports-abc==0.5 backports.ssl-match-hostname==3.5.0.1 bcrypt==3.1.4 billiard==3.5.0.4 bleach==2.1.2 boto==2.49.0 boto3==1.8.5 botocore==1.11.1 cachetools==2.1.0 cassandra-driver==3.15.0 celery==4.2.1 certifi==2018.8.24 cffi==1.11.5 chardet==3.0.4 click==6.7 cloudant==0.5.10 colorama==0.3.9 configparser==3.5.0 cookies==2.2.1 croniter==0.3.25 cryptography==2.3.1 cx-Oracle==6.4.1 datadog==0.22.0 decorator==4.3.0 defusedxml==0.5.0 dicttoxml==1.7.4 dill==0.2.8.2 docker==3.5.0 docker-pycreds==0.3.0 docopt==0.6.2 docutils==0.14 elasticsearch==5.5.3 elasticsearch-dsl==5.4.0 entrypoints==0.2.3 enum34==1.1.6 fastavro==0.9.6 Flask==0.12.4 Flask-Admin==1.4.1 Flask-AppBuilder==1.11.1 Flask-Babel==0.11.1 Flask-Bcrypt==0.7.1 Flask-Caching==1.3.3 Flask-Login==0.2.11 Flask-OpenID==1.2.5 Flask-SQLAlchemy==2.1 flask-swagger==0.2.13 Flask-WTF==0.14.2 flower==0.9.2 freezegun==0.3.10 funcsigs==1.0.0 future==0.16.0 future-fstrings==0.4.4 futures==3.2.0 gitdb2==2.0.4 GitPython==2.1.11 google-api-core==1.3.0 google-api-python-client==1.7.4 google-auth==1.5.1 google-auth-httplib2==0.0.3 google-auth-oauthlib==0.2.0 google-cloud-bigquery==1.5.0 google-cloud-container==0.1.1 google-cloud-core==0.28.1 google-resumable-media==0.3.1 googleapis-common-protos==1.5.3 grpcio==1.14.2 gunicorn==19.9.0 hdfs==2.1.0 hmsclient==0.1.1 html5lib==1.0.1 httplib2==0.11.3 idna==2.7 ijson==2.3 imagesize==1.1.0 inflection==0.3.1 ipaddress==1.0.22 iso8601==0.1.12 isodate==0.6.0 itsdangerous==0.24 JayDeBeApi==1.1.1 Jinja2==2.8.1 jira==2.0.0 jmespath==0.9.3 JPype1==0.6.3 keyring==13.2.1 kombu==4.2.1 kubernetes==7.0.0 ldap3==2.5.1 lockfile==0.12.2 lxml==3.8.0 Mako==1.0.7 Markdown==2.6.11 MarkupSafe==1.0 mock==2.0.0 mongomock==3.11.1 monotonic==1.5 moto==1.1.19 msrest==0.4.29 msrestazure==0.4.34 multi-key-dict==2.0.3 mysqlclient==1.3.13 ndg-httpsclient==0.5.1 nose==1.3.7 nose-ignore-docstring==0.2 nose-timer==0.7.3 ntlm-auth==1.2.0 numpy==1.15.1 oauthlib==2.1.0 ordereddict==1.1 packaging==17.1 pandas==0.23.4 pandas-gbq==0.6.1 parameterized==0.6.1 paramiko==2.4.1 pathlib2==2.3.2 pbr==4.2.0 pendulum==1.4.4 pinotdb==0.1.1 protobuf==3.6.1 psutil==4.4.2 psycopg2==2.7.5 psycopg2-binary==2.7.5 pyaml==17.12.1 pyasn1==0.4.4 pyasn1-modules==0.2.2 pycparser==2.18 pycryptodomex==3.6.6 pydruid==0.4.5 Pygments==2.2.0 PyHive==0.6.0 PyJWT==1.6.4 pykerberos==1.2.1 pymongo==3.7.1 pymssql==2.1.4 PyNaCl==1.2.1 pyOpenSSL==18.0.0 pyparsing==2.2.0 pysftp==0.2.9 PySmbClient==0.1.5 python-daemon==2.1.2 python-dateutil==2.7.3 python-editor==1.0.3 python-http-client==3.1.0 python-jenkins==1.2.1 python-nvd3==0.15.0 python-openid==2.2.5 python-slugify==1.2.5 pytz==2015.7 pytzdata==2018.5 pywinrm==0.3.0 PyYAML==3.13 qds-sdk==1.9.9 redis==2.10.6 rednose==1.3.0 requests==2.19.1 requests-futures==0.9.4 requests-kerberos==0.12.0 requests-mock==1.5.2 requests-ntlm==1.1.0 requests-oauthlib==1.0.0 requests-toolbelt==0.8.0 rsa==3.4.2 ruamel.ordereddict==0.4.13 ruamel.yaml==0.15.64 s3transfer==0.1.13 sasl==0.2.1 scandir==1.9.0 SecretStorage==2.3.1 sendgrid==5.6.0 sentinels==1.0.0 setproctitle==1.1.10 simplejson==3.16.0 singledispatch==3.4.0.3 six==1.11.0 slackclient==1.2.1 smmap2==2.0.4 snakebite==2.11.0 snowballstemmer==1.2.1 snowflake-connector-python==1.6.8 snowflake-sqlalchemy==1.1.2 Sphinx==1.7.8 sphinx-argparse==0.2.2 Sphinx-PyPI-upload==0.2.1 sphinx-rtd-theme==0.4.1 sphinxcontrib-websupport==1.1.0 SQLAlchemy==1.1.18 tabulate==0.7.7 tenacity==4.8.0 termstyle==0.1.11 text-unidecode==1.2 thrift==0.11.0 thrift-sasl==0.3.0 tokenize-rt==2.0.1 tornado==5.1 typing==3.6.6 tzlocal==1.5.1 unicodecsv==0.14.1 uritemplate==3.0.0 urllib3==1.23 vertica-python==0.7.4 vine==1.1.4 webencodings==0.5.1 websocket-client==0.51.0 Werkzeug==0.14.1 WTForms==2.2.1 xmltodict==0.11.0 zdesk==2.7.1 zope.deprecation==4.3.0 This is the stacktrace. I see references to ssl and gunicorn. [2018-09-06 22:14:35 +0000] [54] [CRITICAL] WORKER TIMEOUT (pid:498) [2018-09-06 22:14:35 +0000] [498] [INFO] Worker exiting (pid: 498) [2018-09-06 22:14:35 +0000] [823] [INFO] Booting worker with pid: 823 [2018-09-06 22:14:35,770] \{__init__.py:51} INFO - Using executor CeleryExecutor [2018-09-06 22:14:36,079] \{models.py:258} INFO - Filling up the DagBag from /usr/local/airflow/dags [2018-09-06 22:14:36 +0000] [823] [ERROR] Socket error processing request. Traceback (most recent call last): File ""/usr/local/lib/python2.7/dist-packages/gunicorn/workers/sync.py"", line 134, in handle req = six.next(parser) File ""/usr/local/lib/python2.7/dist-packages/gunicorn/http/parser.py"", line 41, in __next__ self.mesg = self.mesg_class(self.cfg, self.unreader, self.req_count) File ""/usr/local/lib/python2.7/dist-packages/gunicorn/http/message.py"", line 181, in __init__ super(Request, self).__init__(cfg, unreader) File ""/usr/local/lib/python2.7/dist-packages/gunicorn/http/message.py"", line 54, in __init__ unused = self.parse(self.unreader) File ""/usr/local/lib/python2.7/dist-packages/gunicorn/http/message.py"", line 193, in parse self.get_data(unreader, buf, stop=True) File ""/usr/local/lib/python2.7/dist-packages/gunicorn/http/message.py"", line 184, in get_data data = unreader.read() File ""/usr/local/lib/python2.7/dist-packages/gunicorn/http/unreader.py"", line 38, in read d = self.chunk() File ""/usr/local/lib/python2.7/dist-packages/gunicorn/http/unreader.py"", line 65, in chunk return self.sock.recv(self.mxchunk) File ""/usr/lib/python2.7/ssl.py"", line 766, in recv return self.read(buflen) File ""/usr/lib/python2.7/ssl.py"", line 653, in read v = self._sslobj.read(len) error: [Errno 0] Error"
270,AIRFLOW-3039,AIRFLOW,1536681174000,1550149342000,Bug,Open,Critical,2,694,"TriggerDagRunOperator raise error when triggered concurrently The error is caused by multiple independent TriggerDagRunOperator tasks pointing to the same dag. When trigger concurrently, a field in the transaction updating data to the database have collision. this cause TirggerDagRunOperator to fail. but one who makes it the first transaction will be success. I was able to get around with the issue by adding: {code:java} import random import time time.sleep(random.randint(1, 60)) {code} before {code:java} airflow.operators.dagrun_operator.TriggerDagRunOperator.execute {code} as the first line. The issue might be an executing_time related field has collision when update to the database."
271,AIRFLOW-3065,AIRFLOW,1536939540000,1537190080000,Bug,Closed,Critical,2,2957,"Scheduler failing tasks when DAG concurrency limit reached In a DAG with concurrency limit of 4, with about 150 task inside, when the limit of active tasks is reached, the scheduler starts to fail queued tasks. They later are retried, but if they have downstream tasks, these remain in upstream_failed status. A few additional details: * celery executor * environment upgraded from 1.9 (no issues back then) * all configuration in airflow.cfg updated to the latest set of options * issue happens both with PyPi 1.10 and a build from branch v1-10-test (c36ef06) {noformat} [2018-09-14 13:51:23,560] {models.py:1336} INFO - Dependencies all met for <TaskInstance: consolidated_db.item 2018-09-14T12:42:55.379761+00:00 [queued]> [2018-09-14 13:51:23,850] {models.py:1330} INFO - Dependencies not met for <TaskInstance: consolidated_db.item 2018-09-14T12:42:55.379761+00:00 [queued]>, dependency 'Task Instance Slots Available' FAILED: The maximum number of running tasks (4) for this task's DAG 'consolidated_db' has been reached. [2018-09-14 13:51:23,852] {models.py:1531} WARNING - -------------------------------------------------------------------------------- FIXME: Rescheduling due to concurrency limits reached at task runtime. Attempt 1 of 1. State set to NONE. -------------------------------------------------------------------------------- [2018-09-14 13:51:23,853] {models.py:1534} INFO - Queuing into pool None [2018-09-14 13:51:23,560] {models.py:1336} INFO - Dependencies all met for <TaskInstance: consolidated_db.item 2018-09-14T12:42:55.379761+00:00 [queued]> [2018-09-14 13:51:23,850] {models.py:1330} INFO - Dependencies not met for <TaskInstance: consolidated_db.item 2018-09-14T12:42:55.379761+00:00 [queued]>, dependency 'Task Instance Slots Available' FAILED: The maximum number of running tasks (4) for this task's DAG 'consolidated_db' has been reached. [2018-09-14 13:51:23,852] {models.py:1531} WARNING - -------------------------------------------------------------------------------- FIXME: Rescheduling due to concurrency limits reached at task runtime. Attempt 1 of 1. State set to NONE. -------------------------------------------------------------------------------- [2018-09-14 13:51:23,853] {models.py:1534} INFO - Queuing into pool None [2018-09-14 13:52:49,939] {models.py:1336} INFO - Dependencies all met for <TaskInstance: consolidated_db.item 2018-09-14T12:42:55.379761+00:00 [queued]> [2018-09-14 13:52:50,142] {models.py:1336} INFO - Dependencies all met for <TaskInstance: consolidated_db.item 2018-09-14T12:42:55.379761+00:00 [queued]> [2018-09-14 13:52:50,235] {models.py:1548} INFO - -------------------------------------------------------------------------------- Starting attempt 1 of 1 -------------------------------------------------------------------------------- [2018-09-14 13:52:50,646] {models.py:1570} INFO - Executing <Task(PostgresDumpOperator): item> on 2018-09-14T12:42:55.379761+00:00 {noformat}"
272,AIRFLOW-3073,AIRFLOW,1537149168000,1555353755000,Improvement,Resolved,Critical,2,374,"A note is needed in 'Data Profiling' doc page to reminder users it's no longer supported in new webserver UI In [https://airflow.incubator.apache.org/profiling.html,] it's not mentioned at all that these features are no longer supported in new webser (FAB-based) due to security concern (https://github.com/apache/incubator-airflow/blob/master/UPDATING.md#breaking-changes)."
273,AIRFLOW-3075,AIRFLOW,1537175968000,1537211843000,Bug,Closed,Critical,2,1033,"bql deprecated field isn't deprecated - BigQueryOperator docs says: [http://airflow.incubator.apache.org/integration.html?highlight=bigquery#bigquerytobigqueryoperator] * *bql* (_Can receive a str representing a sql statement__,_ _a list of str_ _(__sql statements__)__, or__reference to a template file. Template reference are recognized by str ending in '.sql'._)  (Deprecated. Use ??sql?? parameter instead) the sql code to be executed (templated) * *sql* (_Can receive a str representing a sql statement__,_ _a list of str_ _(__sql statements__)__, or__reference to a template file. Template reference are recognized by str ending in '.sql'._)  the sql code to be executed (templated) When created a operator with *sql* and adding the new DAG the UI shows: Broken DAG: [/home/ubuntu/airflow/dags/my_dag.py] Argument ['bql'] is required The DAG can not run. only when changing the sql to bql the error is gone and the DAG can run. I'm using 1.9.0 This is weird because I must use the deprecated bql and not the new sql argument."
274,AIRFLOW-3097,AIRFLOW,1537463119000,1584578744000,New Feature,Open,Critical,2,2481,"Capability for nested SubDags Unless I'm doing something incorrectly, it appears that you cannot nest SubDags which would be a very helpful feature. I've created a simple pipeline to demonstrate the failure case below. It produces the following in Airflow: Broken DAG: [/home/airflow/airflow/dags/test_dag.py] 'NoneType' object has no attribute 'dag_id' test_dag.py {code:java} from airflow import DAG from airflow.operators.subdag_operator import SubDagOperator import datetime from datetime import timedelta from test_subdag1 import TestSubDag1 startDate = '2018-09-20' default_args = { 'owner': 'airflow', 'depends_on_past': False, 'email': ['email@airflow.com'], 'start_date': datetime.datetime(2018, 3, 20, 9, 0), 'email_on_failure': False, 'email_on_retry': False, 'retries': 5, 'retry_delay': timedelta(seconds=30), 'run_as_user': 'airflow' } Test_DAG = DAG('Test_DAG', default_args=default_args, start_date=datetime.datetime(2018, 3, 20, 9, 0), schedule_interval=None, catchup=False) test_subdag1 = SubDagOperator(subdag=TestSubDag1('Test_DAG', 'test_subdag1', startDate), task_id='test_subdag1', dag=Test_DAG) TestDagConsolidateTask = DummyOperator(task_id='TestDag_Consolidate', dag=Test_DAG) test_subdag1 >> TestDagConsolidateTask {code} test_subdag1.py {code:java} from airflow import DAG from airflow.operators.subdag_operator import SubDagOperator from airflow.operators.dummy_operator import DummyOperator from test_subdag2 import TestSubDag2 import datetime from datetime import timedelta def TestSubDag1(parent_dag_name, child_dag_name, startDate): subdag = DAG( '%s.%s' % (parent_dag_name, child_dag_name), schedule_interval=None, start_date=startDate) test_subdag2 = SubDagOperator(subdag=TestSubDag2('%s.%s' % (parent_dag_name, child_dag_name), 'test_subdag2', startDate), task_id='test_subdag2', dag=subdag) Subdag1ConsolidateTask = DummyOperator(task_id='Subdag1_Consolidate', dag=subdag) test_subdag2 >> Subdag1ConsolidateTask {code} test_subdag2.py {code:java} // code placeholder from airflow import DAG from airflow.operators.dummy_operator import DummyOperator import datetime from datetime import timedelta def TestSubDag2(parent_dag_name, child_dag_name, startDate): subdag = DAG( '%s.%s' % (parent_dag_name, child_dag_name), schedule_interval=None, start_date=startDate) TestTask = DummyOperator(task_id='TestTask', dag=subdag) Subdag2ConsolidateTask = DummyOperator(task_id='Subdag2_Consolidate', dag=subdag) TestTask >> Subdag2ConsolidateTask {code}"
275,AIRFLOW-3181,AIRFLOW,1539164002000,1539204809000,Bug,Open,Critical,2,1875,"RBAC auth error I have a fresh install of airflow 1.10 on an AWS EC2 instance. It seems that the web server and the scheduler services start up fine, however, when trying to load the frontend through my browser I'm getting the following error: {code} Traceback (most recent call last): File ""/home/airflow/.local/lib/python3.5/site-packages/flask/app.py"", line 1982, in wsgi_app response = self.full_dispatch_request() File ""/home/airflow/.local/lib/python3.5/site-packages/flask/app.py"", line 1614, in full_dispatch_request rv = self.handle_user_exception(e) File ""/home/airflow/.local/lib/python3.5/site-packages/flask/app.py"", line 1517, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/home/airflow/.local/lib/python3.5/site-packages/flask/_compat.py"", line 33, in reraise raise value File ""/home/airflow/.local/lib/python3.5/site-packages/flask/app.py"", line 1612, in full_dispatch_request rv = self.dispatch_request() File ""/home/airflow/.local/lib/python3.5/site-packages/flask/app.py"", line 1598, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/home/airflow/.local/lib/python3.5/site-packages/flask_appbuilder/security/decorators.py"", line 25, in wraps if self.appbuilder.sm.has_access(permission_str, self.__class__.__name__): File ""/home/airflow/.local/lib/python3.5/site-packages/flask_appbuilder/security/manager.py"", line 834, in has_access return self._has_view_access(g.user, permission_name, view_name) File ""/home/airflow/.local/lib/python3.5/site-packages/flask_appbuilder/security/manager.py"", line 820, in _has_view_access roles = user.roles File ""/home/airflow/.local/lib/python3.5/site-packages/werkzeug/local.py"", line 347, in __getattr__ return getattr(self._get_current_object(), name) AttributeError: 'AnonymousUserMixin' object has no attribute 'roles' {code} Any ideas what could be the issue?"
276,AIRFLOW-3196,AIRFLOW,1539356073000,1558149416000,Bug,Resolved,Critical,2,1248,"Filtering from admin views on any dates fails Any filtering on any date fields from all admin views fails with : {code} ------------------------------------------------------------------------------- Node: airflow-web-95fd56cd6-csvh7 ------------------------------------------------------------------------------- Traceback (most recent call last): File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1116, in _execute_context context = constructor(dialect, self, conn, *args) File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 623, in _init_compiled param.append(processors[key](compiled_params[key])) File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/sql/type_api.py"", line 1078, in process return process_param(value, dialect) File ""/usr/local/lib/python3.6/site-packages/airflow/utils/sqlalchemy.py"", line 156, in process_bind_param raise ValueError('naive datetime is disallowed') ValueError: naive datetime is disallowed {code} It looks like DateTimeField from wtforms is not TZ aware in https://github.com/apache/incubator-airflow/blob/master/airflow/www/forms.py#L31 Probably using https://github.com/apache/incubator-airflow/blob/master/airflow/utils/timezone.py#L98 will help."
277,AIRFLOW-3203,AIRFLOW,1539445077000,1542215287000,Bug,Closed,Critical,2,369,"Bugs in DockerOperator & Some operator test scripts were named incorrectly Usage of `cpu_shares` and `cpu_shares` is incorrect in DockerOperator, based on documentation of Python package ""docker"". In addition, its test is not really working due to incorrect file name. This also happens for some other test scripts for Operators. This results in test discovery failure."
278,AIRFLOW-3208,AIRFLOW,1539622760000,1620203888000,Bug,Resolved,Critical,2,3558,"Apache airflow 1.8.0 integration with LDAP anonmyously Hello., We wanted to have airflow integration with LDAP anonymously, the LDAP is based on either ""openldap"" or ""389 directory Server"". Below is the detail added in the airflow.cfg : {noformat} [webserver] authenticate = True auth_backend = airflow.contrib.auth.backends.ldap_auth {noformat} {noformat} [ldap] uri = ldap://nsp-daf178e8.ad1.prd.us-phx.odc.im:389 user_filter = user_name_attr = uid group_member_attr = groupMembership=ou=groups,dc=odc,dc=im superuser_filter = memberOf=cn=rvd-sudo_all-prd_usphx,ou=groups,dc=odc,dc=im data_profiler_filter = bind_user = ou=people,dc=odc,dc=im bind_password = basedn = ou=people,dc=odc,dc=im cacert = /opt/orchestration/airflow/ldap_ca.crt search_scope = SUBTREE{noformat} However, when trying to validate, it failed with below exception, please advise what to correct as per provided detail of LDAP as per above ? We only use ""basedn=ou=people,dc=odc,dc=im"" with provided LDAP host and was able to access anonymously when tried using jxplorer workbench. We are able to do LDAP anonymously both on kibana/elasticsearch/jenkins, however coming to airflow, please advise solution. {noformat} Traceback (most recent call last): File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1988, in wsgi_app response = self.full_dispatch_request() File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1641, in full_dispatch_request rv = self.handle_user_exception(e) File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1544, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/usr/local/lib/python3.6/site-packages/flask/_compat.py"", line 33, in reraise raise value File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1639, in full_dispatch_request rv = self.dispatch_request() File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1625, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/usr/local/lib/python3.6/site-packages/flask_admin/base.py"", line 69, in inner return self._run_view(f, *args, **kwargs) File ""/usr/local/lib/python3.6/site-packages/flask_admin/base.py"", line 368, in _run_view return fn(self, *args, **kwargs) File ""/usr/local/lib/python3.6/site-packages/airflow/www/views.py"", line 650, in login return airflow.login.login(self, request) File ""/usr/local/lib/python3.6/site-packages/airflow/contrib/auth/backends/ldap_auth.py"", line 268, in login LdapUser.try_login(username, password) File ""/usr/local/lib/python3.6/site-packages/airflow/contrib/auth/backends/ldap_auth.py"", line 180, in try_login search_scope=native(search_scope)) File ""/usr/local/lib/python3.6/site-packages/ldap3/core/connection.py"", line 779, in search check_names=self.check_names) File ""/usr/local/lib/python3.6/site-packages/ldap3/operation/search.py"", line 372, in search_operation request['filter'] = compile_filter(parse_filter(search_filter, schema, auto_escape, auto_encode, validator, check_names).elements[0]) # parse the searchFilter string and compile it starting from the root node File ""/usr/local/lib/python3.6/site-packages/ldap3/operation/search.py"", line 206, in parse_filter current_node.append(evaluate_match(search_filter[start_pos:end_pos], schema, auto_escape, auto_encode, validator, check_names)) File ""/usr/local/lib/python3.6/site-packages/ldap3/operation/search.py"", line 89, in evaluate_match raise LDAPInvalidFilterError('invalid matching assertion') ldap3.core.exceptions.LDAPInvalidFilterError: invalid matching assertion {noformat}"
279,AIRFLOW-3227,AIRFLOW,1539846983000,1539856262000,Bug,Closed,Critical,2,732,Airflow is broken when deleting a Variable I created a Variable and used it with one of my DAGs. After a while I modified the DAG and it no longer requires this Variable. I deleted the Variable from the UI using the delete button and then Airflow generates this message: {color:#242729}Broken DAG: [/home/ubuntu/airflow/dags/loadairflow.py] u'Variable order_id_imported does not exist'{color} Only when re-creating the Variable the message disappears. This message has nothing to do with my DAG. It's a BUG in loadairflow.py file Please issue a fix for this. It forces me to keep having Variables I don't need. StackOverflow issue: https://stackoverflow.com/questions/52856018/why-airflow-force-me-to-have-variable-which-i-dont-need
280,AIRFLOW-3229,AIRFLOW,1539870825000,1548933813000,Bug,Resolved,Critical,2,247,New install with python3.7 fail - Need to bump tenacity version Current master requires tenacity==4.8 as dependency but [tenacity 4.8|https://github.com/jd/tenacity/tree/4.8.0/tenacity] is not compatible with python 3.7 because of `async` keyword.
281,AIRFLOW-3240,AIRFLOW,1540132141000,1562255101000,Task,Open,Critical,2,46,Airflow dags are not working (not starting t1)
282,AIRFLOW-3255,AIRFLOW,1540457356000,1548349915000,Bug,Open,Critical,2,304,"Custom dependencies are overwriting each other Hi, I am having two different zip files each contains the dag and his dependencies (a python liberty that I wrote). In each zip I have a different version of this dependency. For some reason it seems that dependencies are overwriting each other. any Ideas ?"
283,AIRFLOW-3256,AIRFLOW,1540457916000,1556107469000,Bug,Closed,Critical,2,229,Airflow doesn't run the executions in the correct order I have created a DAG with 'max_active_runs'. But for some reason I see two executions of the same DAG. Also the 6:00 execution appears before the 4:45 execution. Any ideas ?
284,AIRFLOW-3295,AIRFLOW,1541227202000,1541437245000,Bug,Resolved,Critical,2,604,"Require encryption in DaskExecutor when certificates are configured. When user decide to use SSL encryption for DaskExecutor communications, `Distributed.Security` object will be created. However, one argument (`require_encryption`) is missed to be set to `True` (its default value is `False`). This may fail the SSL encryption setting-up. Current implementation: [https://github.com/apache/incubator-airflow/blob/120f4856cdea5134971c4c4a239ddbfdc80db77e/airflow/executors/dask_executor.py#L48] Related Documentation: http://distributed.dask.org/en/latest/tls.html?highlight=require_encryption#parameters"
285,AIRFLOW-3296,AIRFLOW,1541408490000,1556105174000,Bug,Open,Critical,2,1228,"Taks Failed with empty Log and without sending email I have a DAG with 60 tasks (PythonsOperators) and in some executions different tasks are marked as failed, but I dont know the reason, when I go to ""View Log"" the log is empty and when i pass over the red square it says Operator:null what does that mean? It seems like it hasn't executed the task, but I don't understand why. The questions are: * Why Airflow mark it as failed but there is no execution showed on log? * Why hasn't it send email of error if the tasks it's marked as failed? Here is the python code associated to the dag: {code:java} DEFAULT_ARGS = { 'owner': 'blablabla', 'depends_on_past': False, 'start_date': datetime(2018, 5, 8), 'catchup': False, 'email': ['mail@mail.com'], 'email_on_failure': True, 'email_on_retry': False, 'retries': 3, 'max_active_runs': 1, 'retry_delay': timedelta(minutes=5) } dag = DAG('dag_name', default_args=DEFAULT_ARGS, schedule_interval='20 0 * * *') mylist = get_codes_list() for item in mylist: healthcheckerName = 'healthchecker_' + item healthchecker = PythonOperator( dag=dag, task_id=healthcheckerName, python_callable=prime_ops.check_last_budget_calculation(item), queue=SPECIFIC_QUEUE, pool=DEFAULT_PPC_POOL ){code}"
286,AIRFLOW-3301,AIRFLOW,1541471736000,1543620955000,Test,Resolved,Critical,2,170,"Update CI test for [AIRFLOW-3132] (PR #3977) In PR [https://github.com/apache/incubator-airflow/pull/3977,] test is not updated accordingly, and it results in CI failure."
287,AIRFLOW-3323,AIRFLOW,1541757596000,1543689690000,New Feature,Resolved,Critical,2,531,"Support Basic Authentication for Flower The current `airflow flower` doesn't come with any authentication. This may make essential information exposed to in an untrusted environment. Currently Flower itself supports * HTTP Basic Authentication * Google OAuth 2.0 * GitHub OAuth Given Flower is not really the most essential component of Airflow, we don't have to support all its authentication methods. But may be good to at least support Basic Authentication. This ticket adds support to Basic Authentication for `Airflow Flower`."
288,AIRFLOW-3356,AIRFLOW,1542310940000,1558905684000,Bug,Closed,Critical,2,746,"Scheduler gets stuck for certain DAGs I observe the scheduler getting stuck for certain DAGs: Nov 15 19:11:48 ip-172-16-13-120 python3.6[1319]: File Path PID Runtime Last Runtime Last Run Nov 15 19:11:48 ip-172-16-13-120 python3.6[1319]: /home/ubuntu/airflow/dags/stuck_dag.py 14241 *19977.55s* 1.05s 2018-11-15T13:38:47 Nov 15 19:11:48 ip-172-16-13-120 python3.6[1319]: /home/ubuntu/airflow/dags/not_stuck_dag.py 19906 0.05s 1.05s 2018-11-15T19:11:44 The ""Runtime"" of the stuck DAG's scheduling process is huge and I can't tell what it's doing. There's no mention of that DAG in the scheduler logs otherwise. The mapped process looks like this: ubuntu 14241 0.0 0.3 371132 63232 ? S 13:38 0:00 /usr/bin/python3.6 /usr/local/bin/airflow scheduler"
289,AIRFLOW-3418,AIRFLOW,1543501746000,1544126914000,Bug,Open,Critical,2,11800,"Task stuck in running state, unable to clear One of our tasks (a custom operator that sleep-waits until NYSE market close) got stuck in a ""running"" state in the metadata db without making any progress. This is what it looked like in the logs: {code:java} [2018-11-29 00:01:14,064] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close [2018-11-29 00:01:14,063] {{cli.py:484}} INFO - Running <TaskInstance: reconciliation_filemover.after_close 2018-11-28T00:00:00+00:00 [running]> on host airflow-core-i-0a53cac37067d957d.dlg.fnd.dynoquant.com [2018-11-29 06:03:57,643] {{models.py:1355}} INFO - Dependencies not met for <TaskInstance: reconciliation_filemover.after_close 2018-11-28T00:00:00+00:00 [running]>, dependency 'Task Instance State' FAILED: Task is in the 'running' state which is not a valid state for execution. The task must be cleared in order to be run. [2018-11-29 06:03:57,644] {{models.py:1355}} INFO - Dependencies not met for <TaskInstance: reconciliation_filemover.after_close 2018-11-28T00:00:00+00:00 [running]>, dependency 'Task Instance Not Already Running' FAILED: Task is already running, it started on 2018-11-29 00:01:10.876344+00:00. [2018-11-29 06:03:57,646] {{logging_mixin.py:95}} INFO - [2018-11-29 06:03:57,646] {{jobs.py:2614}} INFO - Task is not able to be run {code} Seeing this state, we attempted to ""clear"" it in the web UI. This yielded a complex backtrace: {code:java} Traceback (most recent call last): File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/flask/app.py"", line 1982, in wsgi_app response = self.full_dispatch_request() File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/flask/app.py"", line 1614, in full_dispatch_request rv = self.handle_user_exception(e) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/flask/app.py"", line 1517, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/flask/_compat.py"", line 33, in reraise raise value File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/flask/app.py"", line 1612, in full_dispatch_request rv = self.dispatch_request() File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/flask/app.py"", line 1598, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/flask_appbuilder/security/decorators.py"", line 26, in wraps return f(self, *args, **kwargs) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/www_rbac/decorators.py"", line 55, in wrapper return f(*args, **kwargs) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/www_rbac/views.py"", line 837, in clear include_upstream=upstream) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/models.py"", line 4011, in sub_dag dag = copy.deepcopy(self) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 166, in deepcopy y = copier(memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/models.py"", line 3996, in __deepcopy__ setattr(result, k, copy.deepcopy(v, memo)) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 155, in deepcopy y = copier(x, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 243, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 166, in deepcopy y = copier(memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/models.py"", line 2740, in __deepcopy__ setattr(result, k, copy.deepcopy(v, memo)) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 182, in deepcopy y = _reconstruct(x, rv, 1, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 297, in _reconstruct state = deepcopy(state, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 155, in deepcopy y = copier(x, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 243, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 182, in deepcopy y = _reconstruct(x, rv, 1, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 297, in _reconstruct state = deepcopy(state, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 155, in deepcopy y = copier(x, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 243, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 182, in deepcopy y = _reconstruct(x, rv, 1, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 297, in _reconstruct state = deepcopy(state, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 155, in deepcopy y = copier(x, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 243, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 155, in deepcopy y = copier(x, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 243, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 182, in deepcopy y = _reconstruct(x, rv, 1, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 297, in _reconstruct state = deepcopy(state, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 155, in deepcopy y = copier(x, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 243, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 155, in deepcopy y = copier(x, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 218, in _deepcopy_list y.append(deepcopy(a, memo)) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 182, in deepcopy y = _reconstruct(x, rv, 1, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 297, in _reconstruct state = deepcopy(state, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 155, in deepcopy y = copier(x, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 243, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/copy.py"", line 174, in deepcopy rv = reductor(4) TypeError: cannot serialize '_io.TextIOWrapper' object {code} After browsing through Airflow's code I had a suspicion that this was simply the ""clear"" code in the UI not handling some property on one of our operators. I instead used the Browse feature to edit the metadata state db directly. This did result in the status change; in the task being set to ""up_for_retry"", and the same logfile now having additional contents: {code:java} [2018-11-29 14:18:11,390] {{logging_mixin.py:95}} INFO - [2018-11-29 14:18:11,390] {{jobs.py:2695}} WARNING - State of this instance has been externally set to failed. Taking the poison pill. [2018-11-29 14:18:11,399] {{helpers.py:240}} INFO - Sending Signals.SIGTERM to GPID 5287 [2018-11-29 14:18:11,399] {{models.py:1636}} ERROR - Received SIGTERM. Terminating subprocesses. [2018-11-29 14:18:11,418] {{models.py:1760}} ERROR - Task received SIGTERM signal Traceback (most recent call last): File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/models.py"", line 1654, in _run_raw_task result = task_copy.execute(context=context) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/sensors/base_sensor_operator.py"", line 78, in execute sleep(self.poke_interval) File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/models.py"", line 1638, in signal_handler raise AirflowException(""Task received SIGTERM signal"") airflow.exceptions.AirflowException: Task received SIGTERM signal [2018-11-29 14:18:11,420] {{models.py:1783}} INFO - Marking task as UP_FOR_RETRY [2018-11-29 14:18:11,445] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close Traceback (most recent call last): [2018-11-29 14:18:11,445] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close File ""/home/airflow/virtualenvs/airflow/bin/airflow"", line 32, in <module> [2018-11-29 14:18:11,445] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close args.func(args) [2018-11-29 14:18:11,445] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/utils/cli.py"", line 74, in wrapper [2018-11-29 14:18:11,445] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close return f(*args, **kwargs) [2018-11-29 14:18:11,445] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/bin/cli.py"", line 490, in run [2018-11-29 14:18:11,445] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close _run(args, dag, ti) [2018-11-29 14:18:11,445] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/bin/cli.py"", line 406, in _run [2018-11-29 14:18:11,445] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close pool=args.pool, [2018-11-29 14:18:11,446] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/utils/db.py"", line 74, in wrapper [2018-11-29 14:18:11,446] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close return func(*args, **kwargs) [2018-11-29 14:18:11,446] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/models.py"", line 1654, in _run_raw_task [2018-11-29 14:18:11,446] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close result = task_copy.execute(context=context) [2018-11-29 14:18:11,446] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/sensors/base_sensor_operator.py"", line 78, in execute [2018-11-29 14:18:11,446] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close sleep(self.poke_interval) [2018-11-29 14:18:11,446] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close File ""/home/airflow/virtualenvs/airflow/lib/python3.5/site-packages/airflow/models.py"", line 1638, in signal_handler [2018-11-29 14:18:11,446] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close raise AirflowException(""Task received SIGTERM signal"") [2018-11-29 14:18:11,446] {{base_task_runner.py:101}} INFO - Job 38275: Subtask after_close airflow.exceptions.AirflowException: Task received SIGTERM signal [2018-11-29 14:18:11,693] {{helpers.py:230}} INFO - Process psutil.Process(pid=5287 (terminated)) (5287) terminated with exit code 1 [2018-11-29 14:18:11,694] {{logging_mixin.py:95}} INFO - [2018-11-29 14:18:11,693] {{jobs.py:2627}} INFO - Task exited with return code 0 {code} The log line about ""not able to be run"" comes from jobs.py and it's unclear to me why this would be called in this case (two workers grabbing the same message...?) or why the task would just hang in a ""running"" state: https://github.com/apache/incubator-airflow/blob/1.10.1/airflow/jobs.py#L2614 We had not previously observed any of this behavior. We had just upgraded to 1.10.1 earlier this week."
290,AIRFLOW-3449,AIRFLOW,1544041381000,1556717773000,Bug,Resolved,Critical,2,2264,"Airflow DAG parsing logs aren't written when using S3 logging The default Airflow logging class outputs provides some logs to stdout, some to ""task"" folders, and some to ""processor"" folders (generated during DAG parsing). The 1.10.0 logging update broke this, but only for users who are also using S3 logging. This is because of this feature in the default logging config file: {code:python} if REMOTE_LOGGING and REMOTE_BASE_LOG_FOLDER.startswith('s3://'): DEFAULT_LOGGING_CONFIG['handlers'].update(REMOTE_HANDLERS['s3']) {code} That replaces this functioning handlers block: {code:python} 'task': { 'class': 'airflow.utils.log.file_task_handler.FileTaskHandler', 'formatter': 'airflow', 'base_log_folder': os.path.expanduser(BASE_LOG_FOLDER), 'filename_template': FILENAME_TEMPLATE, }, 'processor': { 'class': 'airflow.utils.log.file_processor_handler.FileProcessorHandler', 'formatter': 'airflow', 'base_log_folder': os.path.expanduser(PROCESSOR_LOG_FOLDER), 'filename_template': PROCESSOR_FILENAME_TEMPLATE, }, {code} With this non-functioning block: {code:python} 'task': { 'class': 'airflow.utils.log.s3_task_handler.S3TaskHandler', 'formatter': 'airflow', 'base_log_folder': os.path.expanduser(BASE_LOG_FOLDER), 's3_log_folder': REMOTE_BASE_LOG_FOLDER, 'filename_template': FILENAME_TEMPLATE, }, 'processor': { 'class': 'airflow.utils.log.s3_task_handler.S3TaskHandler', 'formatter': 'airflow', 'base_log_folder': os.path.expanduser(PROCESSOR_LOG_FOLDER), 's3_log_folder': REMOTE_BASE_LOG_FOLDER, 'filename_template': PROCESSOR_FILENAME_TEMPLATE, }, {code} The key issue here is that both ""task"" and ""processor"" are being given a ""S3TaskHandler"" class to use for logging. But that is not a generic S3 class; it's actually a subclass of FileTaskHandler! https://github.com/apache/incubator-airflow/blob/1.10.1/airflow/utils/log/s3_task_handler.py#L26 Since the template vars don't match the template string, the path to log to evaluates to garbage. The handler then silently fails to log anything at all. It is likely that anyone using a default-like logging config, plus the remote S3 logging feature, stopped getting DAG parsing logs (either locally *or* in S3) as of 1.10.0 Commenting out the DAG parsing section of the S3 block fixed this on my instance."
291,AIRFLOW-3483,AIRFLOW,1544214585000,1562254987000,Bug,Open,Critical,2,859,"Can't authentication with Google Oauth (UnicodeDecodeError: 'utf8' codec can't decode byte) After updating our Airflow installation from 1.8.0 to 1.10.0 (tried 1.10.1 as well) users with non-ascii charts (i.e. , , ) on their usernames can't authenticate anymore through Google Oauth. Users were already on the mysql database from before the upgrade. The mysql database is encoded as latin1. I've tried setting the users table encoding to utf-8 and it didn't fix the problem. I've also tried to explicitly set 'sql_engine_encoding = utf-8' and it didn't fix it either. I've also tried reinstalling with both 'unidecode' and 'text-unidecode' libs and it didn't change. It seems somewhat similar to this bug: https://jira.apache.org/jira/browse/AIRFLOW-2192 Attached are the logs a colleague (Jos Arajo) gets after authenticating, when the front page loads."
292,AIRFLOW-3498,AIRFLOW,1544528367000,1550140350000,Bug,Closed,Critical,2,861,"airflow cli subcommand - trigger_dag not working when start_date = datetime.utcnow() 1. problem : with these default args {code:python} default_args = { ""owner"": ""airflow"", ""depends_on_past"": False, ""start_date"": datetime.utcnow(), } {code} Using airflow trigger_dag <dag_id> to trigger specific dag with success, but that DAG run is stuck with `running` status all the time. But not affect 1.10.0 Quick produce steps : {code:bash} git clone https://github.com/puckel/docker-airflow.git cd docker-airflow sed -i ""s/timedelta(1)/None/g"" dags/tuto.py sed -i ""s/datetime(2015, 6, 1)/datetime.utcnow()/g"" dags/tuto.py export COMPOSE_FILE=docker-compose-LocalExecutor.yml docker-compose up -d # please wait for 30s. docker-compose exec webserver /entrypoint.sh airflow unpause tutorial docker-compose exec webserver /entrypoint.sh airflow trigger_dag tutorial {code}"
293,AIRFLOW-3539,AIRFLOW,1545141593000,1569864384000,Bug,Open,Critical,2,1579,"Error Importing sub-dag (Webserver module load fail, scheduler works fine) Hi, I'm deploying the airflow in a VM instance in google cloud plataform, using the docker image of puckel/docker-airflow, and using the docker-compose-Celery-Executor.yaml. When I run the dags locally the imports seens fine. but when I deploy on this docker-compose the import seems wrong to the webserver, the scheduler runs the dag fine, no problems at sight, when I turn on the dag in the UI the dag runs in the scheduler/ worker. but on the webserver the import gives me this error: {code:java} [2018-12-18 13:06:15,273] {{__init__.py:51}} INFO - Using executor CeleryExecutor [2018-12-18 13:06:15,762] {{models.py:271}} INFO - Filling up the DagBag from /usr/local/airflow/dags [2018-12-18 13:06:16,290] {{models.py:380}} ERROR - Failed to import: /usr/local/airflow/dags/dags/daily_etl.py Traceback (most recent call last): File ""/usr/local/lib/python3.6/site-packages/airflow/models.py"", line 377, in process_file m = imp.load_source(mod_name, filepath) File ""/usr/local/lib/python3.6/imp.py"", line 172, in load_source module = _load(spec) File ""<frozen importlib._bootstrap>"", line 684, in _load File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed File ""/usr/local/airflow/dags/dags/daily_etl.py"", line 4, in <module> from dags.daily_extract import dag as dag_extract ModuleNotFoundError: No module named 'dags.daily_extract' {code}"
294,AIRFLOW-3543,AIRFLOW,1545249937000,1553125416000,Bug,Resolved,Critical,2,1483,"rescheduled tasks block DAG deletion This applies to current master branch after [AIRFLOW-2747|https://github.com/apache/incubator-airflow/commit/dc59d7e2750aa90e099afad8689f2646f18f92a6] was merged. Once a sensor task is rescheduled, the task cannot be deleted from the DB due to a foreign key constraint. This prevents deletion of tasks and DAGS. This occurs regardless of whether the DAG is still running or whether the sensor is actually rescheduled to run in the future or not (ie the task may complete successfully but its entry still resides as a row in the task_reschedule table. I am running a postgres-backed airflow instance. {{Traceback (most recent call last):}} {{ File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1193, in _execute_context}} {{context)}}}} {{File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 509, in do_execute}}}} {{cursor.execute(statement, parameters)}} {{psycopg2.IntegrityError: update or delete on table ""task_instance"" violates foreign key constraint ""task_reschedule_dag_task_date_fkey"" on table ""task_reschedule""}} {{DETAIL: Key (task_id, dag_id, execution_date)=(check_images_ready_11504, flight5105_v0.0.1, 2018-12-13 00:00:00+00) is still referenced from table ""task_reschedule"".}} {{sqlalchemy.exc.IntegrityError: (psycopg2.IntegrityError) update or delete on table ""task_instance"" violates foreign key constraint ""task_reschedule_dag_task_date_fkey"" on table ""task_reschedule""}}"
295,AIRFLOW-3562,AIRFLOW,1545655019000,1620203856000,Improvement,Resolved,Critical,2,753,"[AIP-12] Remove DagBag dependency inside the webserver Currently the webserver does depend on the dag directory and reparsing the dag files. Because this happens with gunicorn each process has there own DagBag. The means the webserver is statefull and depending which process is assigned a request the information can be changed. If all information will come from the database this will not happen. The only process that should add dag/dagruns is the scheduler. The webserver should have enough on only the database. Currently not all information is already in the database. This could be fixable, already started this in AIRFLOW-3561. I do notice this might give some conflicts with the brace change that is coming. Any suggestion how to approach this?"
296,AIRFLOW-3577,AIRFLOW,1545918764000,1545920963000,Bug,Closed,Critical,2,794,"BigQueryToBigQueryOperator doesn't work for BigQuery dataset in regions other than US and EU When using BigQueryToBigQueryOperator to copy a bigquery table to another table that resides in location other than US and EU, it will fail because can't query job status. When the operator runs the copy using Airflow bigquery hook here: [https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/operators/bigquery_to_bigquery.py#L90] Eventually the bigquery hook pools the job by calling[ bigquery's get job API|https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/get] [https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/hooks/bigquery_hook.py#L1092-L1094] The bigquery's get job API requires to pass the location parameter if the job is not in US or EU."
297,AIRFLOW-3606,AIRFLOW,1546319497000,1553704027000,Bug,Resolved,Critical,2,397,Fix Flake8 test & Fix the Flake8 errors introduced since Flake8 test was broken The flake8 test in the Travis CI was broken since [https://github.com/apache/incubator-airflow/pull/4361] ([https://github.com/apache/incubator-airflow/commit/7a6acbf5b343e4a6895d1cc8af75ecc02b4fd0e8] ) And some Flake8 errors (code style/quality issues. found in 10 files) were introduce since flake8 test was broken.
298,AIRFLOW-3627,AIRFLOW,1546572199000,1546993516000,Improvement,Resolved,Critical,2,246,"Improve performance of /task_stats in views.py of both /www and /www_rbac /task_stats is used heavily in UI. Every time the main page is loaded, it will be called. But actually the performance of it can be improved significantly by minor changes."
299,AIRFLOW-3634,AIRFLOW,1546693816000,1546694174000,Test,Resolved,Critical,2,913,"GCP Spanner Test is broken ====================================================================== 25) FAIL: test_database_update_ex_if_database_not_exist (tests.contrib.operators.test_gcp_spanner_operator.CloudSpannerTest) ---------------------------------------------------------------------- Traceback (most recent call last): .tox/py27-backend_mysql/lib/python2.7/site-packages/mock/mock.py line 1305 in patched return func(*args, **keywargs) tests/contrib/operators/test_gcp_spanner_operator.py line 354 in test_database_update_ex_if_database_not_exist ""instance 'instance-id' is missing"", str(err)) AssertionError: ""The Cloud Spanner database 'db1' in project 'project-id' and instance 'instance-id' is missing"" not found in '(""The Cloud Spanner database \'%s\' in project \'%s\' and instance \'%s\' is missing. Create the database first before you can update it."", \'db1\', \'project-id\', \'instance-id\')'"
300,AIRFLOW-3635,AIRFLOW,1546745500000,1564566479000,Improvement,Resolved,Critical,2,223,Fix incorrect logic in delete_dag (introduced in PR #4406) Incorrect logic was introduced in PR #4406 ([https://github.com/apache/airflow/pull/4406)] This was not found out because the Travis CI was not working as expected.
301,AIRFLOW-3700,AIRFLOW,1547429903000,1547494904000,Improvement,Resolved,Critical,2,417,"Change the lowest allowed version of ""requests"" to address security vulnerabilities [https://nvd.nist.gov/vuln/detail/CVE-2018-18074] {color:#24292e}The Requests package through 2.19.1 before 2018-09-14 for Python sends an HTTP Authorization header to an http URI upon receiving a same-hostname https-to-http redirect, which makes it easier for remote attackers to discover credentials by sniffing the network.{color}"
302,AIRFLOW-3761,AIRFLOW,1548306427000,1576625015000,Improvement,Resolved,Critical,2,276,"Decommission User & Chart models; Update doc accordingly In master branch, we have already decommissioned the Flask-Admin UI. In model definitions, User and Chart are only applicable for the ""old"" UI based on Flask-Admin. Hence we should decommission these two models as well."
303,AIRFLOW-3768,AIRFLOW,1548453946000,1553035282000,Bug,Resolved,Critical,2,174,"XSS Vulnerability in Search Query Parameter In the DAGs page, there is a XSS issue in the search parameter. The input is reflected from the search parameter back to the user."
304,AIRFLOW-3769,AIRFLOW,1548454109000,1555516001000,Bug,Resolved,Critical,2,281,"Open Redirect Vulnerability in Admin Create Variable Page In the /admin/variable/new page, it is possible to inject an open redirect URL into the URL query parameter which is executed in the List anchor of the page. This can be exploited to redirect an admin to a malicious domain."
305,AIRFLOW-3873,AIRFLOW,1549938583000,1567605830000,Bug,Open,Critical,2,2667,"Issue with DAG dependency using ExternalTaskSensor I have two DAGs Created and want to set dependencies between them using externalTaskSensor as shown below. I am getting the error as ""Broken DAG: [/data1/airflow/dags/testdagdependency.py] No module named snakebite.client"". Please help me on this. ================================================================== *DAG 1:* from airflow import DAG from airflow.operators.bash_operator import BashOperator from datetime import datetime, timedelta default_args = { 'owner': 'Krishna Garapati', 'depends_on_past': False, 'start_date': datetime(2019, 2, 9), 'email': ['krishna.garapati@transamerica.com'], 'email_on_failure': True, 'email_on_retry': True, 'retries': 0, 'retry_delay': timedelta(minutes=5) #'queue': 'finance-ingestion', # 'run_as_user': 'sptfinactmodel' # 'pool': 'backfill', # 'priority_weight': 10, # 'end_date': datetime(2016, 1, 1), } dag = DAG('pythontest',default_args=default_args,schedule_interval='27 2 * * *') # t1, t2 and t3 are examples of tasks created by instantiating operators t1 = BashOperator( task_id='print_date', bash_command='date', dag=dag) t2 = BashOperator( task_id='pythontest', bash_command='\{{""python /preprod/finance/financedatastagedev/scripts/airflowtest/hive/test.py""}}', dag=dag) t2.set_upstream(t1) ========================================================== *DAG 2 ( Keeping dependency on DAG1)* from airflow import DAG from airflow.operators.bash_operator import BashOperator from datetime import datetime, timedelta from airflow.operators.sensors import ExternalTaskSensor default_args = { 'owner': 'Krishna Garapati', 'depends_on_past': False, 'start_date': datetime(2019, 2, 11), 'email': ['krishna.garapati@transamerica.com'], 'email_on_failure': True, 'email_on_retry': True, 'retries': 0, 'retry_delay': timedelta(minutes=5) #'queue': 'finance-ingestion', # 'run_as_user': 'sptfinactmodel' # 'pool': 'backfill', # 'priority_weight': 10, # 'end_date': datetime(2016, 1, 1), } dag = DAG('testdagdependency',default_args=default_args,schedule_interval='27 15 * * *') wait_for_pythontest = ExternalTaskSensor( task_id='wait_for_pythontest', external_dag_id='pythontest', external_task_id='pythontest', execution_delta=None, # Same day as today dag=dag) # t1, t2 and t3 are examples of tasks created by instantiating operators t1 = BashOperator( task_id='print_date', bash_command='date', dag=dag) t2 = BashOperator( task_id='testdependency', bash_command='\{{""python /preprod/finance/financedatastagedev/scripts/airflowtest/hive/test.py""}}', dag=dag) wait_for_pythontest >> testdagdependency t2.set_upstream(t1) ====================================================="
306,AIRFLOW-3878,AIRFLOW,1549965711000,1558148040000,Bug,Resolved,Critical,2,626,"K8 Pod creation error if dag_id exceeds 63 characters Getting following error while creating K8 worker Pod. Dag Id length is > 63 char and as per [https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/] lable value should be <=63 HTTP response body: \{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""unable to parse requirement: invalid label value: \""<Dag Id>\"": must be no more than 63 characters"",""reason"":""BadRequest"",""code"":400} K8 Executor sets the dagId as the label value while creating k8pod. Avoid adding dag_id and task_id as pod labels if their size exceeds 63 char"
307,AIRFLOW-3931,AIRFLOW,1550693063000,1553518883000,Improvement,Resolved,Critical,2,403,Dataflow template does not support network paramater The DataflowTemplateOperator uses dataflow.projects.locations.templates.launch which has a network and subnetwork parameter but these are not explicitly set by the hook so it uses the network named default in the gcp project. It would be good if you could set the network or subnetwork so that the dataflow job can run in a network other than default
308,AIRFLOW-3947,AIRFLOW,1551076744000,1554243068000,Improvement,Resolved,Critical,2,415,"Add ""Access is Denied"" flash msg for DAG-level access control In FAB UI, when user clicks a page to which he/she doesn't have access, there will be a ""Access is Denied"" flash message. But for the DAG-level access control, when the user does that, he/she would be redirected to the main page without any flash message. This may be confusing to the user. We should add proper flash warning message in the UI for this."
309,AIRFLOW-3964,AIRFLOW,1551231446000,1599605855000,Improvement,Resolved,Critical,2,7895,"Consolidate and de-duplicate sensor tasks h2. Problem h3. Airflow Sensor: Sensors are a certain type of operator that will keep running until a certain criterion is met. Examples include a specific file landing in HDFS or S3, a partition appearing in Hive, or a specific time of the day. Sensors are derived from BaseSensorOperator and run a poke method at a specified poke_interval until it returns True. Airflow Sensor duplication is a normal problem for large scale airflow project. There are duplicated partitions needing to be detected from same/different DAG. In Airbnb there are 88 boxes running four different types of sensors everyday. The number of running sensor tasks ranges from 8k to 16k, which takes great amount of resources. Although Airflow team had redirected all sensors to a specific queue to allocate relatively minor resource, there is still large room to reduce the number of workers and relief DB pressure by optimizing the sensor mechanism. Existing sensor implementation creates an identical task for any sensor task with specific dag_id, task_id and execution_date. This task is responsible of keeping querying DB until the specified partitions exists. Even if two tasks are waiting for same partition in DB, they are creating two connections with the DB and checking the status in two separate processes. In one hand, DB need to run duplicate jobs in multiple processes which will take both cpu and memory resources. At the same time, Airflow need to maintain a process for each sensor to query and wait for the partition/table to be created. h1. ***Design* There are several issues need to be resolved for our smart sensor. h2. Persist sensor infor in DB and avoid file parsing before running Current Airflow implementation need to parse the DAG python file before running a task. Parsing multiple python file in a smart sensor make the case low efficiency and overload. Since sensor tasks need relatively more light weight executing information -- less number of properties with simple structure (most are built in type instead of function or object). We propose to skip the parsing for smart sensor. The easiest way is to persist all task instance information in airflow metaDB. h3. Solution: It will be hard to dump the whole task instance object dictionary. And we do not really need that much information. We add two sets to the base sensor class as persist_fields and execute_fields. h4. persist_fields dump to airflow.task_instance column attr_dict saves the attribute names that should be used to accomplish a sensor poking job. For example: # the NamedHivePartitionSensor define its persist_fields as ('partition_names', 'metastore_conn_id', 'hook') since these properties are enough for its poking function. # While the HivePartitionSensor can be slightly different use persist_fields as ('schema', 'table', 'partition', 'metastore_conn_id') If we have two tasks that have same property value for all field in persist_fields. That means these two tasks are poking the same item and they are holding duplicate poking jobs in senser. *The persist_fields can help us in deduplicate sensor tasks*. In a more broader way. If we can list persist_fields for all operators, it can help to dedup all airflow tasks. h4. Execute_fields dump to airflow.task_instance column exec_dict Saves the execution configuration such as poke_interval, timeout, execution_timeout Fields in this set do not contain information affecting the poking job detail. They are related to how frequent should we poke, when should the task timeout, how many times timeout should be a fail etc. We only put those logic that we can easily handle in a smart sensor for now. This is a smart sensor doable whitelist and can be extended with more logic being unlocked by smart sensor implementation. When we initialize a task instance object. We dump the attribute value of these two sets and persist them in the Airflow metaDB. Smart sensor can visit DB to get all required information of running sensor tasks and dont need to parse any DAG files. h2. Airflow scheduler change We do not want to break any existing logic in scheduler. The smart sensor is a configurable mode and can be easily fallback to scheduler regular logic when it detects the case is not good for smart sensor. h3. Solution h4. Scheduler process_file Right before we set a task instance state to scheduled, add smart sensor check to do: # Check if Airflow is configured as use smart sensor # Check if current task is good for smart sensor running If both check got a yes that means the task instance is qualified for smart sensor. Airflow scheduler set its state to smart_pending instead of scheduled and this task instance will *NOT BE QUEUED* to the executor. It is expected to be picked up by a smart sensor task from DB query. Smart sensor will update the state to final state (success or failed) or up_for_retry and it can come back to normal scheduler world. If any of the above checks has a no answer, either current airflow cluster is not configured to use smart sensor or the task itself is out of smart sensor scope. the scheduler will schedule task instance just like no smart sensor exist. h4. Include smart sensor DAG by configuration We are using a smart sensor DAG to kick off all smart sensor tasks. If airflow is configured to use smart sensor. The DAG will be included in the scheduler parsing paths. Implementation similar as example dags. h2. Smart sensor operator h3. Smart sensor logic In each execute loop: * refresh_all_dict(): Select all tasks from DB with state smart_pending or smart_running and shardcode qualified. * For all tasks in the task dictionary to poke: * If task with same persist_field has been poked in this round * If task poked has a final state, dont need to do anything * If task poked does not have a final state, need to handle timeout * Else (not poked in this round) * Execute the sensor job * For success or failed state, mark states in airflow DB for all tasks that * Have same persist_fields hashcode * State in (smart_pending, smart_running) * Check and handle timeout Issue Smart sensor need to handle the following issues: # Get multiple tasks qualified for smart sensor. # Do the work for all collected sensor tasks # Sensor tasks duplication. # Sharding when there are multiple smart sensor running. Dedup and shard: Attr_dict  hashcode  shardcode Hashcode = hash(attr_dict) Shardcode = Hashcode % (max_shard + 1) The range of shardcode, number of smart sensor tasks can be configured in airflow.cfg Each smart sensor task has a _shardcode range_ and only query tasks whose shardcode in this range. Duplicate sensor task will have the same hash code and same shardcode so they are going to be handled by the same smart sensor task. h2. Schema change: h3. Task_instance table: (add 4 columns and 2 indexes) op.add_column('task_instance', sa.Column('attr_dict', sa.Text(), nullable=True)) op.add_column('task_instance', sa.Column('exec_dict', sa.Text(), nullable=True)) op.add_column('task_instance', sa.Column('hashcode', sa.BigInteger(), nullable=True)) op.add_column('task_instance', sa.Column('shardcode', sa.Integer(), nullable=True)) op.create_index('ti_hashcode', 'task_instance', ['hashcode'], unique=False) op.create_index('ti_shardcode', 'task_instance', ['shardcode'], unique=False) h2. Remaining Issue # Handle timeout: Save the timeout and execution_timeout in exec_dict column. # When a timeout was detected, set the single sensor task to failed or up_for_retry and expect scheduler set it to smart_pending as retry # Calculate the total seconds of final failed duration and keep the task in smart sensor state until it failed/success. (min(timeout, execution_timeout) * (retries + 1)) # Smart sensor DAG handle. Should it be manually or in source code. # Special logic for smart sensor health check."
310,AIRFLOW-3965,AIRFLOW,1551238081000,1569857480000,Bug,Resolved,Critical,2,2839,"GoogleCloudStorageToBigQueryOperator has no ""location"" parameter *GoogleCloudStorageToBigQueryOperator* class does not accept ""location"" parameter and for jobs that are not ""US"", ""EU"" will fail. To be more precise, the bigquery job itself will still work, however the task instance will mark as error because it isn't able to fetch the job status (HTTP 404). Setting the location parameter will fix this problem. See error traceback {code:java} Traceback (most recent call last): File ""/Users/lihanli/.virtualenvs/nbw/lib/python3.6/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 1124, in run_with_configuration try: File ""/Users/lihanli/.virtualenvs/nbw/lib/python3.6/site-packages/googleapiclient/_helpers.py"", line 130, in positional_wrapper return wrapped(*args, **kwargs) File ""/Users/lihanli/.virtualenvs/nbw/lib/python3.6/site-packages/googleapiclient/http.py"", line 851, in execute raise HttpError(resp, content, uri=self.uri) googleapiclient.errors.HttpError: <HttpError 404 when requesting https://www.googleapis.com/bigquery/v2/projects/<my-project-id>/jobs/job_sf_CVX8Pa49m7u6YSaFetj62qxmM?alt=json returned ""Not found: Job <my-project-id>:job_sf_CVX8Pa49m7u6YSaFetj62qxmM""> During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/Users/lihanli/.virtualenvs/nbw/bin/airflow"", line 32, in <module> args.func(args) File ""/Users/lihanli/.virtualenvs/nbw/lib/python3.6/site-packages/airflow/utils/cli.py"", line 74, in wrapper return f(*args, **kwargs) File ""/Users/lihanli/.virtualenvs/nbw/lib/python3.6/site-packages/airflow/bin/cli.py"", line 651, in test ti.run(ignore_task_deps=True, ignore_ti_state=True, test_mode=True) File ""/Users/lihanli/.virtualenvs/nbw/lib/python3.6/site-packages/airflow/utils/db.py"", line 73, in wrapper return func(*args, **kwargs) File ""/Users/lihanli/.virtualenvs/nbw/lib/python3.6/site-packages/airflow/models.py"", line 1750, in run session=session) File ""/Users/lihanli/.virtualenvs/nbw/lib/python3.6/site-packages/airflow/utils/db.py"", line 69, in wrapper return func(*args, **kwargs) File ""/Users/lihanli/.virtualenvs/nbw/lib/python3.6/site-packages/airflow/models.py"", line 1657, in _run_raw_task result = task_copy.execute(context=context) File ""/Users/lihanli/.virtualenvs/nbw/lib/python3.6/site-packages/airflow/contrib/operators/gcs_to_bq.py"", line 257, in execute ignore_unknown_values=self.ignore_unknown_values, File ""/Users/lihanli/.virtualenvs/nbw/lib/python3.6/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 1096, in run_load return self.run_with_configuration(configuration) File ""/Users/lihanli/.virtualenvs/nbw/lib/python3.6/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 1124, in run_with_configuration try: Exception: ('BigQuery job status check failed. Final error was: %s', 404) {code}"
311,AIRFLOW-3989,AIRFLOW,1551451453000,1558148317000,Wish,Resolved,Critical,2,524,"No way to set Kubernetes node_selectors or annotations as environment variables Hey, After checking out this PR: [https://github.com/apache/airflow/pull/4589] I realized, that there is no way to set kubernetes_node_selectors or kubernetes_annotations via environment variables, as the respective sections won't be looked up in environment variables. Is it possible to add this functionality? It blocks us from using environment-based configuration. I saw, that there's the `getsection()` function, but it's not being used..."
312,AIRFLOW-4025,AIRFLOW,1551865173000,1620203871000,Bug,Closed,Critical,2,593,"Rolled my own auth but got error - ValueError('View function did not return a response') As suggested in the docs to roll a different auth for airflow web below conf need to set [webserver] authenticate = True auth_backend = mypackage.auth, I added this in python path, It picked the correct module but It gives an error in the end after doing all the functionality. Like wise other modules of auth I added return redirect(request.args.get(""next"") or url_for(""index"")) in the end ERROR in app: Exception on /admin/airflow/login [GET] raise ValueError('View function did not return a response')"
313,AIRFLOW-4121,AIRFLOW,1552958197000,1552958668000,Bug,Open,Critical,2,1449,"can trigger multiple dag_runs for a dag_id when max_active_runs_per_dag=1 I can trigger multiple dag_runs for a dag_id when max_active_runs_per_dag=1, and I expect Airflow to prevent this with this environment setting. It doesn't fit our use case to be able to trigger/schedule the same dag id when it is currently in the middle of messing with things. It is not really a viable workaround for us to just not trigger the dag if it's running-- this needs to be built in or I need to implement it. Steps I took to reproduce: 1. Download docker image for puckel/docker-airflow:1.10.2 2. Execute the below command {code:java} docker run -p 8080:8080 -e AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=1 -v <absolute path to local dag volumes folder>:/usr/local/airflow/dags -v /$HOME/.aws:/usr/local/airflow/.aws -d <name of local docker image for puckel/docker-airflow:1.10.2> {code} 3. Open Airflow UI in a browser (localhost:8080) and go to Admin --> Configuration and verify max_active_runs_per_dag really is 1. 4. Execute the below command twice but at least 2 seconds apart (work around for https://issues.apache.org/jira/browse/AIRFLOW-764) {code:java} curl -X POST \ http://localhost:8080/api/experimental/dags/<replace with dag id>/dag_runs \ -H 'Cache-Control: no-cache' \ -H 'Content-Type: application/json' -d '{""conf"":""{}""}' {code} 5. In Airflow UI, go to Browse --> Dag Runs 6. Observe multiple dag runs are in the running state for the same dag id"
314,AIRFLOW-4126,AIRFLOW,1553009412000,1566113648000,Improvement,Open,Critical,2,395,Graceful restart of Scheduler I am running Airflow in a 2 node (webserver and scheduler) configuration in k8s using the LocalExecutor. Is there any way to gracefully stop the scheduler? I'd like to stop the scheduler but allow any currently working tasks to finish. I'd like to do this so I can deploy new dags. I looked through the documentation for an answer but have not found one yet. Thanks
315,AIRFLOW-4172,AIRFLOW,1553764991000,1560157647000,Bug,Resolved,Critical,2,232,Invalid parameter --driver-classpath in SparkSubmitHook Spark option --driver-class-path is set wrongly as --driver-classpath while building Spark Submit command in SparkSubmitHook which results in failure while submitting spark job
316,AIRFLOW-4173,AIRFLOW,1553766593000,1553954017000,Improvement,Resolved,Critical,2,598,"Improve scheduler performance by avoid Unnecessary actions in SchedulerJob.process_file() In current implementation of *SchedulerJob.process_file()* ([https://github.com/apache/airflow/blob/068ded96cd279dcd51f5b6d1e96f09205ecf40c8/airflow/jobs.py#L1722-L1734),] action '*dag = dagbag.get_dag(dag_id)*' is to be done no matter if dag_id is pointing to a paused DAG. However, the result will not be used later if that DAG is paused. This is causing inefficiency. We can do the `if DAG is paused` check first, before we invoke '*dag = dagbag.get_dag(dag_id)*'. This may bring considerable improvement."
317,AIRFLOW-4267,AIRFLOW,1554844011000,1555430950000,Bug,Resolved,Critical,2,293,Fix Task Instance duration in Graph View in Fab-Based UI The duration field is missing in the Graph View for the Fab-based RBAC UI. This is because of the bug in the code. Instead of *task.duration* it should be *ti.duration*. The duration is associated with a *Task Instance* and no a *Task*.
318,AIRFLOW-4319,AIRFLOW,1555334716000,1556659201000,Test,Resolved,Critical,2,208,Add tests for Bigquery related Operators Some of the BigQuery Operators are missing tests: * BigQueryToBigQueryOperator * BigQueryToCloudStorageOperator * BigQueryTableDeleteOperator * BigQueryGetDataOperator
319,AIRFLOW-4349,AIRFLOW,1555569730000,1555604352000,Bug,Resolved,Critical,2,1559,"Athena Operator Marked Successful on Failure Steps to recreate: Execute an Athena operator that returns a failed stage (in this case some missing S3 pemissions). Athena returns a failed state but the failed state code is not operating as expected. {code:java} [2019-04-18 00:06:35,222] {{logging_mixin.py:95}} INFO - [2019-04-18 00:06:35,222] {{aws_athena_hook.py:129}} INFO - Trial 1: Query is still in an intermediate state - RUNNING [2019-04-18 00:07:05,272] {{logging_mixin.py:95}} INFO - [2019-04-18 00:07:05,272] {{connectionpool.py:203}} INFO - Starting new HTTP connection (1): XXX.XXX.XXX.XXX [2019-04-18 00:07:05,282] {{logging_mixin.py:95}} INFO - [2019-04-18 00:07:05,282] {{connectionpool.py:203}} INFO - Starting new HTTP connection (1): XXX.XXX.XXX.XXX [2019-04-18 00:07:05,286] {{logging_mixin.py:95}} INFO - [2019-04-18 00:07:05,286] {{connectionpool.py:238}} INFO - Resetting dropped connection: athena.us-east-1.amazonaws.com [2019-04-18 00:07:05,377] {{logging_mixin.py:95}} INFO - [2019-04-18 00:07:05,377] {{aws_athena_hook.py:132}} INFO - Trial 2: Query execution completed. Final state is FAILED ... ... ... [2019-04-18 00:07:08,560] {{logging_mixin.py:95}} INFO - [2019-04-18 00:07:08,559] {{jobs.py:2562}} INFO - Task exited with return code 0 {code} Looking at the code if the query is a failure it should then return this from the Operator: {code:java} 'Final state of Athena job is {}, query_execution_id is {}.' {code} But that is not in the logs. The task is then mark as successfully completed in Airflow and the DAG continues."
320,AIRFLOW-4428,AIRFLOW,1556566351000,1579647868000,Improvement,Resolved,Critical,2,1043,"Externally triggered DagRun marked as 'success' state but all tasks for it are 'None' state Dagrun status showing success for a given execdate but: * all tasks are white in graph/tree view * on cli running airflow task_state for any of the tasks showing in the graph/tree view returns None * taskinstance view shows no tasks for the given dagid This dag has 800 tasks, non_pooled_task_slot_count is 2000, max_tis_per_query is 2000, no pools created/assigned to the dag This is an externally triggered dag, all my dags are purely externally triggered. For execdates 20150101, 20160101, 20170101 getting this issue but 20180101 works. Maybe it has something to do with this line in my dag mentioning 2017 ---> default_args = {'owner': 'airflow','start_date': dt.datetime(2017, 6, 1),'retries': 0,'retry_delay': dt.timedelta(minutes=5),} As per comments below this issue was introduced in 1.10.3. release, i suspect the below jira caused it: * https://github.com/apache/airflow/pull/4808/files (https://issues.apache.org/jira/browse/AIRFLOW-3982)"
321,AIRFLOW-4434,AIRFLOW,1556603185000,1558125061000,Bug,Resolved,Critical,2,486,AIRFLOW-2463 broke support for impala [PR#3405|https://github.com/apache/airflow/pull/3405] added [functionality|https://github.com/apache/airflow/blame/master/airflow/hooks/hive_hooks.py#L800] that made task instance context available for hive queries. Unfortunately this usage of the set command is [not supported|https://impala.apache.org/docs/build/html/topics/impala_set.html] on impala (currently only impala-shell supports set and only for query options not variables like hive).
322,AIRFLOW-4483,AIRFLOW,1557332573000,1620203846000,Bug,Resolved,Critical,2,5818,"sqlalchemy.orm.exc.ObjectDeletedError on webserver startup While running on Flask AppBuilder GUI, occasionally I've been seeing errors like this one on webserver startup, that causes the webserver to crash and become unresponsive: {code:java} | [2019-05-08 16:07:07,087] {security.py:307} INFO - Cleaning faulty perms webserver_1 | [2019-05-08 16:07:07 +0000] [54] [ERROR] Exception in worker process webserver_1 | Traceback (most recent call last): webserver_1 | File ""/usr/local/lib/python3.6/site-packages/gunicorn/arbiter.py"", line 583, in spawn_worker webserver_1 | worker.init_process() webserver_1 | File ""/usr/local/lib/python3.6/site-packages/gunicorn/workers/base.py"", line 129, in init_process webserver_1 | self.load_wsgi() webserver_1 | File ""/usr/local/lib/python3.6/site-packages/gunicorn/workers/base.py"", line 138, in load_wsgi webserver_1 | self.wsgi = self.app.wsgi() webserver_1 | File ""/usr/local/lib/python3.6/site-packages/gunicorn/app/base.py"", line 67, in wsgi webserver_1 | self.callable = self.load() webserver_1 | File ""/usr/local/lib/python3.6/site-packages/gunicorn/app/wsgiapp.py"", line 52, in load webserver_1 | return self.load_wsgiapp() webserver_1 | File ""/usr/local/lib/python3.6/site-packages/gunicorn/app/wsgiapp.py"", line 41, in load_wsgiapp webserver_1 | return util.import_app(self.app_uri) webserver_1 | File ""/usr/local/lib/python3.6/site-packages/gunicorn/util.py"", line 362, in import_app webserver_1 | app = eval(obj, vars(mod)) webserver_1 | File ""<string>"", line 1, in <module> webserver_1 | File ""/usr/local/lib/python3.6/site-packages/airflow/www_rbac/app.py"", line 222, in cached_app webserver_1 | app, _ = create_app(config, session, testing) webserver_1 | File ""/usr/local/lib/python3.6/site-packages/airflow/www_rbac/app.py"", line 182, in create_app webserver_1 | security_manager.sync_roles() webserver_1 | File ""/usr/local/lib/python3.6/site-packages/airflow/www_rbac/security.py"", line 460, in sync_roles webserver_1 | self.clean_perms() webserver_1 | File ""/usr/local/lib/python3.6/site-packages/airflow/www_rbac/security.py"", line 316, in clean_perms webserver_1 | deleted_count = pvms.delete() webserver_1 | File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3440, in delete webserver_1 | delete_op.exec_() webserver_1 | File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py"", line 1635, in exec_ webserver_1 | self._do_pre_synchronize() webserver_1 | File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py"", line 1726, in _do_pre_synchronize webserver_1 | ), obj in query.session.identity_map.items() webserver_1 | File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py"", line 1727, in <listcomp> webserver_1 | if issubclass(cls, target_cls) and eval_condition(obj) webserver_1 | File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/evaluator.py"", line 114, in evaluate webserver_1 | value = sub_evaluate(obj) webserver_1 | File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/evaluator.py"", line 148, in evaluate webserver_1 | return eval_left(obj) == eval_right(obj) webserver_1 | File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/evaluator.py"", line 105, in <lambda> webserver_1 | return lambda obj: get_corresponding_attr(obj) webserver_1 | File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/attributes.py"", line 275, in __get__ webserver_1 | return self.impl.get(instance_state(instance), dict_) webserver_1 | File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/attributes.py"", line 669, in get webserver_1 | value = state._load_expired(state, passive) webserver_1 | File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/state.py"", line 632, in _load_expired webserver_1 | self.manager.deferred_scalar_loader(self, toload) webserver_1 | File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/loading.py"", line 985, in load_scalar_attributes webserver_1 | raise orm_exc.ObjectDeletedError(state) webserver_1 | sqlalchemy.orm.exc.ObjectDeletedError: Instance '<PermissionView at 0x7fc66822d550>' has been deleted, or its row is otherwise not present. webserver_1 | [2019-05-08 16:07:07 +0000] [54] [INFO] Worker exiting (pid: 54) webserver_1 | [2019-05-08 16:07:14,775] {security.py:307} INFO - Cleaning faulty perms webserver_1 | [2019-05-08 16:07:14 +0000] [53] [INFO] Worker exiting (pid: 53) webserver_1 | [2019-05-08 16:07:16,867] {security.py:307} INFO - Cleaning faulty perms webserver_1 | [2019-05-08 16:07:17 +0000] [51] [INFO] Worker exiting (pid: 51) webserver_1 | [2019-05-08 16:07:18,178] {security.py:307} INFO - Cleaning faulty perms webserver_1 | [2019-05-08 16:07:18 +0000] [52] [INFO] Worker exiting (pid: 52) webserver_1 | [2019-05-08 16:07:18,943] {cli.py:826} ERROR - [2 / 2] some workers seem to have died and gunicorndid not restart them as expected webserver_1 | [2019-05-08 16:07:19 +0000] [44] [INFO] Shutting down: Master webserver_1 | [2019-05-08 16:07:19 +0000] [44] [INFO] Reason: Worker failed to boot. {code} This only happen sometimes, and I've seen it happen while running locally via docker-compose and when I deploy onto k8s. It looks like maybe there's a race condition going on here: looking at the code for clean_perms {code:java} def clean_perms(self): """""" FAB leaves faulty permissions that need to be cleaned up """""" self.log.info('Cleaning faulty perms') sesh = self.get_session pvms = ( sesh.query(sqla_models.PermissionView) .filter(or_( sqla_models.PermissionView.permission == None, # NOQA sqla_models.PermissionView.view_menu == None, # NOQA )) ) deleted_count = pvms.delete() sesh.commit() if deleted_count: self.log.info('Deleted %s faulty permissions', deleted_count) {code} I wonder if all the workers are running if one could beat the other worker to pvms.delete()"
323,AIRFLOW-4499,AIRFLOW,1557523531000,1577312715000,Bug,Open,Critical,2,547,"scheduler process running (in ps) but not doing anything, not writing to log for 3+hrs and not processing tasks blogs mention this as long-standing issue but i could not see open JIRA for it. scheduler process running (in ps -ef) but not doing anything, not writing to log for 3+hrs and not processing tasks band-aid solution here: new config value ---> scheduler_restart_mins = x implement auto-restart of scheduler process if scheduler log file not updated within 2*x mins and scheduler process start time is older than x mins env: localexecutor"
324,AIRFLOW-4507,AIRFLOW,1557771134000,1560276561000,Bug,Closed,Critical,2,536,"UI DAG status circle icons not displaying I upgraded our Airflow from 1.10.1 to 1.10.3 and the UI is no longer displaying the status circles on the `/home` page. I've tried viewing these on Ubuntu (Chrome, FireFox) and Windows 10 (Edge) all do not display them. This is running behind nginx (so I hope its not an nginx config issue, if so I will take this issue down) I see Edge and Chrome complain about some fonts not being able to load but HTML inspection makes it look like those status circles are SVG so I'm not sure it's related."
325,AIRFLOW-4524,AIRFLOW,1558011335000,1561647588000,Bug,Resolved,Critical,2,1464,"""Ignore All Deps"" and ""Ignore Task Deps"" doesn't ignore task dependencies After an upgrade from Airflow 1.10.1->1.10.3, we're seeing this behavior when trying to ""Run"" a task in the UI with ""Ignore All Deps"" and ""Ignore Task Deps"": {code:java} ""Could not queue task instance for execution, dependencies not met: Trigger Rule: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_tasks_state={'successes': 1, 'skipped': 0, 'upstream_failed': 0, 'failed': 0, 'done': 1, 'total': 2}, upstream_task_ids={'XXX', 'XXX'}"" {code} I suspected this was due to the checkbox not getting applied so I investigated this in Chrome. Here's a 1.10.3 POST (broken): {code:java} dag_id: MY_DAG_ID task_id: MY_TASK_ID execution_date: 2019-05-15T00:00:00+00:00 origin: MY_AIRFLOW_SERVER ignore_all_deps: ignore_ti_state: ignore_task_deps: ignore_all_deps: true ignore_task_deps: true downstream: true recursive: true {code} And here's a 1.10.1 POST (working): {code:java} task_id: MY_TASK_ID dag_id: MY_DAG_ID ignore_all_deps: true ignore_task_deps: true ignore_ti_state: false execution_date: 2019-05-16T11:00:00+00:00 origin: MY_AIRFLOW_SERVER {code} It looks like the JS here is doing something wonky and sending blank values for unchecked boxes, as well as checkboxes that don't apply to the current action, which is presumably breaking parsing when the server receives a json object with multiple conflicting keys."
326,AIRFLOW-4576,AIRFLOW,1558987762000,1620203565000,Bug,Resolved,Critical,2,1089,"Rendered Template & email_on_failure displays password variable in clear text I have a DAG with a SSHOperator, which uses a ssh_conn_id to run the below command. As shown below, I am using Airflow Variables to pass credentials to the script that needs to run. *tac_job_run_command = ""\{{ var.value.tac_metaservlet_path }}/MetaServletAirflowCaller.sh --tac-url=http://\{{ var.value.tac_server_ip }}:8080/tac/ --json-params='\{\""authPass\"":\""{{ var.value.tac_tadmin_password }}\"",\""authUser\"":\""tadmin@abc.com\"",\""taskId\"":\{{ ti.xcom_pull(\""get_tac_job_id\"")[0] }}}' ""* The password variable (tac_tadmin_password), in the UI's variables screen shows as ***** and all works good, but once the job has run, the SSHOperator task's Rendered Template section displays the command with the variable values and also displays the password (tac_tadmin_password) in clear text. Is there any way we can avoid this or is this an issue that needs to be fixed? If the DAG fails, I have email_on_failure set to True, and the email also ends up displaying the rendered template with password in clear text."
327,AIRFLOW-4908,AIRFLOW,1562555639000,1571312649000,New Feature,Resolved,Critical,2,794,"Implement BigQuery Hooks/Operators for update_dataset, patch_dataset and get_dataset To create a BigQuery sink for GCP Stackdriver Logging, I have to assign `WRITER` access to group `cloud-logs@google.com` to access BQ dataset. However, current BigQueryHook doesn't support updating/patching dataset. Reference: [https://googleapis.github.io/google-cloud-python/latest/logging/usage.html#export-to-bigquery] Implement GCP Stackdriver Logging: https://issues.apache.org/jira/browse/AIRFLOW-4779 While it is missing update_dataset and patch_dataset, BigQueryHook has get_dataset but it doesn't have operator for it. Features to be implemented: BigQueryBaseCursor.patch_dataset BigQueryBaseCursor.update_dataset BigQueryPatchDatasetOperator BigQueryUpdateDatasetOperator BigQueryGetDatasetOperator"
328,AIRFLOW-4931,AIRFLOW,1562850925000,1574802295000,Improvement,Resolved,Critical,2,236,Add KMS Encryption Configuration to BigQuery Hook and Operators One of the clients requires adding KMS encryption on BigQuery tables. Reference: [https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration]
329,AIRFLOW-5017,AIRFLOW,1563810316000,1563810717000,Bug,Open,Critical,2,1636,"Cannot Parse Environment Variable Connection for Spark on K8s Currently, if you create a Spark on Kubernetes based URL [connection with environment variables|https://airflow.apache.org/howto/connection/index.html#creating-a-connection-with-environment-variables] for the Spark Submit operator, urllib is unable to parse it correctly due to the double scheme that Spark expects (k8s://https://<k8s cluster hostname>:<k8s cluster port>). To test this, first set an environment variable for the connection: {code:java} $ export AIRFLOW_CONN_SPARK_K8S=k8s://https://localhost:8080{code} Add the following example DAG: {code:java} from datetime import datetime, timedelta from airflow import DAG from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2019, 7, 19), 'retries': 0 } dag = DAG('spark_submit_k8s', default_args=default_args, schedule_interval=timedelta(days=1)) run_example_jar = SparkSubmitOperator( task_id='spark_submit_example', application='file:///usr/local/spark-2.4.2/examples/jars/spark-examples_2.11- 2.4.2.jar', java_class='org.apache.spark.examples.SparkPi', conn_id='spark_k8s', dag=dag ) {code} This fails to parse as per the logs: {code:java} [2019-07-22 15:05:58,925] {logging_mixin.py:95} INFO - [2019-07-22 15:05:58,925] {base_hook.py:83} INFO - Using connection to: id: spark_k8s. Host: https, Port: None, Schema: /localhost:8080, Login: None, Password: None, extra: {} {code} Because of the preceding ""k8s://"" urllib fails to parse this correctly. This consequently will cause the DAG task to fail."
330,AIRFLOW-5025,AIRFLOW,1563863042000,1577812914000,Improvement,Resolved,Critical,2,393,DataflowPythonOperator uses Python2 `DataflowPythonOperator` explicitly uses Python2 as below: [https://github.com/apache/airflow/blob/57924c2a60c4b8a837114d9c8f120f3817f4e97b/airflow/contrib/hooks/gcp_dataflow_hook.py#L394-L395] though we can use Python3 for Apache Beam and Google Cloud Dataflow. I think it's better that we can choose Python2 or 3. I can create pull request if it's needed.
331,AIRFLOW-5039,AIRFLOW,1563998177000,1590550918000,Bug,In Progress,Critical,2,2152,"xcom_push execution_date setting is broken when core.enable_xcom_pickling=False Function [xcom_push from airflow.models.taskinstance automatically sets the execution_date to XCOM|https://github.com/apache/airflow/blob/7f66d0a6dc13a790b4d2d31694cb50737983f82e/airflow/models/taskinstance.py#L1336], while this is fine when pickling is enabled it just won't work when it isn't as [XCOM will attempt to JSON serialize a `datetime` object|https://github.com/apache/airflow/blob/7f66d0a6dc13a790b4d2d31694cb50737983f82e/airflow/models/xcom.py#L229], which is unsupported. This issue is critical as having pickle enabled poses security risks and [in environments such as Google Cloud Composer pickling is disabled for good|[https://cloud.google.com/composer/docs/concepts/airflow-configurations]]. Attempting to run DAGs with tasks which pushes content to XCOM when `core-enable_xcom_pickling=False` in `airflow.cfg` will result in the following stacktrace (tested in version 1.10.2-composer): {quote} TypeError: Object of type 'datetime' is not JSON serializable [2019-07-24 19:14:53,193] \{models.py:1796} ERROR - Object of type 'datetime' is not JSON serializable Traceback (most recent call last): File ""/usr/local/lib/airflow/airflow/models.py"", line 1668, in _run_raw_task self.xcom_push(key=XCOM_RETURN_KEY, value=result) File ""/usr/local/lib/airflow/airflow/models.py"", line 2063, in xcom_push execution_date=execution_date or self.execution_date) File ""/usr/local/lib/airflow/airflow/utils/db.py"", line 73, in wrapper return func(*args, **kwargs) File ""/usr/local/lib/airflow/airflow/models.py"", line 4785, in set value = json.dumps(value).encode('UTF-8') File ""/opt/python3.6/lib/python3.6/json/__init__.py"", line 231, in dumps return _default_encoder.encode(obj) File ""/opt/python3.6/lib/python3.6/json/encoder.py"", line 199, in encode chunks = self.iterencode(o, _one_shot=True) File ""/opt/python3.6/lib/python3.6/json/encoder.py"", line 257, in iterencode return _iterencode(o, 0) File ""/opt/python3.6/lib/python3.6/json/encoder.py"", line 180, in default o.__class__.__name__) TypeError: Object of type 'datetime' is not JSON serializable {quote}"
332,AIRFLOW-5062,AIRFLOW,1564345605000,1584352943000,New Feature,Closed,Critical,2,94,Allow ACL header in S3 Hook Definition of Done * Add ACL field in S3Hook when uploading a file
333,AIRFLOW-5071,AIRFLOW,1564487722000,1624050428000,Bug,Open,Critical,2,864,Thousand os Executor reports task instance X finished (success) although the task says its queued. Was the task killed externally? I'm opening this issue because since I update to 1.10.3 I'm seeing thousands of daily messages like the following in the logs: ``` {{__init__.py:1580}} ERROR - Executor reports task instance <TaskInstance: X 2019-07-29 00:00:00+00:00 [queued]> finished (success) although the task says its queued. Was the task killed externally? {{jobs.py:1484}} ERROR - Executor reports task instance <TaskInstance: X 2019-07-29 00:00:00+00:00 [queued]> finished (success) although the task says its queued. Was the task killed externally? ``` -And looks like this is triggering also thousand of daily emails because the flag to send email in case of failure is set to True.- I have Airflow setup to use Celery and Redis as a backend queue service.
334,AIRFLOW-5168,AIRFLOW,1565606381000,1566994444000,Bug,Resolved,Critical,2,860,"Dataproc operators fail on 1.10.4 We've found a problem with DataprocOperators introduced in 1.10.4. There is a commit to master which adds label support, this has a change in the hook & base operator: https://github.com/apache/airflow/commit/4d58f36df3b533edfce873799dc161ee34571d63#diff-bc55ad8b749b7a136f51ffabfdbaf13dL684 Then in the 1.10.4 branch there is a massive commit for GCP DLP: https://github.com/apache/airflow/commit/222c6ac45de54ed0398645c1d456a592e194325b This seems to have leaked the operator part of the label change only: https://github.com/apache/airflow/commit/222c6ac45de54ed0398645c1d456a592e194325b#diff-bc55ad8b749b7a136f51ffabfdbaf13dR693 This then manifests as this error when trying to execute a Dataproc job: {{[2019-08-09 14:32:02,971] \{taskinstance.py:1047} ERROR - _DataProcJobBuilder instance has no attribute 'add_labels'}}"
335,AIRFLOW-5181,AIRFLOW,1565674041000,1565674041000,Bug,Open,Critical,2,583,Connection reset by peer during Dataproc job Dataproc job fails intermittently saying `Connection reset by peer` This is because of a stale connection in httplib2 cache that might cause this issue. This issue happens intermittently and it is still an open issue in httplib2. There have been suggestions to not use the httplib2 library and use httplib2shim. Link to conversation: https://github.com/googleapis/google-cloud-python/issues/2028 Link to open issue on httplib2: https://github.com/httplib2/httplib2/issues/40 Should we also remove httplib2 dependency and use httplib2shim?
336,AIRFLOW-5214,AIRFLOW,1565794089000,1570799113000,Bug,Open,Critical,2,1174,"Airflow leaves too many TIME_WAIT TCP connections Dear experts, in Airflow version 1.10.2 as well as 1.10.4, we experience a severe problem with the limitation of the number of concurrent tasks. We observe that for more than 8 tasks being started and executed in parallel, that the majority of those tasks fails with the error ""Can't connect to MySQL server"" and error code 2006(99). This error code boils down to ""Cannot bind socket to resource"", which is why we started looking into the TCP conenctions of our Airflow host (a single node that hosts the webserver, scheduler and worker). When the 8 tasks are simultaneously running, we observe more than 15,000 TIME_WAIT connections while less than 50 are established. Given, that the number of available ports is somewhat smaller than 30,000, this large number of blocked but unused TCP connections would explain the failing of further task executions. Can anyone explain how these many open connections blocking ports/sockets come about? Given that we have connection pooling enabled, we do not see any explanation yet. Your help is very much appreciated, this issue strongly limits our current performance! Cheers Oliver"
337,AIRFLOW-5272,AIRFLOW,1566375535000,1620203806000,Bug,Resolved,Critical,2,789,"Flower is not working with Redis Sentinel Flower is hang and not accessible via link (HTTP TIMEOUT) if broker url use Redis Sentinel. Scheduler and Worker are working fine. No Flower error message displayed in log. After checking code, I can see {color:#000000}broker_transport_options is not passed to Flower, master_name is None. {color} related issue: AIRFLOW-966 ======= config ======== {color:#000000}[celery]{color} {color:#0000ff}broker_url{color}{color:#000000} = sentinel://192.168.1.1:26379{color}{color:#008000};sentinel://192.168.1.2:26379;sentinel://192.168.1.3:26379;sentinel://192.168.1.4:26379{color} {color:#000000}[celery_broker_transport_options]{color} {color:#008000}#visibility_timeout = 21600{color} {color:#0000ff}master_name{color}{color:#000000} = mymaster{color}"
338,AIRFLOW-5283,AIRFLOW,1566460587000,1620203785000,Bug,Closed,Critical,2,1255,"Separate scheduling jobs from executing jobs Currently, Airflow does not schedule new jobs if the number of active runs > `max_active_runs`, [see this|[https://github.com/apache/airflow/blob/d760d63e1a141a43a4a43daee9abd54cf11c894b/airflow/jobs.py#L768]] for Airflow 1.9, Airflow 1.10 behaves the same. A result of this is that if a DAG (incidentally) runs longer than the time between scheduled DAG runs, some runs will be missed because the next DAG run being scheduled is first planned one after the one that ran longer. For example, imagine DAG runs every hour and the DAG run of 02:00 takes (for some reason) 2 hours 45 minutes to complete instead of the usual 15 minutes. And the `max_active_runs` is set to 1. This would mean that: * The DAG run of 02:00 is finished at 04:45 * The DAG runs of 03:00 and 04:00 are not scheduled because there is already a DAG active and `max_active_runs` is set to 1 * The next DAG run scheduled will be the one from 05:00 * The DAG runs of 03:00 and 04:00 are never scheduled. The problem is that scheduling and execution of DAG runs are now both tight to the `max_active_runs` setting. This should be separated so that jobs are scheduled at all planned times, but only `max_active_runs` are executed concurrently."
339,AIRFLOW-5335,AIRFLOW,1567009395000,1567180277000,Improvement,Resolved,Critical,2,328,"Update GCSHook methods so they need min IAM perms After we refactored to using Storage client in GCS, we need more IAM permissions. This is because we use `get_bucket` method which requires `storage.bucket.list` permission. Instead of that if we use `bucket` method that creates Bucket object we don't need the above permission."
340,AIRFLOW-5342,AIRFLOW,1567068672000,1570014502000,Bug,Resolved,Critical,2,642,"Can't run initdb using Microsoft SQL server I'm using Microsoft SQL server 2016 as metadata db (for the sql_alchemy_conn), and pymssql as the Python driver. When running airflow initdb, airflow creates a table named `TaskInstance` with column `pool` that is initially Nullable (when the table is created). Later, an index named `ti_pool` on a few columns, including `pool` column in table `TaskInstance`. Then, airflow will try to alter table `TaskInstance` and change the column `pool` to `NOT NULL`. This does not work on Microsoft SQL Server since a column with an index defined on it cannot be changed, unless the index is deleted before."
341,AIRFLOW-5350,AIRFLOW,1567111071000,1567194538000,Bug,Resolved,Critical,2,2379,"Fix bug in the num_retires field in BigQueryHook The `num_retries` extra is no set in old connections that were created before 1.10.4, for those fields it's value is None which causes the below error: From the StackOverflow Post: {noformat} [2019-08-27 02:49:58,076] {cli.py:516} INFO - Running <TaskInstance: cadastro_remessas_paises2.gcs_to_bq 2019-08-27T02:42:43.970619+00:00 [running]> on host cadastroremessaspaises2gcstobq-78f1ea099c3b4e718ba707cb03ffda1e [2019-08-27 02:49:58,136] {logging_mixin.py:95} INFO - [[34m2019-08-27 02:49:58,136[0m] {[34mdiscovery.py:[0m271} INFO[0m - URL being requested: GET [1mhttps://www.googleapis.com/discovery/v1/apis/bigquery/v2/rest[0m[0m [2019-08-27 02:49:59,259] {logging_mixin.py:95} INFO - [[34m2019-08-27 02:49:59,259[0m] {[34mmy_functions_google.py:[0m2224} INFO[0m - Project not included in [1mdestination_project_dataset_table[0m: [1mcadastro_remessas.paises2[0m; using project ""[1mbigdata-staging[0m""[0m [2019-08-27 02:49:59,266] {logging_mixin.py:95} INFO - [[34m2019-08-27 02:49:59,266[0m] {[34mdiscovery.py:[0m867} INFO[0m - URL being requested: POST https://www.googleapis.com/bigquery/v2/projects/bigdata-staging/jobs?alt=json[0m [2019-08-27 02:49:59,266] {taskinstance.py:1047} ERROR - unsupported operand type(s) for +: 'NoneType' and 'int' Traceback (most recent call last): File ""/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 922, in _run_raw_task result = task_copy.execute(context=context) File ""/airflow/dags/git/subfolder/my_functions_google.py"", line 2502, in execute cluster_fields=self.cluster_fields) File ""/airflow/dags/git/subfolder/my_functions_google.py"", line 1396, in run_load return self.run_with_configuration(configuration) File ""/airflow/dags/git/subfolder/my_functions_google.py"", line 1414, in run_with_configuration .execute(num_retries=self.num_retries) File ""/usr/local/lib/python3.7/site-packages/googleapiclient/_helpers.py"", line 130, in positional_wrapper return wrapped(*args, **kwargs) File ""/usr/local/lib/python3.7/site-packages/googleapiclient/http.py"", line 851, in execute method=str(self.method), body=self.body, headers=self.headers) File ""/usr/local/lib/python3.7/site-packages/googleapiclient/http.py"", line 153, in _retry_request for retry_num in range(num_retries + 1): TypeError: unsupported operand type(s) for +: 'NoneType' and 'int' {noformat}"
342,AIRFLOW-5500,AIRFLOW,1568633712000,1597836644000,Bug,Resolved,Critical,2,589,"Bug in trigger api endpoint Unable to trigger workflow with nested sub dags, getting following error: sqlalchemy.exc.IntegrityError: (psycopg2.IntegrityError) duplicate key value (dag_id,execution_date)=('dummy.task1.task_level1.task_level2','2019-09-10 13:00:27+00:00') violates unique constraint ""dag_run_dag_id_execution_date_key"" trigger_dag for nested sub_dags is called twice. fix: in airflow/api/common/experimental/trigger_dag.py - while populating subdags for a dag, each subdag's subdags is also populated to main dag. So no need to repopulate subdags for each subdag separately."
343,AIRFLOW-5528,AIRFLOW,1569013695000,1569441081000,Improvement,Resolved,Critical,2,111,end of log mark in broken in es_task_handler introduced in: [https://github.com/apache/airflow/pull/5048/files]
344,AIRFLOW-5545,AIRFLOW,1569286306000,1601411236000,Bug,Resolved,Critical,2,364,"dag.test_cycle uses recursion, limiting DAG length. DAG Parsing is currently breaking for long DAGs because test_cycle uses recursive DFS Example breaking DAG {code:java} dag = DAG( 'dag', start_date=DEFAULT_DATE, default_args={'owner': 'owner1'}) with dag: last = dag for i in range(1,10000): op = DummyOperator(task_id='{}'.format(i)) last >> op last = op {code}"
345,AIRFLOW-5571,AIRFLOW,1569762311000,1579906620000,Bug,Open,Critical,2,5131,"Kubernetes operator's bug that get logs will make pod exit unexpectedly In my project, I manage 200+ jobs in kubernetes, as I work on, I found a critical bug for kubernetes operator. In pod_launcher.py, *_monitor_pod* function: {code:python} if get_logs: logs = self.read_pod_logs(pod) # Here has a retry. for line in logs: # But exception throw from here! self. {code} in above code, *logs* is a HttpResponse, as it implemented __iter__() function, you can use for loop to print the lines. In the other words, here use a http long connection to get endless log. There is only try catch over *self.read_pod_logs*, however, If the network is disconnected or jitter occurs, for loop will throw error. As I have 200+ job run everyday, I can get 4~5 errors everyday, and each error will let monitor think the pod is down, and then mark the task as failed, then retry it. This eventually lead to data error. Below is a typical error log: {code:java} [2019-09-17 20:50:02,532] {logging_mixin.py:95} INFO - [2019-09-17 20:50:02,532] {pod_launcher.py:105} INFO - b'19/09/17 20:50:00 INFO yarn.Client: Application report for application_1565926539066_3866 (state: RUNNING)\n' [2019-09-17 20:50:02,532] {logging_mixin.py:95} INFO - [2019-09-17 20:50:02,532] {pod_launcher.py:105} INFO - b'19/09/17 20:50:01 INFO yarn.Client: Application report for application_1565926539066_3866 (state: RUNNING)\n' [2019-09-17 20:50:02,533] {taskinstance.py:1047} ERROR - ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read)) Traceback (most recent call last): File ""/usr/local/lib/python3.6/dist-packages/urllib3/response.py"", line 639, in _update_chunk_length self.chunk_left = int(line, 16) ValueError: invalid literal for int() with base 16: b'' During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/usr/local/lib/python3.6/dist-packages/urllib3/response.py"", line 397, in _error_catcher yield File ""/usr/local/lib/python3.6/dist-packages/urllib3/response.py"", line 704, in read_chunked self._update_chunk_length() File ""/usr/local/lib/python3.6/dist-packages/urllib3/response.py"", line 643, in _update_chunk_length raise httplib.IncompleteRead(line) http.client.IncompleteRead: IncompleteRead(0 bytes read) During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/usr/local/lib/python3.6/dist-packages/airflow/models/taskinstance.py"", line 922, in _run_raw_task result = task_copy.execute(context=context) File ""/usr/local/lib/python3.6/dist-packages/airflow/contrib/operators/k8s_pod_operator.py"", line 45, in execute super().execute(context) File ""/usr/local/lib/python3.6/dist-packages/airflow/contrib/operators/kubernetes_pod_operator.py"", line 148, in execute get_logs=self.get_logs) File ""/usr/local/lib/python3.6/dist-packages/airflow/contrib/kubernetes/pod_launcher.py"", line 97, in run_pod return self._monitor_pod(pod, get_logs) File ""/usr/local/lib/python3.6/dist-packages/airflow/contrib/kubernetes/pod_launcher.py"", line 104, in _monitor_pod for line in logs: File ""/usr/local/lib/python3.6/dist-packages/urllib3/response.py"", line 747, in __iter__ for chunk in self.stream(decode_content=True): File ""/usr/local/lib/python3.6/dist-packages/urllib3/response.py"", line 527, in stream for line in self.read_chunked(amt, decode_content=decode_content): File ""/usr/local/lib/python3.6/dist-packages/urllib3/response.py"", line 732, in read_chunked self._original_response.close() File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__ self.gen.throw(type, value, traceback) File ""/usr/local/lib/python3.6/dist-packages/urllib3/response.py"", line 415, in _error_catcher raise ProtocolError('Connection broken: %r' % e, e) urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read)) [2019-09-17 20:50:02,536] {taskinstance.py:1070} INFO - Marking task as UP_FOR_RETRY [2019-09-17 20:50:02,717] {logging_mixin.py:95} INFO - [2019-09-17 20:50:02,717] {email.py:126} INFO - Sent an alert email to ['data_alert@sensetime.com', 'zhaoyun@sensetime.com'] [2019-09-17 20:50:03,921] {base_task_runner.py:115} INFO - Job 16307: Subtask ods_senseid_log_ingestion [2019-09-17 20:50:03,921] {cli_action_loggers.py:82} DEBUG - Calling callbacks: [] [2019-09-17 20:50:03,925] {base_task_runner.py:115} INFO - Job 16307: Subtask ods_senseid_ {code} I have add a try-catch over for log loop, and it works fine. Below is trace for exception catch: {code} ProtocolError('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))ods-senseid-img-identity-liven-ods-senseid-img-identity-liven-83570f1fGetting log error, stop getting log. ProtocolError('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))ods-senseid-idnumber-log-shang-ods-senseid-identity-idnumber--885013e7Getting log error, stop getting log. ProtocolError('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))ods-senseid-idnumber-log-shang-ods-senseid-identity-idnumber--9627184eGetting log error, stop getting log. {code}"
346,AIRFLOW-5633,AIRFLOW,1570730062000,1570782770000,New Feature,Open,Critical,2,1217,"Apache Airflow Integration with HashiCorp Nomad Nomad is an orchestrator, similar to Kubernetes and used in production by companies like PagerDuty, Target, Citadel, Trivago, Pandora, Roblox, etc. Built by HashiCorp (company behind other tools such as Terraform, Vault, and Consul). We've gotten many requests from our customers and open source community for an integration with Airflow. There are a variety of community PRs from supportive members who are trying to build their own (linked notable ones below) - our approach would be to build one officially supported integration ourselves for all our users and customers. [https://github.com/apache/airflow/pull/2708] [https://github.com/hashicorp/nomad/issues/419] We'd love to understand how much interest and support we can gauge from the Airflow committee around an integration like this. Ideally, we'd aim to avoid a situation where our integration is not accepted + Nomad users using Airflow are subject to an incomplete/less-than-ideal experience on a lagging HashiCorp-maintained-fork. This has unfortunately happened with our Spark integration, so we'd love to gauge the Airflow committee early-on in the process before we potentially build the integration."
347,AIRFLOW-5641,AIRFLOW,1570839846000,1593118229000,Bug,Resolved,Critical,2,322,"Not able to run git sync init container as root We used to be able to run git sync init container as root user. A recent refactoring of the code base (AIRFLOW-4851) introduced a regression that makes it impossible to run git sync container as root. As a result, we are not able to start any task using kubernetes executor."
348,AIRFLOW-5685,AIRFLOW,1571274082000,1576615837000,Bug,Resolved,Critical,2,800,"Loading AVRO file from GCS to BQ throwing ValueError Using {{GoogleCloudStorageToBigQueryOperator to load AVRO file is causing exception as follows:}} {{Traceback (most recent call last): File ""/Users/ryanyuan/dev/airflow/airflow/models/taskinstance.py"", line 932, in _run_raw_task result = task_copy.execute(context=context) File ""/Users/ryanyuan/dev/airflow/airflow/operators/gcs_to_bq.py"", line 293, in execute encryption_configuration=self.encryption_configuration) File ""/Users/ryanyuan/dev/airflow/airflow/gcp/hooks/bigquery.py"", line 1290, in run_load backward_compatibility_configs) File ""/Users/ryanyuan/dev/airflow/airflow/gcp/hooks/bigquery.py"", line 2374, in _validate_src_fmt_configs .format(k, source_format))*ValueError: skipLeadingRows is not a valid src_fmt_configs for type AVRO.*}}"
349,AIRFLOW-5686,AIRFLOW,1571307047000,1591864868000,Bug,Open,Critical,2,728,"Deleting DAG can leave scheduled/queued tasks consuming pool slots If you delete a dag file when it had tasks in the scheduled or queued state, those tasks instances are never touched again. With the slight tweak in Pool (making the default pool an explicit one) this now matters, and this ends up with the scheduler being ""blocked"" from running new tasks wiht this message: {noformat} Figuring out tasks to run in Pool(name=default_pool) with -9022 open slots and 45 task instances ready to be queued {noformat} The fix should be to set any task instance in a non-terminal state (None, queued, scheduled, running, up_for_retry etc.) to ""removed"" inside DAG.deactivate_stale_dags (which is already called on scheduler shutdown)."
350,AIRFLOW-5700,AIRFLOW,1571447698000,1571449811000,New Feature,Closed,Critical,2,1284,"KubeExecutor - Specify YAML template As a developer - I find myself struggling with the Airflow implementation of launching pods within the KubeExecutor. Things are implemented in ways that are difficult to get working with my kubernetes implementation, are difficult because certain features of the Kubernetes Spec aren't implemented in a way that is configurable, or features are forced to be implemented in a certain way because of design decisions in the airflow wrappers around around the kubernetes client. I believe supporting a YAML template would solve a lot of problems and reduce the load on airflow maintainers for feature requests around supporting more of the Kubernetes API Dask has good Kubernetes support. They provide a hook to define a pod template via YAML, OR to use defaults a thin abstraction via `make_pod_spec` You can see an example here: https://kubernetes.dask.org/en/latest/#quickstart and source here: https://github.com/dask/dask-kubernetes/blob/master/dask_kubernetes/kubernetes.yaml#L35-L64 https://github.com/dask/dask-kubernetes/blob/master/dask_kubernetes/objects.py#L100-L163 https://github.com/dask/dask-kubernetes/blob/master/dask_kubernetes/core.py#L279-L291 https://github.com/dask/dask-kubernetes/blob/master/dask_kubernetes/core.py#L531-L566"
351,AIRFLOW-5822,AIRFLOW,1572518634000,1620203598000,Bug,Resolved,Critical,2,249,"CSRF_ENABLED was changed to WTF_CSRF_ENABLED Variable CSRF_ENABLED was renamed to WTF_CSRF_ENABLED, but not changed in webserver_config_template. And we need to change it from environment variables, anything like conf.get('csrf', 'WTF_CSRF_ENABLED')"
352,AIRFLOW-5871,AIRFLOW,1573184714000,1573229422000,Bug,Open,Critical,2,608,"Stopping/Clearing a running airflow instance doesnt't terminate the job actually. Hi Team, When I change the state of a running job instance to cleared/failed, it doesn't completely terminate the existing job. I tried using pythonOperator, bashOperator. The job connects to oracle and executes a package. Even terminating/killing airflow job process won't terminate the oracle sessions. This is an issue as whenever we would need to compile the package we would have to stop the dags, marks existing dag runs to clear state and then kill the oracle sessions. Is there a way to clean stop dag runs in airflow."
353,AIRFLOW-5881,AIRFLOW,1573247596000,1612430485000,Bug,Open,Critical,2,939,"Dag gets stuck in ""Scheduled"" State when scheduling a large number of tasks Running with the KubernetesExecutor in and AKS cluster, when we upgraded to version 1.10.6 we noticed that the all the Dags stop making progress but start running and immediate exiting with the following message: ""Instance State' FAILED: Task is in the 'scheduled' state which is not a valid state for execution. The task must be cleared in order to be run."" See attached log file for the worker. Nothing seems out of the ordinary in the Scheduler log. Reverting to 1.10.5 clears the problem. Note that at the time of the failure maybe 100 or so tasks are in this state, with 70 coming from one highly parallelized dag. Clearing the scheduled tasks just makes them reappear shortly thereafter. Marking them ""up_for_retry"" results in one being executed but then the system is stuck in the original zombie state. Attached is the also a redacted airflow config flag."
354,AIRFLOW-5938,AIRFLOW,1573819048000,1620203761000,Task,Closed,Critical,2,346,How to start airflow with kubernetes [https://airflow.readthedocs.io/en/stable/kubernetes.html] I am following this tut and i changed executor as kubernetes in airflow setting file. then how to run this and if i do airflow scheduler it gives me an error which is - kubernetes.config.config_exception.ConfigException: Service host/port is not set.
355,AIRFLOW-5948,AIRFLOW,1573867078000,1599148714000,Improvement,Resolved,Critical,2,396,"Replace SimpleDag with serialized version Replace SimpleDag with serialized version (json over multiprocessing) in SchedulerJob etc., no other change in scheduler behaviour. (This doesn't make sense long term, but does tidy up the code) Currently, we have 2 Serialized Representation: # SimpleDags (were created because SimpleDags were not pickleable) # Serialized DAG We should remove SimpleDags"
356,AIRFLOW-6124,AIRFLOW,1575034100000,1575034302000,Bug,Open,Critical,2,3771,"tmp folder gets deleted, shell-init: error retrieving current directory My dag has three SparkSubmitOperators and throws this error occasionally after executing one task. If I trigger the job manually by ignoring all dependencies, it works well. Most likely /tmp folder is getting deleted and it cannot find it again {{```[2019-11-29 12:58:54,548] \{logging_mixin.py:112} INFO - [2019-11-29 12:58:54,547] \{base_hook.py:84} INFO - Using connection to: id: spark_default. Host: yarn, Port: None, Schema: None, Login: None, Password: None, extra: \{'deploy-mode': 'cluster', 'spark-binary': '/usr/bin/spark2-submit/'} [2019-11-29 12:58:54,548] \{logging_mixin.py:112} INFO - [2019-11-29 12:58:54,548] \{spark_submit_hook.py:302} INFO - Spark-Submit cmd: ['spark-submit', '--master', 'yarn', '--conf', 'spark.yarn.maxAppAttempts=2', '--files', '/home/centos/airflow/dags/conf/aws_keys.pickle', '--num-executors', '4', '--executor-cores', '4', '--executor-memory', '20G', '--name', 'universal_cross_matrix_joins_AUS_27-11-2019', '--deploy-mode', 'cluster', '/home/centos/airflow/scripts/near-airflow-repo/scripts/universal_cross_matrix/join_ifa_ncid.py', '--input_date', '27-11-2019', '--country', 'AUS', '--tenant_id', '63ce7457'] [2019-11-29 12:58:54,553] \{logging_mixin.py:112} INFO - [2019-11-29 12:58:54,553] \{spark_submit_hook.py:433} INFO - shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [2019-11-29 12:58:54,555] \{logging_mixin.py:112} INFO - [2019-11-29 12:58:54,555] \{spark_submit_hook.py:433} INFO - shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [2019-11-29 12:58:54,557] \{logging_mixin.py:112} INFO - [2019-11-29 12:58:54,557] \{spark_submit_hook.py:433} INFO - shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [2019-11-29 12:58:54,560] \{logging_mixin.py:112} INFO - [2019-11-29 12:58:54,560] \{spark_submit_hook.py:433} INFO - chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [2019-11-29 12:58:54,563] \{logging_mixin.py:112} INFO - [2019-11-29 12:58:54,563] \{spark_submit_hook.py:433} INFO - chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [2019-11-29 12:58:54,565] \{logging_mixin.py:112} INFO - [2019-11-29 12:58:54,564] \{spark_submit_hook.py:433} INFO - chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [2019-11-29 12:58:54,567] \{logging_mixin.py:112} INFO - [2019-11-29 12:58:54,567] \{spark_submit_hook.py:433} INFO - shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [2019-11-29 12:58:54,569] \{logging_mixin.py:112} INFO - [2019-11-29 12:58:54,569] \{spark_submit_hook.py:433} INFO - chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [2019-11-29 12:58:54,571] \{logging_mixin.py:112} INFO - [2019-11-29 12:58:54,571] \{spark_submit_hook.py:433} INFO - chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [2019-11-29 12:58:54,574] \{logging_mixin.py:112} INFO - [2019-11-29 12:58:54,574] \{spark_submit_hook.py:433} INFO - chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [2019-11-29 12:58:54,576] \{logging_mixin.py:112} INFO - [2019-11-29 12:58:54,576] \{spark_submit_hook.py:433} INFO - chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory```}}"
357,AIRFLOW-6159,AIRFLOW,1575327366000,1575935744000,Improvement,Resolved,Critical,2,208,"Change logging level of the heartbeat message to DEBUG As mentioned in https://stackoverflow.com/questions/59142508/airflow-disable-heartbeat-logs , the logs containing heartbeat details can become too noisy."
358,AIRFLOW-6207,AIRFLOW,1575923665000,1586474733000,Bug,Open,Critical,2,824,"Dag run twice in airflow [!https://user-images.githubusercontent.com/26398961/70459068-67356780-1ad9-11ea-86a8-44d1bbfb574d.png!|https://user-images.githubusercontent.com/26398961/70459068-67356780-1ad9-11ea-86a8-44d1bbfb574d.png] As you can see the dag xyz_dag run twice a day and XYZ dag is the scheduled dag, not manual trigger for the first run it runs at 5:10:16 and the second run it ran at 5:10:58 similarly this behavior has been observed for my all 400 dags. I don't know why this behavior has occurred and I don't know how this can be solved any help will be appreciated. below I have shared xyx_dag.py file. [!https://user-images.githubusercontent.com/26398961/70460387-30ad1c00-1adc-11ea-8241-ce3dd6d7556c.png!|https://user-images.githubusercontent.com/26398961/70460387-30ad1c00-1adc-11ea-8241-ce3dd6d7556c.png]"
359,AIRFLOW-6385,AIRFLOW,1577571955000,1578695321000,Bug,Resolved,Critical,2,971,"SlackAPIPostOperator fails when blocks not set After upgrade to 1.10.7 all slack notifications became faulty SlackAPIPostOperator fails when blocks not set {code} t_slack_ok = SlackAPIPostOperator( task_id = 'notify_ok', token = Variable.get('slack.token'), channel = Variable.get('slack.channel'), text = ':rocket: {{ dag.dag_id }} for {{ ds }} has succeeded', dag = dag) {code} Causing an operator error: {code} [2019-12-28 21:55:14,668] {taskinstance.py:1088} ERROR - Slack API call failed (invalid_blocks_format) Traceback (most recent call last): File ""/usr/local/lib/python3.6/dist-packages/airflow/models/taskinstance.py"", line 955, in _run_raw_task result = task_copy.execute(context=context) File ""/usr/local/lib/python3.6/dist-packages/airflow/operators/slack_operator.py"", line 85, in execute slack.call(self.method, self.api_params) File ""/usr/local/lib/python3.6/dist-packages/airflow/hooks/slack_hook.py"", line 62, in call raise AirflowException(msg) {code}"
360,AIRFLOW-6402,AIRFLOW,1577707430000,1620203497000,Bug,Resolved,Critical,2,769,"SSH operator exception - 'NoneType' object has no attribute 'startswith' Hi I found a bug in the constructor off the new airflow (1.10.7). It's affects me when I am implementing my own operator (I'ts inherit from SshOperator) The default value for the ""self.command"" variable is None, while this line of code cant run because of the default value: {code:java} self.get_pty = self.command.startswith('sudo') or get_pty {code} My solution right now (In my plugin) is to send value of empty string to the constructor: {code:java} super().__init__(command='', *args, **kwargs){code} instead of: {code:java} super().__init__(*args, **kwargs){code} The problematic file is (line 83): {code:java} /v1-10-stable/airflow/contrib/operators/ssh_operator.py {code} !ddd.png! Thanks"
361,AIRFLOW-6426,AIRFLOW,1577976988000,1577984909000,Bug,Resolved,Critical,2,443,"Fix broken example_gcs Dag I get the following error with the current example_gcs DAG: https://github.com/apache/airflow/blob/aa90753cf5f64bf435044cf2e6b81a02fdcf6b33/airflow/gcp/example_dags/example_gcs.py#L130 {noformat} File ""/opt/airflow/airflow/gcp/operators/gcs.py"", line 372, in __init__ raise ValueError(""Either object or prefix should be set. Both are None"") ValueError: Either object or prefix should be set. Both are None {noformat}"
362,AIRFLOW-6436,AIRFLOW,1578030879000,1578770776000,Improvement,Resolved,Critical,2,349,"Create & Automate docs on Airflow Configs Follow Up of https://issues.apache.org/jira/browse/AIRFLOW-6414 This PR aims to automate the creation of Config docs and add pre-commit hooks that would also serve as tests. Also, add a structure to the documentation by adding the following sections: * Description * Example * Type * Default for each config"
363,AIRFLOW-6560,AIRFLOW,1579028781000,1582712063000,Bug,Open,Critical,2,623,"db password leaks to logs I have configured Airflow metadata db as MySQL. {code:java} sql_alchemy_conn = sqla+mysql://airflow:airflow@localhost:3306/airflow{code} After that I have used initdb command: {code:java} airflow initdb{code} Tables in the airflow db have been created. Then I have runed commands: {code:java} airflow worker{code} in logs: {code:java} [2020-01-14 18:39:03,457: INFO/MainProcess] Connected to sqla+mysql://airflow:airflow@localhost:3306/airflow{code} password as a plain text. Probably comes that from Celery - [https://github.com/celery/celery/blob/master/celery/worker/consumer/connection.py#L24]"
364,AIRFLOW-6696,AIRFLOW,1580490974000,1620203726000,Bug,Closed,Critical,2,1622,"Package manager installable not a full distribution for rbac use +Problem+ The 'pip' or 'conda' distribution of airflow is not a full distribution for anyone wanting a turn-key package. The 'dist' folder is not included in '.../www_rbac/static'. +Proposed solution+ The expectation of the package manager is to have a useable asset once the package is installed. Packages relying on additional dependencies tend to stick with +sdist+ packaging route, so that they can expose those dependencies during build time (think C-backed packages, for instance). 'conda install airflow' did not expose this need for an additional step, and based on numerous other issues open in the Airflow backlog, chances are 'pip install' suffers the same fate. Preferably for the proposed solution, just prescribe defaults for the 'dist' folder, and include ways to override it through some mechanism. Airflow already has this pattern for 'webserver_config.py', so the pattern is already there, if you'd want to extend the pattern to these assets. Whatever the solution, 'pip install' or 'conda install' should provide a usable, turn-key package without the need for an additional build system downstream. +Why+ Many of our deployment targets don't have npm, and won't ever have it. We are then forced into a position of composing an environment at a tag revision, then using 'conda-pack' to send that revision to another place where the static resources can be built, back to a second 'conda-pack' with the built bundle. From our perspective, this build pattern has minimal lift over the previous static deployment path and is mainly a hassle."
365,AIRFLOW-6772,AIRFLOW,1581424206000,1595333276000,Bug,Open,Critical,2,3891,"{celery_executor.py:224} ERROR - Error sending Celery task:No module named 'MySQLdb' I have installed airflow with python3.6 and airflow version as 1.10.7 I did necessary changes in airflow.cfg as below {code:java} // celery_result_backend = db+postgresql://airflow:airflow@localhost/airflow{code} When I am trying to run a sample DAG (tutorial), it is throwing this error in my scheduler. {code:java} [2020-02-11 13:20:50,892] {celery_executor.py:224} ERROR - Error sending Celery task:No module named 'MySQLdb' Celery Task ID: ('tutorial', 'print_date', datetime.datetime(2020, 2, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) Traceback (most recent call last): File ""/usr/local/lib/python3.6/dist-packages/airflow/executors/celery_executor.py"", line 118, in send_task_to_executor result = task.apply_async(args=[command], queue=queue) File ""/usr/local/lib/python3.6/dist-packages/celery/app/task.py"", line 568, in apply_async **options File ""/usr/local/lib/python3.6/dist-packages/celery/app/base.py"", line 771, in send_task amqp.send_task_message(P, name, message, **options) File ""/usr/local/lib/python3.6/dist-packages/celery/app/amqp.py"", line 559, in send_task_message **properties File ""/usr/local/lib/python3.6/dist-packages/kombu/messaging.py"", line 181, in publish exchange_name, declare, File ""/usr/local/lib/python3.6/dist-packages/kombu/connection.py"", line 518, in _ensured return fun(*args, **kwargs) File ""/usr/local/lib/python3.6/dist-packages/kombu/messaging.py"", line 194, in _publish [maybe_declare(entity) for entity in declare] File ""/usr/local/lib/python3.6/dist-packages/kombu/messaging.py"", line 194, in <listcomp> [maybe_declare(entity) for entity in declare] File ""/usr/local/lib/python3.6/dist-packages/kombu/messaging.py"", line 102, in maybe_declare return maybe_declare(entity, self.channel, retry, **retry_policy) File ""/usr/local/lib/python3.6/dist-packages/kombu/common.py"", line 121, in maybe_declare return _maybe_declare(entity, channel) File ""/usr/local/lib/python3.6/dist-packages/kombu/common.py"", line 161, in _maybe_declare entity.declare(channel=channel) File ""/usr/local/lib/python3.6/dist-packages/kombu/entity.py"", line 609, in declare self._create_queue(nowait=nowait, channel=channel) File ""/usr/local/lib/python3.6/dist-packages/kombu/entity.py"", line 618, in _create_queue self.queue_declare(nowait=nowait, passive=False, channel=channel) File ""/usr/local/lib/python3.6/dist-packages/kombu/entity.py"", line 653, in queue_declare nowait=nowait, File ""/usr/local/lib/python3.6/dist-packages/kombu/transport/virtual/base.py"", line 531, in queue_declare self._new_queue(queue, **kwargs) File ""/usr/local/lib/python3.6/dist-packages/kombu/transport/sqlalchemy/__init__.py"", line 82, in _new_queue self._get_or_create(queue) File ""/usr/local/lib/python3.6/dist-packages/kombu/transport/sqlalchemy/__init__.py"", line 70, in _get_or_create obj = self.session.query(self.queue_cls) \ File ""/usr/local/lib/python3.6/dist-packages/kombu/transport/sqlalchemy/__init__.py"", line 65, in session _, Session = self._open() File ""/usr/local/lib/python3.6/dist-packages/kombu/transport/sqlalchemy/__init__.py"", line 56, in _open engine = self._engine_from_config() File ""/usr/local/lib/python3.6/dist-packages/kombu/transport/sqlalchemy/__init__.py"", line 51, in _engine_from_config return create_engine(conninfo.hostname, **transport_options) File ""/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/__init__.py"", line 479, in create_engine return strategy.create(*args, **kwargs) File ""/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/strategies.py"", line 87, in create dbapi = dialect_cls.dbapi(**dbapi_args) File ""/usr/local/lib/python3.6/dist-packages/sqlalchemy/dialects/mysql/mysqldb.py"", line 118, in dbapi return __import__(""MySQLdb"") ModuleNotFoundError: No module named 'MySQLdb' {code} what could be the issue here"
366,AIRFLOW-6779,AIRFLOW,1581471205000,1581710743000,Sub-task,Open,Critical,2,263,Schedule stopped scheduling tasks with concurrency limit or depends on past https://github.com/apache/airflow/pull/4751 introduced a bug that stops the scheduler from scheduling tasks with the following properties: * depends on past * has custom concurrency limit
367,AIRFLOW-6802,AIRFLOW,1581656536000,1585690057000,Bug,Resolved,Critical,2,233,scheduler not honoring dag.max_active_run config commit https://github.com/apache/airflow/commit/50efda5c69c1ddfaa869b408540182fb19f1a286 introduced a bug that prevents the scheduler from enforcing max active run config for all DAGs.
368,AIRFLOW-6904,AIRFLOW,1582555328000,1620203654000,Bug,Resolved,Critical,2,339,"Airflow 1.10.7 suppresses Operator logs After upgrading from 1.10.2 to 1.10.9, we noticed that the Operator logs are no longer printed. See the attachments for comparison. There is also a slack channel discussion pointing to a recen change that may have broken this - [https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1582548602014200]"
369,AIRFLOW-7020,AIRFLOW,1583779426000,1583779588000,Bug,Open,Critical,2,933,"Jobs running with cron is not working as expected Job submitted with @daily is finished executing. It worked as expected and finished execution of all tasks like 1 hour ago: [|https://imgur.com/uqRz8W6)] But another dag running with [15 * * * *|https://w2ec996050d80d4f7-tp.appspot.com/admin/dagrun/?flt2_dag_id_equals=load_num_tasks_t4] is not working as expected all it's task are clogged. (queued not at all running) ![Clogged]([https://imgur.com/rnxvOdw)] I waited for hours but still the same result, the job's task is in null state. So, I copied the code and just changed the cron from 15 * * * *. to @daily and changed dag name and started as new dag this one works like a charm ![DailyDag]([https://imgur.com/wECr2Qg)] As you can see from the following image: ![Difference]([https://imgur.com/ibvA2W3)] * 1st one is dag with 15 * * * *. * 2nd one with @daily Why I am getting this? Is it a known issue? Is there any solution?"
370,AIRFLOW-7039,AIRFLOW,1583880565000,1607630901000,Bug,Open,Critical,2,8165,"Specific DAG Schedule & DST Results in Skipped DAG Run *Scenario:* EC2 running airflow is in Eastern Time (America/New_York), airflow.cfg>[core]>default_timezone=America/New_York (automatically changes correctly) Monday morning after Daylight Savings Time applied a handful of DAG runs were not executed as expected. The strange part is that these DAGs were the only jobs that did not behave as expected, all other DAGs ran normally. Additionally, only the first expected run after DST was skipped, subsequent runs later that day were scheduled successfully. Here is the pattern observed: DAG Schedule which skipped first run: (0 <hour_a>,<hour_b> * * 1,2,3,4,5) e.g. Schedules M-F, with two distinct runs per day. DAGs that run at one time, M-F & DAGs that run at two times, not M-F did not experience this issue. Based on the logs, it appears as if the expected run that was missed was not seen by the scheduler whatsoever (see below): 2020 03 06 6:30 AM ET (BEFORE DST, EXPECTED BEHAVIOR): [2020-03-06 06:31:01,220] \{logging_mixin.py:112} INFO - [2020-03-06 06:31:01,220] \{settings.py:254} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=697 [2020-03-06 06:31:01,222] \{scheduler_job.py:153} INFO - Started process (PID=697) to work on /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py [2020-03-06 06:31:01,228] \{scheduler_job.py:1539} INFO - Processing file /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py for tasks to queue [2020-03-06 06:31:01,228] \{logging_mixin.py:112} INFO - [2020-03-06 06:31:01,228] \{dagbag.py:403} INFO - Filling up the DagBag from /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py [2020-03-06 06:31:01,238] \{scheduler_job.py:1551} INFO - DAG(s) dict_keys(['<DAG_NAME>']) retrieved from /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py [2020-03-06 06:31:01,305] \{scheduler_job.py:1262} INFO - Processing <DAG_NAME> [2020-03-06 06:31:01,348] \{logging_mixin.py:112} WARNING - /home/ec2-user/venv/lib64/python3.7/site-packages/pymysql/cursors.py:170: Warning: (1300, ""Invalid utf8mb4 character string: '80047D'"") result = self._query(query) [2020-03-06 06:31:01,362] \{scheduler_job.py:1272} INFO - Created <DagRun <DAG_NAME> @ 2020-03-05T15:30:00+00:00: scheduled__2020-03-05T15:30:00+00:00, externally triggered: False> [2020-03-06 06:31:01,366] \{scheduler_job.py:740} INFO - Examining DAG run <DagRun <DAG_NAME> @ 2020-03-05 15:30:00+00:00: scheduled__2020-03-05T15:30:00+00:00, externally triggered: False> [2020-03-06 06:31:01,389] \{scheduler_job.py:440} INFO - Skipping SLA check for <DAG: <DAG_NAME>> because no tasks in DAG have SLAs [2020-03-06 06:31:01,395] \{scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: <DAG_NAME>.<TASK_NAME> 2020-03-05 15:30:00+00:00 [scheduled]> in ORM [2020-03-06 06:31:01,414] \{scheduler_job.py:161} INFO - Processing /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py took 0.192 seconds 20200306 10 AM ET (BEFORE DST, EXPECTED BEHAVIOR): [2020-03-06 10:30:00,083] \{logging_mixin.py:112} INFO - [2020-03-06 10:30:00,082] \{settings.py:254} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=16194 [2020-03-06 10:30:00,085] \{scheduler_job.py:153} INFO - Started process (PID=16194) to work on /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py [2020-03-06 10:30:00,090] \{scheduler_job.py:1539} INFO - Processing file /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py for tasks to queue [2020-03-06 10:30:00,090] \{logging_mixin.py:112} INFO - [2020-03-06 10:30:00,090] \{dagbag.py:403} INFO - Filling up the DagBag from /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py [2020-03-06 10:30:00,099] \{scheduler_job.py:1551} INFO - DAG(s) dict_keys(['<DAG_NAME>']) retrieved from /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py [2020-03-06 10:30:00,159] \{scheduler_job.py:1262} INFO - Processing <DAG_NAME> [2020-03-06 10:30:00,193] \{logging_mixin.py:112} WARNING - /home/ec2-user/venv/lib64/python3.7/site-packages/pymysql/cursors.py:170: Warning: (1300, ""Invalid utf8mb4 character string: '80047D'"") result = self._query(query) [2020-03-06 10:30:00,207] \{scheduler_job.py:1272} INFO - Created <DagRun <DAG_NAME> @ 2020-03-06T11:30:00+00:00: scheduled__2020-03-06T11:30:00+00:00, externally triggered: False> [2020-03-06 10:30:00,212] \{scheduler_job.py:740} INFO - Examining DAG run <DagRun <DAG_NAME> @ 2020-03-06 11:30:00+00:00: scheduled__2020-03-06T11:30:00+00:00, externally triggered: False> [2020-03-06 10:30:00,232] \{scheduler_job.py:440} INFO - Skipping SLA check for <DAG: <DAG_NAME>> because no tasks in DAG have SLAs [2020-03-06 10:30:00,236] \{scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: <DAG_NAME>.<TASK_NAME> 2020-03-06 11:30:00+00:00 [scheduled]> in ORM [2020-03-06 10:30:00,251] \{scheduler_job.py:161} INFO - Processing /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py took 0.166 seconds 2020 03 09 6:30 ET (AFTER DST, *UNEXPECTED BEHAVIOR*): [2020-03-09 06:31:55,240] \{logging_mixin.py:112} INFO - [2020-03-09 06:31:55,240] \{settings.py:254} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=23336 [2020-03-09 06:31:55,242] \{scheduler_job.py:153} INFO - Started process (PID=23336) to work on /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py [2020-03-09 06:31:55,247] \{scheduler_job.py:1539} INFO - Processing file /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py for tasks to queue [2020-03-09 06:31:55,247] \{logging_mixin.py:112} INFO - [2020-03-09 06:31:55,247] \{dagbag.py:403} INFO - Filling up the DagBag from /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py [2020-03-09 06:31:55,257] \{scheduler_job.py:1551} INFO - DAG(s) dict_keys(['<DAG_NAME>']) retrieved from /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py [2020-03-09 06:31:55,312] \{scheduler_job.py:1262} INFO - Processing <DAG_NAME> [2020-03-09 06:31:55,338] \{scheduler_job.py:440} INFO - Skipping SLA check for <DAG: <DAG_NAME>> because no tasks in DAG have SLAs [2020-03-09 06:31:55,343] \{scheduler_job.py:161} INFO - Processing /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py took 0.101 seconds 20200309 10 AM ET (AFTER DST, EXPECTED BEHAVIOR): [2020-03-09 10:30:54,487] \{logging_mixin.py:112} INFO - [2020-03-09 10:30:54,487] \{settings.py:254} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=6421 [2020-03-09 10:30:54,490] \{scheduler_job.py:153} INFO - Started process (PID=6421) to work on /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py [2020-03-09 10:30:54,498] \{scheduler_job.py:1539} INFO - Processing file /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py for tasks to queue [2020-03-09 10:30:54,499] \{logging_mixin.py:112} INFO - [2020-03-09 10:30:54,499] \{dagbag.py:403} INFO - Filling up the DagBag from /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py [2020-03-09 10:30:54,508] \{scheduler_job.py:1551} INFO - DAG(s) dict_keys(['<DAG_NAME>']) retrieved from /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py [2020-03-09 10:30:54,585] \{scheduler_job.py:1262} INFO - Processing <DAG_NAME> [2020-03-09 10:30:54,621] \{logging_mixin.py:112} WARNING - /home/ec2-user/venv/lib64/python3.7/site-packages/pymysql/cursors.py:170: Warning: (1300, ""Invalid utf8mb4 character string: '80047D'"") result = self._query(query) [2020-03-09 10:30:54,634] \{scheduler_job.py:1272} INFO - Created <DagRun <DAG_NAME> @ 2020-03-09T10:30:00+00:00: scheduled__2020-03-09T10:30:00+00:00, externally triggered: False> [2020-03-09 10:30:54,638] \{scheduler_job.py:740} INFO - Examining DAG run <DagRun <DAG_NAME> @ 2020-03-09 10:30:00+00:00: scheduled__2020-03-09T10:30:00+00:00, externally triggered: False> [2020-03-09 10:30:54,660] \{scheduler_job.py:440} INFO - Skipping SLA check for <DAG: <DAG_NAME>> because no tasks in DAG have SLAs [2020-03-09 10:30:54,665] \{scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: <DAG_NAME>.<TASK_NAME> 2020-03-09 10:30:00+00:00 [scheduled]> in ORM [2020-03-09 10:30:54,683] \{scheduler_job.py:161} INFO - Processing /home/ec2-user/airflow/s3fuse/dags/<DAG_NAME>.py took 0.193 seconds"
371,AIRFLOW-7091,AIRFLOW,1584621782000,1620203634000,Bug,Closed,Critical,2,552,"use external worker pod template I am trying to use my own pod template file with KubernetesExecutor. So, in the official documentation of airflow.cfg, there's a reference that we can use external pod template {code:java} # Path to the YAML pod file. If set, all other kubernetes-related fields are ignored. pod_template_file = /path/to/file{code} I tried to use it by specifying the path to a pod template file, but it's not working at all as the scheduler cannot launch workers. Do you have any ideas how can I use it or how to solve this? Thank you."
372,AIRFLOW-1,AIRFLOW,1460693246000,1501720327000,Improvement,Resolved,Major,3,33,Migrate GitHub code to Apache git
373,AIRFLOW-6,AIRFLOW,1461283488000,1535980882000,Bug,Closed,Major,3,163,"Remove high-charts According to LEGAL-246, we can't use High Charts in Airflow. The license is incompatible with Apache. We should replace it with another library."
374,AIRFLOW-9,AIRFLOW,1461781700000,1468245384000,Bug,Closed,Major,3,100,Create Airflow website We should set up an Airflow website for: http://airflow.incubator.apache.org/
375,AIRFLOW-10,AIRFLOW,1461782400000,1464031880000,Bug,Closed,Major,3,32,Migrate GH issues to Apache JIRA
376,AIRFLOW-11,AIRFLOW,1461782412000,1462809913000,Bug,Closed,Major,3,30,Migrate mailing list to Apache
377,AIRFLOW-12,AIRFLOW,1461782490000,1461809600000,Bug,Closed,Major,3,35,Migrate GitHub wiki to Apache cwiki
378,AIRFLOW-13,AIRFLOW,1461782511000,1535109397000,Bug,Resolved,Major,3,32,Migrate Travis CI to Apache repo
379,AIRFLOW-14,AIRFLOW,1461852833000,1585329324000,Improvement,Closed,Major,3,1990,"DagRun Refactor (Scheduler 2.0) For full proposal, please see the Wiki: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=62694286 Borrowing from that page: *Description of New Workflow* DagRuns represent the state of a DAG at a certain point in time (perhaps they should be called DagInstances?). To run a DAG  or to manage the execution of a DAG  a DagRun must first be created. This can be done manually (simply by creating a DagRun object) or automatically, using methods like dag.schedule_dag(). Therefore, both scheduling new runs OR introducing ad-hoc runs can be done by any process at any time, simply by creating the appropriate object. Just creating a DagRun is not enough to actually run the DAG (just as creating a TaskInstance is not the same as actually running a task). We need a Job for that. The DagRunJob is fairly simple in structure. It maintains a set of DagRuns that it is tasked with executing, and loops over that set until all the DagRuns either succeed or fail. New DagRuns can be passed to the job explicitly via DagRunJob.submit_dagruns() or by defining its DagRunJob.collect_dagruns() method, which is called during each loop. When the DagRunJob is executing a specific DagRun, it locks it. Other DagRunJobs will not try to execute locked DagRuns. This way, many DagRunJobs can run simultaneously in either a local or distributed setting, and can even be pointed at the same DagRuns, without worrying about collisions or interference. The basic DagRunJob loop works like this: - refresh dags - collect new dagruns - process dagruns (including updating dagrun states for success/failure) - call executor/own heartbeat By tweaking the DagRunJob, we can easily recreate the behavior of the current SchedulerJob and BackfillJob. The Scheduler simply runs forever and picks up ALL active DagRuns in collect_dagruns(); Backfill generates DagRuns corresponding to the requested start/end dates and submits them to itself prior to initiating its loop."
380,AIRFLOW-15,AIRFLOW,1461854580000,1512769963000,Task,Resolved,Major,3,892,"Remove GCloud from Airflow After speaking with Google, there was some concern about using the [gcloud-python|https://github.com/GoogleCloudPlatform/gcloud-python] library for Airflow. There are several concerns: # It's not clear (even to people at Google) what this library is, who owns it, etc. # It does not support all services (the way [google-api-python-client|https://github.com/google/google-api-python-client] does). # There are compatibility issues between google-api-python-client and gcloudpython. We currently support both, after libraries depending on which package you you install: {{airfow[gcp_api]}} or {{airflow[gcloud]}}. This ticket is to remove the {{airflow[gcloud]}} packaged, and all associated code. The main associated code, afaik, is the use of the {{gcloud}} library in the Google cloud storage hooks/operators--specifically for Google cloud storage Airfow logging."
381,AIRFLOW-16,AIRFLOW,1461861027000,1462288946000,Improvement,Resolved,Major,3,561,"Use GCP-specific fields in hook view Once AIRFLOW-15 is done, we should update the Google cloud base hook to use fields for project, service account, etc. We currently just use a JSON blob in the {{extras}} field. We can steal this code from [this|https://github.com/airbnb/airflow/pull/1119/files] PR, where {{extras\_\_google_cloud_platform_*}} is introduced in views.py. We should also look at creating just one hook of type google_cloud_platform, rather than one hook per Google cloud service. Again, this is how the PR (above) works, and it's pretty handy."
382,AIRFLOW-17,AIRFLOW,1461861631000,1461869704000,Improvement,Closed,Major,3,213,Master Travis CI build is broken It looks like master is broken: https://travis-ci.org/airbnb/airflow/branches This build seems to be the first one that broke: https://travis-ci.org/airbnb/airflow/builds/126014622
383,AIRFLOW-20,AIRFLOW,1461938297000,1585507539000,Improvement,Closed,Major,3,1913,"Improving the scheduler by making dag runs more coherent The need to align the start_date with the interval is counter intuitive and leads to a lot of questions and issue creation, although it is in the documentation. If we are able to fix this with none or little consequences for current setups that should be preferred, I think. The dependency explainer is really great work, but it doesnt address the core issue. If you consider a DAG a description of cohesion between work items (in OOP java terms a class), then a DagRun is the instantiation of a DAG in time (in OOP java terms an instance). Tasks are then the description of a work item and a TaskInstance the instantiation of the Task in time. In my opinion issues pop up due to the current paradigm of considering the TaskInstance the smallest unit of work and asking it to maintain its own state in relation to other TaskInstances in a DagRun and in a previous DagRun of which it has no (real) perception. Tasks are instantiated by a cartesian product with the dates of DagRun instead of the DagRuns itself. The very loose coupling between DagRuns and TaskInstances can be improved while maintaining flexibility to run tasks without a DagRun. This would help with a couple of things: 1. start_date can be used as a execution_date or a point in time when to start looking 2. a new interval for a dag will maintain depends_on_past 3. paused dags do not give trouble 4. tasks will be executed in order 5. the ignore_first_depend_on_past could be removed as a task will now know if it is really the first In PR-1431 a lot of this work has been done by: 1. Adding a previous field to a DagRun allowing it to connect to its predecessor 2. Adding a dag_run_id to TaskInstances so a TaskInstance knows about the DagRun if needed 3. Using start_date + interval as the first run date unless start_date is on the interval then start_date is the first run date"
384,AIRFLOW-21,AIRFLOW,1461945650000,1462467960000,Improvement,Resolved,Major,3,367,"Upgrade GCP libraries to latest We should upgrade the Google cloud platform libraries: oauth2client google-api-python-client To their latest publish versions. This will involve moving off of {{SignedJwtAssertionCredentials}}, as it was removed. I think we might need to use {{ServiceAccountCredentials}} instead. See https://github.com/google/oauth2client/issues/401."
385,AIRFLOW-22,AIRFLOW,1461947262000,1521017925000,Improvement,Resolved,Major,3,681,"Google cloud code needs tests The Google cloud contrib hooks and operators need tests to verify that they work. This is somewhat tricky since they're hard to mock, and most of the value comes from integration tests, not unit tests. I spoke with [~alexvanboxel], and he suggested just writing a script that can be run manually, and exercises the hooks/operators to verify that they work as expected. I think this is about the best we can do without spending a ton of time on it, since most of the integration tests require a real Google cloud project, with real buckets/BQ datasets/DBs, etc. Running it locally means we can run the tests against whatever projects we have access to."
386,AIRFLOW-23,AIRFLOW,1461956355000,1535980895000,New Feature,Closed,Major,3,222,Support Google cloud DataProc in Airflow It would be useful to be able to interact with [Google cloud DataProc|https://cloud.google.com/dataproc/] clusters from Airflow. We should add the proper hooks/operators to do this.
387,AIRFLOW-24,AIRFLOW,1461956381000,1535980897000,New Feature,Closed,Major,3,222,Support Google cloud Dataflow in Airflow It would be useful to be able to interact with [Google cloud Dataflow|https://cloud.google.com/dataflow/] clusters from Airflow. We should add the proper hooks/operators to do this.
388,AIRFLOW-27,AIRFLOW,1462178996000,1462291937000,Bug,Closed,Major,3,2662,"MsSqlOperator: ERROR - 'pymssql.Connection' object attribute 'autocommit' is read-only I have the following code: {code} from airflow import DAG from airflow.operators import MsSqlOperator from datetime import datetime, timedelta args = { 'owner': 'airflow', 'depends_on_past': True, 'start_date': datetime(2016, 4, 6, 1, 0), 'email': ['example@email.com'], 'email_on_failure': True, 'email_on_retry': True, 'retries': 1, 'retry_delay': timedelta(minutes=5), 'execution_timeout': timedelta(hours=23), 'priority_weight': 10 } dag = DAG('dag_name', default_args=args, schedule_interval='0 1 * * *') t_generate_metadata_csv = MsSqlOperator(task_id='task_id', sql='SELECT a FROM b', mssql_conn_id='mssql_conn', dag=dag) {code} And I'm getting this error: {code} ERROR - 'pymssql.Connection' object attribute 'autocommit' is read-only Traceback (most recent call last): File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1064, in run result = task_copy.execute(context=context) File ""/usr/local/lib/python2.7/dist-packages/airflow/operators/mssql_operator.py"", line 34, in execute hook.run(self.sql, parameters=self.parameters) File ""/usr/local/lib/python2.7/dist-packages/airflow/hooks/dbapi_hook.py"", line 101, in run self.set_autocommit(conn, autocommit) File ""/usr/local/lib/python2.7/dist-packages/airflow/hooks/dbapi_hook.py"", line 115, in set_autocommit conn.autocommit = autocommit {code} Looking at the source code I think I might know why it fails (I have added links to the referenced lines in the source code). In dbapi_hook, [if the connection supports autocommit|https://github.com/airbnb/airflow/blob/master/airflow/hooks/dbapi_hook.py#L123] then we call set_autocommit, which does [conn.autocommit = autocommit|https://github.com/airbnb/airflow/blob/master/airflow/hooks/dbapi_hook.py#L138]. MsSqlHook has [supports_autocommit set to true|https://github.com/airbnb/airflow/blob/master/airflow/hooks/mssql_hook.py#L13], but the connection type it uses, which is pymssql has no autocommit attribute. [Instead it is called _autocommit|https://github.com/pymssql/pymssql/blob/f814fd9dedc7cd87fabbe92dfb1b6d19a5ed31a3/src/pymssql.pyx#L209] and [autocommit is defined as a function|https://github.com/pymssql/pymssql/blob/f814fd9dedc7cd87fabbe92dfb1b6d19a5ed31a3/src/pymssql.pyx#L255] to turn autocommit on or off. So it seems that the error is caused by trying to set the conn.autocommit to a boolean value when it is not actually a variable but a function. This seems to be supported by this [link|http://www.gossamer-threads.com/lists/python/python/514536] where someone causes the same error by trying to assign a value to a function."
389,AIRFLOW-30,AIRFLOW,1462180860000,1585613910000,Improvement,Closed,Major,3,1079,"Make preoperators part of the same transaction as the actual operation All my use cases would work better if each operator would execute everything in one transaction. Two examples: - I want to {{GenericTransfer}} a set of rows from one DB to another, and I have to create the table first in the destination DB. I feel like it'd be a lot more clean if I didn't have empty tables lying around if the insertion fails for some reason later on. - I want to {{GenericTransfer}} all rows from an entire table periodically to sync it from one DB to another. To do this correctly I want to clear the destination table first to make sure I end up with no duplicate rows, so I'd have a {{DELETE * FROM dst_table}} preoperator. If the insertions fail afterwards, I'd end up with no data (it would be better in most cases to fall back to the old data), and even if everything is working correctly, I'll have an empty table while the insertions as still executing. To fix this, the relevant {{DbApiHook}} methods could support a new kwarg to set whether it should commit at the end. Thoughts?"
390,AIRFLOW-31,AIRFLOW,1462192209000,1535981007000,Improvement,Closed,Major,3,4409,"Use standard imports for hooks/operators (Migrated from https://github.com/airbnb/airflow/issues/1238) Currently, Airflow uses a relatively complex import mechanism to import hooks and operators without polluting the namespace with submodules. I would like to propose that Airflow abandon that system and use standard Python importing. Here are a few major reasons why I think the current system has run its course. h3. Polluting namespace The biggest advantage of the current system, as I understand it, is that only Operators appear in the `airflow.operators` namespace. The submodules that actually contain the operators do not. So for example while `airflow.operators.python_operator.PythonOperator` is a thing, `PythonOperator` is in the `airflow.operators` namespace but `python_operator` is not. I think this sort of namespace pollution was helpful when Airflow was a smaller project, but as the number of hooks/operators grows -- and especially as the `contrib` hooks/operators grow -- I'd argue that namespacing is a *good thing*. It provides structure and organization, and opportunities for documentation (through module docstrings). In fact, I'd argue that the current namespace is itself getting quite polluted -- the only way to know what's available is to use something like Ipython tab-completion to browse an alphabetical list of Operator names, or to load the source file and grok the import definition (which no one installing from pypi is likely to do). h3. Conditional imports There's a second advantage to the current system that any module that fails to import is silently ignored. It makes it easy to have optional dependencies. For example, if someone doesn't have `boto` installed, then they don't have an `S3Hook` either. Same for a HiveOperator Again, as Airflow grows and matures, I think this is a little too magic. If my environment is missing a dependency, I want to hear about it. On the other hand, the `contrib` namespace sort of depends on this -- we don't want users to have to install every single dependency. So I propose that contrib modules all live in their submodules: `from airflow.contrib.operators.my_operator import MyOperator`. As mentioned previously, having structure and namespacing is a good thing as the project gets more complex. Other ways to handle this include putting ""non-standard"" dependencies inside the operator/hook rather than the module (see `HiveOperator`/`HiveHook`), so it can be imported but not used. Another is judicious use of `try`/`except ImportError`. The simplest is to make people import things explicitly from submodules. h3. Operator dependencies Right now, operators can't depend on each other if they aren't in the same file. This is for the simple reason that there is no guarantee on what order the operators will be loaded. It all comes down to which dictionary key gets loaded first. One day Operator B could be loaded after Operator A; the next day it might be loaded before. Consequently, A and B can't depend on each other. Worse, if a user makes two operators that do depend on each other, they won't get an error message when one fails to import. For contrib modules in particular, this is sort of killer. h3. Ease of use It's *hard* to set up imports for a new operator. The dictionary-based import instructions aren't obvious for new users, and errors are silently dismissed which makes debugging difficult. h3. Identity Surprisingly, `airflow.operators.SubDagOperator != airflow.operators.subdag_operator.SubDagOperator`. See #1168. h2. Proposal Use standard python importing for hooks/operators/etc. - `__init__.py` files use straightforward, standard Python imports - major operators are available at `airflow.operators.OperatorName` or `airflow.operators.operator_module.OperatorName`. - contrib operators are only available at `airflow.contrib.operators.operator_module.OperatorName` in order to manage dependencies - operator authors are encouraged to use `__all__` to define their module's exports Possibly delete namespace afterward - in `operators/__init__.py`, run a function at the end of the file which deletes all modules from the namespace, leaving only `Operators`. This keeps the namespace clear but lets people use familiar import mechanisms. Possibly use an import function to handle `ImportError` gracefully - rewrite `import_module_attrs` to take one module name at a time instead of a dictionary."
391,AIRFLOW-32,AIRFLOW,1462194035000,1585507638000,Improvement,Closed,Major,3,523,Remove deprecated features prior to releasing Airflow 2.0 A number of features have been marked for deprecation in Airflow 2.0. They need to be deleted prior to release. Usually the error message or comments will mention Airflow 2.0 with either a #TODO or #FIXME. Tracking list (not necessarily complete!): JIRA: AIRFLOW-31 AIRFLOW-200 GitHub: https://github.com/airbnb/airflow/pull/1137/files#diff-1c2404a3a60f829127232842250ff406R233 https://github.com/airbnb/airflow/pull/1219 https://github.com/airbnb/airflow/pull/1285
392,AIRFLOW-33,AIRFLOW,1462222359000,1507182483000,Bug,Closed,Major,3,7356,"The Celery Executor did start successfully,jobs are running successfully but the same is not reflected in the UI recent status section The Celery Executor did start successfully,jobs are running successfully but the same is not reflected in the UI recent status section. Recent Status to show respective status of the Airflow jobs No Status shown in the Recent Status. airflow.cfg: [core] # The home folder for airflow, default is ~/airflow airflow_home = /root/airflow # The folder where your airflow pipelines live, most likely a # subfolder in a code repository dags_folder = /root/airflow/dags # The folder where airflow should store its log files. This location base_log_folder = /root/airflow/logs # An S3 location can be provided for log backups # For S3, use the full URL to the base folder (starting with ""s3://..."") s3_log_folder = None # The executor class that airflow should use. Choices include # SequentialExecutor, LocalExecutor, CeleryExecutor #executor = SequentialExecutor executor = CeleryExecutor # The SqlAlchemy connection string to the metadata database. # SqlAlchemy supports many different database engine, more information # their website #sql_alchemy_conn = sqlite:////root/airflow/airflow.db sql_alchemy_conn = mysql://username:pasword@username.us-east-1.rds.amazonaws.com:3306/airflow # The SqlAlchemy pool size is the maximum number of database connections # in the pool. sql_alchemy_pool_size = 5 # The SqlAlchemy pool recycle is the number of seconds a connection # can be idle in the pool before it is invalidated. This config does # not apply to sqlite. sql_alchemy_pool_recycle = 3600 # The amount of parallelism as a setting to the executor. This defines # the max number of task instances that should run simultaneously # on this airflow installation parallelism = 32 # The number of task instances allowed to run concurrently by the scheduler dag_concurrency = 16 # Are DAGs paused by default at creation dags_are_paused_at_creation = False # The maximum number of active DAG runs per DAG max_active_runs_per_dag = 16 # Whether to load the examples that ship with Airflow. It's good to # get started, but you probably want to set this to False in a production # environment load_examples = True # Where your Airflow plugins are stored plugins_folder = /root/airflow/plugins # Secret key to save connection passwords in the db fernet_key = cryptography_not_found_storing_passwords_in_plain_text # Whether to disable pickling dags donot_pickle = False # How long before timing out a python file import while filling the DagBag dagbag_import_timeout = 30 [webserver] # The base url of your website as airflow cannot guess what domain or # cname you are using. This is use in automated emails that # airflow sends to point links to the right web server base_url = http://localhost:8080 # The ip specified when starting the web server web_server_host = 0.0.0.0 # The port on which to run the web server web_server_port = 8080 # Secret key used to run your flask app secret_key = temporary_key # Number of workers to run the Gunicorn web server workers = 4 # The worker class gunicorn should use. Choices include # sync (default), eventlet, gevent worker_class = sync # Expose the configuration file in the web server expose_config = true # Set to true to turn on authentication : http://pythonhosted.org/airflow/installation.html#web-authentication authenticate = False # Filter the list of dags by owner name (requires authentication to be enabled) filter_by_owner = False [email] email_backend = airflow.utils.send_email_smtp [smtp] # If you want airflow to send emails on retries, failure, and you want to # the airflow.utils.send_email function, you have to configure an smtp # server here smtp_host = localhost smtp_starttls = True smtp_ssl = False smtp_user = airflow smtp_port = 25 smtp_password = airflow smtp_mail_from = airflow@airflow.com [celery] # This section only applies if you are using the CeleryExecutor in # [core] section above # The app name that will be used by celery celery_app_name = airflow.executors.celery_executor # The concurrency that will be used when starting workers with the # ""airflow worker"" command. This defines the number of task instances that # a worker will take, so size up your workers based on the resources on # your worker box and the nature of your tasks celeryd_concurrency = 16 # When you start an airflow worker, airflow starts a tiny web server # subprocess to serve the workers local log files to the airflow main # web server, who then builds pages and sends them to users. This defines # the port on which the logs are served. It needs to be unused, and open # visible from the main web server to connect into the workers. worker_log_server_port = 8793 # The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally # a sqlalchemy database. Refer to the Celery documentation for more # information. #broker_url = sqla+mysql://airflow:airflow@localhost:3306/airflow broker_url = sqla+mysql://username:password@username.us-east-1.rds.amazonaws.com:3306/airflow # Another key Celery setting #celery_result_backend = db+mysql://airflow:airflow@localhost:3306/airflow celery_result_backend = db+mysql://username:pasword@username.us-east-1.rds.amazonaws.com:3306/airflow # Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start # it `airflow flower`. This defines the port that Celery Flower runs on flower_port = 5555 # Default queue that tasks get assigned to and that worker listen on. default_queue = default [scheduler] # Task instances listen for external kill signal (when you clear tasks # from the CLI or the UI), this defines the frequency at which they should # listen (in seconds). job_heartbeat_sec = 5 # The scheduler constantly tries to trigger new tasks (look at the # scheduler section in the docs for more information). This defines # how often the scheduler should run (in seconds). scheduler_heartbeat_sec = 5 # Statsd (https://github.com/etsy/statsd) integration settings # statsd_on = False # statsd_host = localhost # statsd_port = 8125 # statsd_prefix = airflow [mesos] # Mesos master address which MesosExecutor will connect to. master = localhost:5050 # The framework name which Airflow scheduler will register itself as on mesos framework_name = Airflow # Number of cpu cores required for running one task instance using # 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>' # command on a mesos slave task_cpu = 1 # Memory in MB required for running one task instance using # 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>' # command on a mesos slave task_memory = 256 # Enable framework checkpointing for mesos # See http://mesos.apache.org/documentation/latest/slave-recovery/ checkpoint = False # Failover timeout in milliseconds. # When checkpointing is enabled and this option is set, Mesos waits until the configured timeout for # the MesosExecutor framework to re-register after a failover. Mesos shuts down running tasks if the # MesosExecutor framework fails to re-register within this timeframe. # failover_timeout = 604800 # Enable framework authentication for mesos # See http://mesos.apache.org/documentation/latest/configuration/ authenticate = False # Mesos credentials, if authentication is enabled # default_principal = admin # default_secret = admin"
393,AIRFLOW-35,AIRFLOW,1462235105000,1462307083000,Improvement,Closed,Major,3,522,"UI toggle to recursively set success for nested DAG operators Currently, when marking the success status of an operator via the UI, there is no way to recursively include all operators that are nested within a SubDagOperator We need a way to recursively set the success status (e.g., a UI toggle ""Recursive"" on the same level as Past/Future/Downstream/Upstream) of all operators within a SubDagOperator There is an open pull request on Github which proposes a solution for this: https://github.com/airbnb/airflow/pull/1430"
394,AIRFLOW-36,AIRFLOW,1462235531000,1462291215000,Bug,Closed,Major,3,1032,"Email on failure not working Airflow Following is my code: from airflow import DAG from airflow.operators import BashOperator from datetime import datetime, timedelta default_args = { 'owner': 'airflow', 'start_date': datetime.now()-timedelta(minutes=5), 'email': ['divya.h@gmail.com'], 'email_on_failure': True, 'email_on_retry': True, 'sla':datetime.now()-timedelta(seconds=60), 'retries': 1, 'retry_delay': timedelta(seconds=60) } dag = DAG('mysql_test', default_args=default_args) hdfs_copy = """""" cd /root/airflow/scripts/ ./sqoop_import.sh """""" # Importing the data from Mysql table to HDFS task1 = BashOperator( task_id= 'hdfs_copy', bash_command=hdfs_copy, dag=dag ) # Inserting the data from Hive external table to the target mysql table mysql_copy= """""" cd /root/airflow/scripts/ ./sqoop_export.sh """""" task2 = BashOperator( task_id= 'mysql_copy', bash_command=mysql_copy, depends_on_past=True, dag=dag ) # defining the job dependency task2.set_upstream(task1) The task1 and task2 both fail.I am not receiving any email alerts"
395,AIRFLOW-41,AIRFLOW,1462296508000,1558131590000,Bug,Resolved,Major,3,458,SubdagOperators can oversubscribe to pools due to race condition SubDagOperators essentially create their own mini-scheduler. Which can interfere with the main scheduler. SubdagOperators check if there is slot available in a Pool. However this slot is not claimed at the same time leaving room for main scheduler to also check for the slot. Both can then obtain a slot and thus oversubscribe A solution could be a centralized PoolHandler that gives out slots
396,AIRFLOW-42,AIRFLOW,1462301779000,1520285519000,Bug,Closed,Major,3,41,Adding logging.debug DagBag loading stats
397,AIRFLOW-44,AIRFLOW,1462307255000,1462320704000,Bug,Closed,Major,3,242,"Need steps or command to uninstall or remove airflow Need steps or command to uninstall or remove airflow web server on cluster.So that airflow can be reinstalled from scratch.I am new to airflow,seeing issues with CeleryExecutor.Please help."
398,AIRFLOW-45,AIRFLOW,1462314507000,1497877449000,Improvement,Closed,Major,3,677,"Support hidden Airflow variables We have a use case where someone wants to set a variable for their DAG, but they don't want it visible via the UI. I see that variables are encrypted in the DB (if the crypto package is installed), but the variables are still visible via the UI, which is a little annoying. Obviously, this is not 100% secure, since you can still create a DAG to read the variable, but it will at least keep arbitrary users from logging in/loading the UI and seeing the variable. I propose basically handling this the same way that DB hook passwords are handled. Don't show them in the UI when the edit button is clicked, but allow the variables to be editable."
399,AIRFLOW-46,AIRFLOW,1462325014000,1594907113000,Bug,Closed,Major,3,2884,"Running Trigger DAG test with SequentialExecutor yields error On a fresh install of Airflow 1.7.0, executed the following way : * git checkout -b b_1.7.0rc1 1.7.0rc1 * python setup.py develop * airflow initdb (against sqllite and using SequentialExecutor - delete the previous db file) * airflow webserver -d * airflow scheduler * Load examples in the airflow.cfg file and disable all but 2 via the UI as shown below : example_trigger_controller_dag and example_trigger_target_dag The controller dag will run and you will see the following line in the scheduler output to show it succeeded : {panel} [2016-05-04 02:11:47,680] {__init__.py:36} INFO - Using executor SequentialExecutor Controller DAG : conditionally_trigger = True { 'message': 'Hello World'} [2016-05-04 02:11:51,809] {jobs.py:516} INFO - Prioritizing 0 queued jobs {panel} However, when the target dag runs, you will see : {noformat} [2016-05-04 02:11:56,924] {sequential_executor.py:27} INFO - Executing command: airflow run example_trigger_target_dag bash_task 2016-05-04T01:11:47 --local -sd DAGS_FOLDER/example_dags/example_trigger_target_dag.py [2016-05-04 02:11:57,870] {__init__.py:36} INFO - Using executor SequentialExecutor Logging into: /Users/siddharth/Projects/airflow/logs/example_trigger_target_dag/bash_task/2016-05-04T01:11:47 [2016-05-04 02:11:59,008] {__init__.py:36} INFO - Using executor SequentialExecutor [2016-05-04 02:12:03,131] {sequential_executor.py:27} INFO - Executing command: airflow run example_trigger_target_dag run_this 2016-05-04T01:11:47 --local -sd DAGS_FOLDER/example_dags/example_trigger_target_dag.py [2016-05-04 02:12:04,056] {__init__.py:36} INFO - Using executor SequentialExecutor Logging into: /Users/siddharth/Projects/airflow/logs/example_trigger_target_dag/run_this/2016-05-04T01:11:47 [2016-05-04 02:12:05,179] {__init__.py:36} INFO - Using executor SequentialExecutor Traceback (most recent call last): File ""/Users/siddharth/Projects/airflow/venv/bin/airflow"", line 6, in <module> exec(compile(open(__file__).read(), __file__, 'exec')) File ""/Users/siddharth/Projects/airflow/airflow/bin/airflow"", line 15, in <module> args.func(args) File ""/Users/siddharth/Projects/airflow/airflow/bin/cli.py"", line 203, in run pool=args.pool, File ""/Users/siddharth/Projects/airflow/airflow/models.py"", line 1067, in run result = task_copy.execute(context=context) File ""/Users/siddharth/Projects/airflow/airflow/operators/python_operator.py"", line 65, in execute return_value = self.python_callable(*self.op_args, **self.op_kwargs) File ""/Users/siddharth/Projects/airflow/airflow/example_dags/example_trigger_target_dag.py"", line 37, in run_this_func print( ""Remotely received value of {} for key=message"".format(kwargs['dag_run'].conf['message'])) AttributeError: 'NoneType' object has no attribute 'conf' [2016-05-04 02:12:09,318] {jobs.py:516} INFO - Prioritizing {noformat}"
400,AIRFLOW-47,AIRFLOW,1462344029000,1561109535000,Bug,Closed,Major,3,1697,"ExternalTaskSensor causes scheduling dead lock We are trying to use 'ExternalTaskSensor' to coordinate between a daily DAG and an hourly DAG (daily dags depend on hourly). Relevant code: *Daily DAG definition:* {code:title=2_daily_dag.py|borderStyle=solid} default_args = {  'start_date': datetime(2016, 4, 2),  } dag = DAG(dag_id='2_daily_agg', default_args=default_args, schedule_interval=""@daily"") ext_dep = ExternalTaskSensor( external_dag_id='1_hourly_agg', external_task_id='print_hourly1', task_id='evening_hours_sensor', dag=dag) {code} *Hourly DAG definition:* {code:title=1_hourly_dag.py|borderStyle=solid} default_args = {  'start_date': datetime(2016, 4, 1),  } dag = DAG(dag_id='1_hourly_agg', default_args=default_args, schedule_interval=""@hourly"") t1 = BashOperator( task_id='print_hourly1', bash_command='echo hourly job1', dag=dag) {code} The hourly dag was executed twice for the following execution dates: 04-01T00:00:00 04-01T01:00:00 Then the daily dag was executed, and is still running.... According to logs, daily dag is waiting for hourly dag to complete: {noformat} [2016-05-04 06:01:20,978] {models.py:1041} INFO - Executing<Task(ExternalTaskSensor): evening_hours_sensor> on 2016-04-03 00:00:00 [2016-05-04 06:01:20,984] {sensors.py:188} INFO - Poking for 1_hourly_agg.print_hourly1 on 2016-04-02 00:00:00 ... [2016-05-04 06:02:21,053] {sensors.py:188} INFO - Poking for 1_hourly_agg.print_hourly1 on 2016-04-02 00:00:00 ... }} {noformat} How can I solve this dead-lock? In Addition- I didn't understand if it means that the daily dag depends only on the ""last"" hourly dag of the same day (23-24pm)? What happens if the hourly dag of other hour fails? Thanks a lot!"
401,AIRFLOW-48,AIRFLOW,1462384561000,1526460854000,Improvement,Resolved,Major,3,860,"can't pass extra parameters via a connection URI when using environment variables (and therefore can't tell Postgres to use SSL) We're trying to use a postgres connection to hook into our Redshift backend (which has sslmode set to 'require'). Since we're using chef to deploy we've got the connection string stored in an environment variable, but it looks like the Connection model doesn't parse any connection parameters when it gets initialized with a uri (https://github.com/airbnb/airflow/blob/3c3f5a67ff80f3e8942aef441f481c62baf97184/airflow/models.py#L529). We can't find a way to tell postgres to use SSL without setting sslmode via the connection string, but airflow doesn't seem to allow this, even though you can do it through the UI. Is there something we're missing? Has anybody else run into this using Chef + Postgres + SSL? Thanks for your help!"
402,AIRFLOW-50,AIRFLOW,1462432546000,1462468972000,Bug,Closed,Major,3,173,cx_Oracle insert is not performant What did you expect to happen? OracleHook should allow me to insert 1 million rows quickly (2-4min) What happened instead? It's very slow.
403,AIRFLOW-52,AIRFLOW,1462471729000,1561366458000,Task,Closed,Major,3,788,Release airflow 1.7.1 Release the airflow 1.7.1 tag. Current status: There are three issues blocking this release caused by this commit: https://github.com/apache/incubator-airflow/commit/fb0c5775cda4f84c07d8d5c0e6277fc387c172e6 -1. DAGs with a lot of tasks take much longer to parse (~25x slowdown)- -2. The following kind of patterns fail:- {code} email.set_upstream(dag.roots) dag.add_task(email) {code} This is because set_upstream now calls add_task and a task can't be added more than once. -3. Airflow losing queued tasks (see linked issue)- -4. Airflow putting dags in a stuck state (AIRFLOW-92)- I'm working with the owner of the commit to resolve these issues. The way to catch (1) in the future is an integration test that asserts a given non-trivial DAG parses under X seconds
404,AIRFLOW-53,AIRFLOW,1462480875000,1462551719000,New Feature,Closed,Major,3,59,Add CLI DagBag loading stats option to list_dags subcommand
405,AIRFLOW-55,AIRFLOW,1462524581000,1585329640000,New Feature,Closed,Major,3,113,Add HDFS Log Support To support save task logs on HDFS. PR: https://github.com/apache/incubator-airflow/pull/1409
406,AIRFLOW-61,AIRFLOW,1462546366000,1462837960000,Bug,Closed,Major,3,723,"Corner case with deadlocked multiprocessing queues I've never experienced this before, but it came up while testing this morning. Joining a process that puts items in a queue can apparently lead to deadlocks. This happens in the multiprocessing-enabled scheduler. See for example: https://docs.python.org/3/library/multiprocessing.html#all-start-methods (""Joining processes that use queues"") and related issues: http://stackoverflow.com/questions/31665328/python-3-multiprocessing-queue-deadlock-when-calling-join-before-the-queue-is-em http://stackoverflow.com/questions/31708646/process-join-and-queue-dont-work-with-large-numbers http://stackoverflow.com/questions/19071529/python-multiprocessing-125-list-never-finishes"
407,AIRFLOW-62,AIRFLOW,1462565791000,1481902378000,Bug,Closed,Major,3,1288,"XCom push not working reliably I have a DAG that polls for activity in various data streams from a database and then uploads the activity statuses to a table. Each of the polling tasks are python operators that once they get the polling result, return a dict as an XCom push. The dict contains two entries which are strings, one which is a bool, and one which is a datetime object. There is a final task that pulls all the results and uploads the collective statuses to a table. I chose this pattern since I figured it might be better to do one collective write operation on all the results. Before I moved ahead to the github master branch I was using 1.7.0 from PyPI and this worked fine. Now that I am on the github master branch, I find that the XCom pushing is unreliable. The returned values in the logs show up correctly but when doing the XCom pull, I get None for some of the returned values. Investigating the XCom result in the Webserver also shows nothing there. But if I rerun a task where the XCom failed, the push works and the XCom result is as it should be. Nothing appears to have changed in the codebase so I am at a loss. Perhaps it really wasn't working before? How would the backing postgres handle these simultaneous writes? I can't imagine that would be a problem."
408,AIRFLOW-64,AIRFLOW,1462571211000,1465060014000,Bug,Resolved,Major,3,786,"dags_folder not handled correctly running `airflow list_dags` with the command above from a location with a `dags` subdirectory, the cli erros the following message: ``` airflow.utils.AirflowException: subdir has to be part of your DAGS_FOLDER as defined in your airflow.cfg ``` if I change the airflow.cfg to `dags_folder = dags`, list_dags seem to work correctly, but any other task doesn't seem to handle the dags_folder correctly. Here is what `airflow scheduler` outputs when it tries to execute a scheduled dag: ``` DAG [process_deleted_runs] could not be found in /Users/giacomo/workspace/qsi/scheduler/dags/Users/giacomo/workspace/qsi/scheduler/process_deleted_runs.py ``` As you see, it seems that the absolute path (`/Users/giacomo/workspace/qsi/scheduler/`) is chained twice."
409,AIRFLOW-67,AIRFLOW,1462638816000,1536008058000,Bug,Closed,Major,3,921,"Dag is not avaialble on UI Dear Airflow Maintainers, Before I tell you about my issue, let me describe my Airflow environment: Airflow version: 1.7.0 Airflow components: CeleryExecutor Python Version: 2.7.6 Operating System: Centos Description of Issue Now that you know a little about me, let me tell you about the issue I am having: I am not able to see a new dag I wrote on the web-UI. The dag shows up in the *airflow list_dags* output. I am able to see all other dags that I wrote previously. Though, the dag starts executing because some of the tasks move into *running* state. The new dag is not click-able and has the below info. {quote} This DAG isn't available in the web server's DagBag object. It shows up in this list because the scheduler marked it as active in the metadata database. {quote} I had similar experience with a dag I published before this one. But, that one became available after about a day."
410,AIRFLOW-68,AIRFLOW,1462661235000,1535980972000,Sub-task,Closed,Major,3,43,Align start_date with the schedule_interval
411,AIRFLOW-70,AIRFLOW,1462661301000,1585613964000,Sub-task,Closed,Major,3,43,Use dag_run_id in TaskInstances for lineage
412,AIRFLOW-72,AIRFLOW,1462706315000,1585613987000,Improvement,Closed,Major,3,1163,"Implement proper capacity scheduler The scheduler is supposed to maintain queues and pools according to a ""capacity"" model. However it is currently not properly implemented as therefore issues as being able to oversubscribe to pools exist, race conditions for queuing/dequeuing exist and probably others. This Jira Epic is to track all related issues to pooling/queuing and the (tbd) roadmap to a proper capacity scheduler. Why queuing / scheduling broken: Locking is not properly implemented and cannot be as a check for slot availability is spread throughout the scheduler, taskinstance and executor. This makes obtaining a slot non-atomic and results in over subscribing. In addition it leads to race conditions as having two tasks being picked from the queue at the same time as the scheduler determines that a queued task still needs to be send to the executor, while in an earlier run this already happened. In order to fix this Pool handling needs to be centralized (code wise) and work with a mutex (with_for_update()) on the database records. The scheduler/taskinstance can then do something like: slot = Pool.obtain_slot(pool_id) Pool.release_slot(slot)"
413,AIRFLOW-74,AIRFLOW,1462721091000,1534928276000,Bug,Resolved,Major,3,3025,"SubdagOperators can consume all celeryd worker processes If the amount of concurrent ```SubdagOperator``` running >= the no. of celery worker processes tasks are unable to work. All SDOs come to a complete halt. Futhermore performance of a DAG is drastically reduced even before full saturation of the workers as less workers are gradually available for actual tasks. A workaround for this is to specify ```SequentialExecutor``` be used by the ```SubdagOperator``` ``` from datetime import timedelta, datetime from airflow.models import DAG, Pool from airflow.operators import BashOperator, SubDagOperator, DummyOperator from airflow.executors import SequentialExecutor import airflow # -----------------------------------------------------------------\ # DEFINE THE POOLS # -----------------------------------------------------------------/ session = airflow.settings.Session() for p in ['test_pool_1', 'test_pool_2', 'test_pool_3']: pool = ( session.query(Pool) .filter(Pool.pool == p) .first()) if not pool: session.add(Pool(pool=p, slots=8)) session.commit() # -----------------------------------------------------------------\ # DEFINE THE DAG # -----------------------------------------------------------------/ # Define the Dag Name. This must be unique. dag_name = 'hanging_subdags_n16_sqe' # Default args are passed to each task default_args = { 'owner': 'Airflow', 'depends_on_past': False, 'start_date': datetime(2016, 04, 10), 'retries': 0, 'retry_interval': timedelta(minutes=5), 'email': ['your@email.com'], 'email_on_failure': True, 'email_on_retry': True, 'wait_for_downstream': False, } # Create the dag object dag = DAG(dag_name, default_args=default_args, schedule_interval='0 0 * * *' ) # -----------------------------------------------------------------\ # DEFINE THE TASKS # -----------------------------------------------------------------/ def get_subdag(dag, sd_id, pool=None): subdag = DAG( dag_id='{parent_dag}.{sd_id}'.format( parent_dag=dag.dag_id, sd_id=sd_id), params=dag.params, default_args=dag.default_args, template_searchpath=dag.template_searchpath, user_defined_macros=dag.user_defined_macros, ) t1 = BashOperator( task_id='{sd_id}_step_1'.format( sd_id=sd_id ), bash_command='echo ""hello"" && sleep 60', dag=subdag, pool=pool, executor=SequentialExecutor ) t2 = BashOperator( task_id='{sd_id}_step_two'.format( sd_id=sd_id ), bash_command='echo ""hello"" && sleep 15', dag=subdag, pool=pool, executor=SequentialExecutor ) t2.set_upstream(t1) sdo = SubDagOperator( task_id=sd_id, subdag=subdag, retries=0, retry_delay=timedelta(seconds=5), dag=dag, depends_on_past=True, ) return sdo start_task = DummyOperator( task_id='start', dag=dag ) for n in range(1, 17): sd_i = get_subdag(dag=dag, sd_id='level_1_{n}'.format(n=n), pool='test_pool_1') sd_ii = get_subdag(dag=dag, sd_id='level_2_{n}'.format(n=n), pool='test_pool_2') sd_iii = get_subdag(dag=dag, sd_id='level_3_{n}'.format(n=n), pool='test_pool_3') sd_i.set_upstream(start_task) sd_ii.set_upstream(sd_i) sd_iii.set_upstream(sd_ii) ```"
414,AIRFLOW-79,AIRFLOW,1462809046000,1585330346000,New Feature,Closed,Major,3,197,End-to-end DAG testing This is the placeholder issue for the End-to-end DAG testing framework that we are proposing to make airflow more robust. https://github.com/apache/incubator-airflow/pull/748
415,AIRFLOW-80,AIRFLOW,1462818811000,1462831773000,Bug,Closed,Major,3,1077,"`airflow initdb` fails for new environment I just created a new dev env with ""setup.py develop "" {code} (airflow)[scattaneo@scattaneo-mba ~/workspace/airflow]$ airflow initdb [2016-05-09 11:25:52,576] {__init__.py:36} INFO - Using executor SequentialExecutor DB: sqlite:////Users/scattaneo/airflow/airflow.db [2016-05-09 11:25:53,770] {db.py:222} INFO - Creating tables INFO [alembic.runtime.migration] Context impl SQLiteImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. ERROR [airflow.models.DagBag] Failed to import: /Users/scattaneo/workspace/airflow/incubator-airflow/airflow/example_dags/example_twitter_dag.py Traceback (most recent call last): File ""/Users/scattaneo/workspace/airflow/incubator-airflow/airflow/models.py"", line 250, in process_file m = imp.load_source(mod_name, filepath) File ""/Users/scattaneo/workspace/airflow/incubator-airflow/airflow/example_dags/example_twitter_dag.py"", line 26, in <module> from airflow.operators import BashOperator, HiveOperator, PythonOperator ImportError: cannot import name HiveOperator Done. {code}"
416,AIRFLOW-84,AIRFLOW,1462832218000,1556710839000,Bug,Closed,Major,3,1667,"Queued tasks in Airflow not being prioritized and scheduled We consistently hit an issue that Airflow didn't schedule any tasks/jobs for several hours until we restart worker & scheduler. We're on 1.6.2 with celeryd enabled. From scheduler logs, we didn't observe any suspicious error messages. Other than during the entire time period (several hours), it's always {quote} Prioritizing 0 queued jobs. {quote} While in the worker logs, we did find error messages like following. The message is sort of generic, so not sure anything quite useful there. But wondering what might cause MainProcess keeps failing in celery_executor? e.g. OOM on the client machine etc.? {quote} 2016-05-04T07:44:10.10441 2016-05-04 07:44:10,104: ERROR/MainProcess Task airflow.executors.celery_executor.execute_commanda900d1f1-6abf-4d13-a403-7ec884abd565 raised unexpected: AirflowException('Celery command failed',) 2016-05-04T07:44:10.10442 Traceback (most recent call last): 2016-05-04T07:44:10.10442 File ""/usr/local/lib/service_venv/local/lib/python2.7/site-packages/celery/app/trace.py"", line 240, in trace_task 2016-05-04T07:44:10.10442 R = retval = fun(*args, **kwargs) 2016-05-04T07:44:10.10442 File ""/usr/local/lib/service_venv/local/lib/python2.7/site-packages/celery/app/trace.py"", line 438, in protected_call 2016-05-04T07:44:10.10443 return self.run(*args, **kwargs) 2016-05-04T07:44:10.10443 File ""/usr/local/lib/service_venv/local/lib/python2.7/site-packages/airflow/executors/celery_executor.py"", line 43, in execute_command 2016-05-04T07:44:10.10443 raise AirflowException('Celery command failed') 2016-05-04T07:44:10.10443 AirflowException: Celery command failed {quote}"
417,AIRFLOW-85,AIRFLOW,1462832682000,1589992983000,Bug,Resolved,Major,3,1556,"Create DAGs UI Airflow currently provides only an {{/admin}} UI interface for the webapp. This UI provides three distinct roles: * Admin * Data profiler * None In addition, Airflow currently provides the ability to log in, either via a secure proxy front-end, or via LDAP/Kerberos, within the webapp. We run Airflow with LDAP authentication enabled. This helps us control access to the UI. However, there is insufficient granularity within the UI. We would like to be able to grant users the ability to: # View their DAGs, but no one else's. # Control their DAGs, but no one else's. This is not possible right now. You can take away the ability to access the connections and data profiling tabs, but users can still see all DAGs, as well as control the state of the DB by clearing any DAG status, etc. (From Airflow-1443) The authentication capabilities in the [RBAC design proposal|https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+RBAC+proposal] introduces a significant amount of work that is otherwise already built-in in existing frameworks. Per [community discussion|https://www.mail-archive.com/dev@airflow.incubator.apache.org/msg02946.html], Flask-AppBuilder (FAB) is the best fit for Airflow as a foundation to implementing RBAC. This will support integration with different authentication backends out-of-the-box, and generate permissions for views and ORM models that will simplify view-level and dag-level access control. This implies modifying the current flask views, and deprecating the current Flask-Admin in favor of FAB's crud."
418,AIRFLOW-86,AIRFLOW,1462842615000,1463610772000,Bug,Closed,Major,3,223,Python3: UI crash without wrapping dict.items() in list Found while investigating AIRFLOW-62. Randomly causes a catastrophic failure with Postgres backend when sqlalchemy tries to write the generator object to the database.
419,AIRFLOW-94,AIRFLOW,1462887859000,1561559215000,Bug,Closed,Major,3,493,"'Mark upstream tasks as successful' does not actually select upstream tasks Affected version: {{dddfd3b5bf2cabaac6eec123dfa3cb59e73a56f5}} Steps to reproduce: 1. Add any DAG with a task depending on another 2. Go to the web UI's tree view 3. Click the depending task's latest execution (make sure the upstream task is not successful yet) 4. Click the 'Upstream' button next 'Mark Success', then click 'Mark Success' Only the depending task will be selected, its dependencies will not be there."
420,AIRFLOW-95,AIRFLOW,1462888140000,1561559299000,Bug,Closed,Major,3,2039,"'Mark downstream tasks as successful' raises an exception Affected version: {{dddfd3b5bf2cabaac6eec123dfa3cb59e73a56f5}} Steps to reproduce: 1. Add any DAG with a task depending on another 2. Go to the web UI's tree view 3. Click the depended upon task's latest execution 4. Click the 'Downstream' button next 'Mark Success', then click 'Mark Success' This gives me the following exception (but only if there's at least one downstream task that should be selected): {code} Traceback (most recent call last): File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1817, in wsgi_app response = self.full_dispatch_request() File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1477, in full_dispatch_request rv = self.handle_user_exception(e) File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1381, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/usr/local/lib/python3.5/site-packages/flask/_compat.py"", line 33, in reraise raise value File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1475, in full_dispatch_request rv = self.dispatch_request() File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1461, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/usr/local/lib/python3.5/site-packages/flask_admin/base.py"", line 68, in inner return self._run_view(f, *args, **kwargs) File ""/usr/local/lib/python3.5/site-packages/flask_admin/base.py"", line 367, in _run_view return fn(self, *args, **kwargs) File ""/usr/local/lib/python3.5/site-packages/flask_login.py"", line 755, in decorated_view return func(*args, **kwargs) File ""/usr/local/lib/python3.5/site-packages/airflow/www/utils.py"", line 118, in wrapper return f(*args, **kwargs) File ""/usr/local/lib/python3.5/site-packages/airflow/www/utils.py"", line 167, in wrapper return f(*args, **kwargs) File ""/usr/local/lib/python3.5/site-packages/airflow/www/views.py"", line 1185, in success task=task_id_to_dag[task_id].get_task(task_id), KeyError: 'id_of_downstream_task' {code}"
421,AIRFLOW-98,AIRFLOW,1462902754000,1585614006000,Bug,Closed,Major,3,245,Using Flask extensions from a plugin I am creating a plugin which should be able to use some Flask extensions. Is there any way to do this? Basically I need to import the airflow.www.app.app object in the plugin to initialise the extension with.
422,AIRFLOW-99,AIRFLOW,1462906567000,1462923501000,Bug,Closed,Major,3,344,"Scheduler bug causing dead-lock I have 2 DAGs - hourly DAG and Daily DAG. The Daily DAG depends on the hourly (using ExternalTaskSensor). The Daily DAG instances consume all the resources, and all of them are in ""running"" state - waiting for the hourly DAG to be executed. DAG that wait for something to happen should move to the waiting queue."
423,AIRFLOW-107,AIRFLOW,1462996207000,1569250183000,Bug,Closed,Major,3,1303,"syntax error when running airflow initdb with mysql connection Not sure if I've setup my mysql database correctly or not, assume that you just specify a connection string to a mysql server and port? In any case, I'm getting a syntax error when running 'airflow initdb' {code} cursor.execute(statement, parameters) File ""build/bdist.linux-x86_64/egg/MySQLdb/cursors.py"", line 226, in execute File ""build/bdist.linux-x86_64/egg/MySQLdb/connections.py"", line 36, in defaulterrorhandler sqlalchemy.exc.ProgrammingError: (_mysql_exceptions.ProgrammingError) (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'connection (\n\tid INTEGER NOT NULL AUTO_INCREMENT, \n\tconn_id VARCHAR(250), \n\tconn' at line 1"") [SQL: u'\nCREATE TABLE connection (\n\tid INTEGER NOT NULL AUTO_INCREMENT, \n\tconn_id VARCHAR(250), \n\tconn_type VARCHAR(500), \n\thost VARCHAR(500), \n\t`schema` VARCHAR(500), \n\tlogin VARCHAR(500), \n\tpassword VARCHAR(500), \n\tport INTEGER, \n\textra VARCHAR(5000), \n\tPRIMARY KEY (id)\n)\n\n'] {code} python 2.7.11, mysqlclient 1.3.7, sqlalchemy 1.0.12 mysql Ver 14.14 Distrib 5.5.14, for Linux (x86_64) using EditLine wrapper Server version: 5.0.27-max MySQL Community Edition - Experimental (GPL)"
424,AIRFLOW-108,AIRFLOW,1463067681000,1585351289000,Wish,Closed,Major,3,591,"Add data retention policy to Airflow Airflow's DB currently holds the entire history of all executions for all time. This is problematic as the DB grows. The UI starts to get slower, and the DB's disk usage grows. There is no bound to how large the DB will grow. It would be useful to add a feature in Airflow to do two things: # Delete old data from the DB # Mark some lower watermark, past which DAG executions are ignored For example, (2) would allow you to tell the scheduler ""ignore all data prior to a year ago"". And (1) would allow Airflow to delete all data prior to January 1, 2015."
425,AIRFLOW-111,AIRFLOW,1463171087000,1491859698000,Sub-task,Resolved,Major,3,999,"DAG concurrency is not honored Description of Issue In airflow.cfg, we set: max_active_runs_per_dag = 1 In our dag, we set the dag_args['concurrency'] = 8, however, when the scheduler starts to run, we can see this concurrency is not being honored, airflow scheduler will run up to num of the 'parallelism' (we set as 25) task instances for the ONE run dag_run. What did you expect to happen? dag_args['concurrency'] = 8 is honored, e.g. only run at most 8 task instances concurrently. What happened instead? when the dag starts to run, we can see the concurrency is not being honored, airflow scheduler/celery worker will run up to the 'parallelism' (we set as 25) task instances. Here is how you can reproduce this issue on your machine: create a dag which contains nothing but 25 parallelized tasks. set the dag dag_args['concurrency'] = 8 set the airflow parallelism = 25, and max_active_runs_per_dag = 1 then run: airflow scheduler you will see all 25 task instance are scheduled to run, not 8."
426,AIRFLOW-113,AIRFLOW,1463171394000,1463181006000,Improvement,Closed,Major,3,913,"DAG concurrency is not honored This is reported in airflow github also: https://github.com/apache/incubator-airflow/issues/1424 In our dag, we set the dag_args['concurrency'] = 8, however, when the scheduler starts to run, we can see this concurrency is not being honored, airflow scheduler will run up to num of the 'parallelism' (we set as 25) jobs. What did you expect to happen? dag_args['concurrency'] = 8 is honored, e.g. only run at most 8 jobs concurrently. What happened instead? when the dag starts to run, we can see the concurrency is not being honored, airflow scheduler will run up to the 'parallelism' (we set as 25) jobs. Here is how you can reproduce this issue on your machine: create a dag which contains nothing but 25 parallelized jobs. set the dag dag_args['concurrency'] = 8 set the airflow parallelism to 25 then run: airflow scheduler you will see all 25 jobs are scheduled to run, not 8."
427,AIRFLOW-118,AIRFLOW,1463271241000,1466872071000,Improvement,Closed,Major,3,524,"use targetPartitionSize as the default partition spec for HiveToDruidTransfer operator The definition of the two partition spec can be found http://druid.io/docs/latest/ingestion/batch-ingestion.html. Originally, the HiveToDruidTransfer uses numShards. The disadvantage of that is users need to tune the numbers repeatedly, and do that again when the data size changes. This is not scalable as the number of data sources grows. targetPartitionSize approach calculates the number of segments automatically and is hassle free."
428,AIRFLOW-119,AIRFLOW,1463388070000,1463758115000,Bug,Closed,Major,3,297,"""Template Not Found"" error from QuboleOperator From the given example set, when I am using qbol operator for a hive workload, whose script resides in s3 and ends with "".qbl"", I am getting ""Template Not Found"" error. Also it would be nice, if airflow always tags commands going from airflow to qds."
429,AIRFLOW-120,AIRFLOW,1463388136000,1463409913000,Bug,Closed,Major,3,297,"""Template Not Found"" error from QuboleOperator From the given example set, when I am using qbol operator for a hive workload, whose script resides in s3 and ends with "".qbl"", I am getting ""Template Not Found"" error. Also it would be nice, if airflow always tags commands going from airflow to qds."
430,AIRFLOW-124,AIRFLOW,1463469661000,1535981047000,Improvement,Resolved,Major,3,97,Implement create_dagrun Dag runs should be created by the dag to ensure tasks are attached to it.
431,AIRFLOW-126,AIRFLOW,1463516907000,1579476988000,Improvement,Closed,Major,3,321,"Client facing functions should not raise exceptions but have proper exit codes Many client facing functions (cli) raise exceptions on failure, which is not very user friendly. It is better to define a set of exit codes that can be used on failure (sys.exit) and print out a proper message. Logging can occur to log files."
432,AIRFLOW-128,AIRFLOW,1463567207000,1537525872000,Improvement,Closed,Major,3,845,"Optimize and refactor process_dag process_dag is currently taskinstance based and programmatically determines which tasks should be part of a ""dagrun"" (between quotes as it is not a real dagrun). This requires a round trip to the database for every task, easily touching 10-20 per dag per execution_ date every heartbeat or even higher for more complex dags. In addition the session is not reused within process_dag thus for every dag it will open 10-20 sessions per execution_date every heartbeat. This is suboptimal. Using dag runs that are instantiated with their associated tasks (see AIRFLOW-124) it can be reduced to one roundtrip per dagrun. Lowering the pressure on the db significantly, in addition if using the database session carefully it can be done within one session further lowering the db pressure and speeding up the scheduler."
433,AIRFLOW-129,AIRFLOW,1463575616000,1535980226000,Improvement,Resolved,Major,3,312,Allow CELERYD_PREFETCH_MULTIPLIER to be configurable Airflow needs to allow everyone to customise their prefetch limit. Some might have short running task and don't want the overhead of celery latency. More on that here: http://docs.celeryproject.org/en/latest/userguide/optimizing.html#optimizing-prefetch-limit
434,AIRFLOW-130,AIRFLOW,1463591557000,1479273495000,Bug,Resolved,Major,3,2058,"SSHHook fails in OSX due to mktemp implementation h3. Description of Issue * What did you expect to happen? In the {{__enter__}} method in the {{SSHTempFileContent}} class ({{ssh_execute_operator.py}}) a file should be created using {{mktemp}} * What happened instead? The call to mktemp fails, because the {{--tmpdir}} command line option is not supported in OSX (see https://developer.apple.com/legacy/library/documentation/Darwin/Reference/ManPages/man1/mktemp.1.html) * Stack trace, if appropriate: {noformat} [2016-05-18 17:39:31,735] {models.py:1144} ERROR - Failed to create remote temp file Traceback (most recent call last): File ""/Users/rlanda/.virtualenvs/datapath-airflow/bin/airflow"", line 15, in <module> args.func(args) File ""/Users/rlanda/.virtualenvs/datapath-airflow/lib/python2.7/site-packages/airflow/bin/cli.py"", line 316, in test ti.run(force=True, ignore_dependencies=True, test_mode=True) File ""/Users/rlanda/.virtualenvs/datapath-airflow/lib/python2.7/site-packages/airflow/models.py"", line 1067, in run result = task_copy.execute(context=context) File ""/Users/rlanda/.virtualenvs/datapath-airflow/lib/python2.7/site-packages/airflow/contrib/operators/ssh_execute_operator.py"", line 116, in execute self.task_id) as remote_file_path: File ""/Users/rlanda/.virtualenvs/datapath-airflow/lib/python2.7/site-packages/airflow/contrib/operators/ssh_execute_operator.py"", line 52, in __enter__ raise AirflowException(""Failed to create remote temp file"") airflow.utils.AirflowException: Failed to create remote temp file {noformat} The stack trace is not very useful. However looking at the tuple returned by {{pmktemp.communicate()}} is helpful: {noformat} mktemp: illegal option -- - usage: mktemp [-d] [-q] [-t prefix] [-u] template ... mktemp [-d] [-q] [-u] -t prefix {noformat} h3. Reproducing the Issue Issue happens with normal usage of {{SSHHook}} when attempting to run commands on an OSX box. h3. Fix Changing {{--tmpdir}} by {{-t}} seems to fix the issue in OSX, but I do not have access to a linux box to check if it breaks there."
435,AIRFLOW-131,AIRFLOW,1463591677000,1465249895000,Bug,Resolved,Major,3,388,"XCom data is cleared even if a task already succeeded In {{TaskInstance.run()}}, the XCom data is immediately cleared. It should not be cleared until we determine that the task is actually going to run. For example, if the task already succeeded it won't run and we want the XCom data to remain. Reported here: https://github.com/apache/incubator-airflow/issues/999#issuecomment-220047510"
436,AIRFLOW-132,AIRFLOW,1463593164000,1463593340000,Bug,Closed,Major,3,78,Adding NerdWallet to company list Adding NerdWallet as a company using Airflow
437,AIRFLOW-134,AIRFLOW,1463609663000,1463769742000,Improvement,Closed,Major,3,89,Improve GitHub PR workflow Merging PRs is hard to do and easy to mess up. Make it better!
438,AIRFLOW-136,AIRFLOW,1463641255000,1463642406000,Bug,Closed,Major,3,119,Increasing License Coverage Noticed some travis failures on license check. Increasing coverage to quiet those failures.
439,AIRFLOW-139,AIRFLOW,1463657361000,1485381608000,Bug,Resolved,Major,3,1873,"Executing VACUUM with PostgresOperator Dear Airflow Maintainers, h1. Environment * Airflow version: *v1.7.0* * Airflow components: *PostgresOperator* * Python Version: *Python 3.5.1* * Operating System: *15.4.0 Darwin* h1. Description of Issue I am trying to execute a `VACUUM` command as part of DAG with the `PostgresOperator`, which fails with the following error: {quote} [2016-05-14 16:14:01,849] {__init__.py:36} INFO - Using executor SequentialExecutor Traceback (most recent call last): File ""/usr/local/bin/airflow"", line 15, in <module> args.func(args) File ""/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/airflow/bin/cli.py"", line 203, in run pool=args.pool, File ""/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/airflow/models.py"", line 1067, in run result = task_copy.execute(context=context) File ""/usr/local/lib/python3.5/site-packages/airflow/operators/postgres_operator.py"", line 39, in execute self.hook.run(self.sql, self.autocommit, parameters=self.parameters) File ""/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/airflow/hooks/dbapi_hook.py"", line 109, in run cur.execute(s) psycopg2.InternalError: VACUUM cannot run inside a transaction block {quote} I could create a small python script that performs the operation, as explained in [this stackoverflow entry](http://stackoverflow.com/questions/1017463/postgresql-how-to-run-vacuum-from-code-outside-transaction-block). However, I would like to know first if the `VACUUM` command should be supported by the `PostgresOperator`. h1. Reproducing the Issue The operator can be declared as follows: {quote} conn = ('postgres_default') t4 = PostgresOperator( task_id='vacuum', postgres_conn_id=conn, sql=(""VACUUM public.table""), dag=dag ) {quote}"
440,AIRFLOW-144,AIRFLOW,1463680053000,1463720056000,Bug,Closed,Major,3,63,Add Enhancements to VersionView Add enhancements to VersionView
441,AIRFLOW-145,AIRFLOW,1463683355000,1463719647000,Bug,Closed,Major,3,1493,"trigger_rule='one_success' not allowing tasks downstream of a BranchPythonOperator to be executed h3.Background We are constructing a workflow to automate standard business processes around the creation and maintenance of virtual machines. After creation, we verify several information points on the VM to ensure that it is a viable machine and that no configuration errors occurred. If it fails verification and is not running, then it should be deleted. If it fails verification and is running, then we stop it first, then delete it. h3.What did you expect to happen? After researching the BranchPythonOperator, I found that I should be using trigger_rule='one_success' to allow a task at a join point downstream of the branch(es) to be triggered, as mentioned in #1078. So, I defined the task as follows: {code} delete_vm = PythonOperator( task_id='delete_vm', trigger_rule=TriggerRule.ONE_SUCCESS, python_callable=_delete_vm, provide_context=True, dag=dag) delete_vm.set_upstream({poll_vm_stop, verify_vm}) {code} h3.What happened instead? Rather than executing correctly, the delete_vm task is marked as skipped and is not re-evaluated following poll_vm_stop. There is no stack trace available, as the task simply does not execute. Sidenote: the PythonSensor you see in the attached picture is a sensor which evaluates the truthy- or falsey-ness of a Python callable. It has been tested extensively and works as intended. reference: https://github.com/apache/incubator-airflow/issues/1521"
442,AIRFLOW-146,AIRFLOW,1463685184000,1585614078000,Bug,Closed,Major,3,467,"Clear status button on task instance page It would be nice to be able to clear the status (including past/future, upstream/downstream) of a task from the task instance page, similar to the way you can when you click on a task in the tree view. The use case is that we had an outage on one of our systems, so several hundred tasks failed. We wanted to clear all of their statuses, including their downstream tasks. We had no way to do this without doing one at a time."
443,AIRFLOW-149,AIRFLOW,1463692146000,1472249415000,New Feature,Resolved,Major,3,442,"Blocked TI Explainer + Dependency Engine Created from: https://github.com/apache/incubator-airflow/issues/1383 - Simplify, consolidate, and make consistent the logic of whether or not a task should be run - Provide a view that gives insight into why a task instance is not currently running (no more viewing the scheduler logs to find out why a task instance isn't running for the majority of cases) e.g. (this will not be the final product):"
444,AIRFLOW-150,AIRFLOW,1463704223000,1535981020000,Bug,Closed,Major,3,171,"setup.py classifiers should be a list not a dict I ran into this issue when trying to release to pypi, setuptools expects classifiers in setup.py to be a list, not a dict."
445,AIRFLOW-151,AIRFLOW,1463714599000,1464017656000,Bug,Closed,Major,3,2100,"trigger_rule='one_success' not allowing tasks downstream of a BranchPythonOperator to be executed Porting from https://github.com/apache/incubator-airflow/issues/1521 Dear Airflow Maintainers, *Environment* {panel} Airflow version: 1.7.0rc3 Airflow components: webserver, scheduler, worker, postgres database, CeleryExecutor Relevant airflow.cfg settings: nothing special here; mostly defaults Python Version: 3.4.3 Operating System: Centos 6.7 Python packages: virtualenv with standard airflow install {panel} *Background* We are constructing a workflow to automate standard business processes around the creation and maintenance of virtual machines. After creation, we verify several information points on the VM to ensure that it is a viable machine and that no configuration errors occurred. If it fails verification and is not running, then it should be deleted. If it fails verification and is running, then we stop it first, then delete it. *What did you expect to happen?* After researching the BranchPythonOperator, I found that I should be using trigger_rule='one_success' to allow a task at a join point downstream of the branch(es) to be triggered, as mentioned in #1078. So, I defined the task as follows: {code} delete_vm = PythonOperator( task_id='delete_vm', trigger_rule=TriggerRule.ONE_SUCCESS, python_callable=_delete_vm, provide_context=True, dag=dag) delete_vm.set_upstream({poll_vm_stop, verify_vm}) {code} *What happened instead?* Rather than executing correctly, the delete_vm task is marked as skipped and is not re-evaluated following poll_vm_stop. There is no stack trace available, as the task simply does not execute. Sidenote: the PythonSensor you see in the picture below is a sensor which evaluates the truthy- or falsey-ness of a Python callable. It has been tested extensively and works as intended. !DAG_Problem.png! Any help would be greatly appreciated. I've tested various ways of linking the dag, providing DummyOperators as buffers, using a second BranchPythonOperator to explicitly call the task; all of these have failed. Am I missing something obvious here?"
446,AIRFLOW-155,AIRFLOW,1463757818000,1535980918000,Task,Closed,Major,3,138,Documentation for Qubole Operator Creating placeholder ticket for documentation work related to Qubole operator. Please assign this to me.
447,AIRFLOW-158,AIRFLOW,1463775416000,1463778818000,Task,Resolved,Major,3,118,Test issue (please disregard!) This issue is for testing the PR-Tool's ability to close JIRA issues. Please disregard!
448,AIRFLOW-159,AIRFLOW,1463920530000,1472994942000,Task,Resolved,Major,3,320,"Documentation: Cloud integration : GCP Start documenting all GCP operators and hooks. I propose a new top-level documentation section that's called ""Integration"". Under that section I would make a sub-section ""Google Cloud Platform"". This way other Cloud integration can be documented as well in the Integration section."
449,AIRFLOW-160,AIRFLOW,1463965040000,1535980996000,Improvement,Closed,Major,3,1339,"Parse DAG files through child processes Currently, the Airflow scheduler parses all user DAG files in the same process as the scheduler itself. We've seen issues in production where bad DAG files cause scheduler to fail. A simple example is if the user script calls `sys.exit(1)`, the scheduler will exit as well. We've also seen an unusual case where modules loaded by the user DAG affect operation of the scheduler. For better uptime, the scheduler should be resistant to these problematic user DAGs. The proposed solution is to parse and schedule user DAGs through child processes. This way, the main scheduler process is more isolated from bad DAGs. There's a side benefit as well - since parsing is distributed among multiple processes, it's possible to parse the DAG files more frequently, reducing the latency between when a DAG is modified and when the changes are picked up. Another issue right now is that all DAGs must be scheduled before any tasks are sent to the executor. This means that the frequency of task scheduling is limited by the slowest DAG to schedule. The changes needed for scheduling DAGs through child processes will also make it easy to decouple this process and allow tasks to be scheduled and sent to the executor in a more independent fashion. This way, overall scheduling won't be held back by a slow DAG."
450,AIRFLOW-161,AIRFLOW,1463987080000,1561372555000,Improvement,Resolved,Major,3,518,"Redirection to external url Hi, I am not able to find a good way (apart from loading everything upfront), where I can redirect someone to a external service url, using the information stored in airflow. There could be many use cases like downloading a signed file from s3, redirecting to hadoop job tracker, or a direct case on which I am working which is linking airflow tasks to qubole commands. I already have a working model and will open a PR soon. Please let me know if there existing ways already. Thanks, Sumit"
451,AIRFLOW-164,AIRFLOW,1463996876000,1579477021000,Improvement,Closed,Major,3,1103,"Disable the web UI's page load animations Alright, some people might disagree, looking forward to the discussion here. Basically my qualm is that opening almost any page (even the DAGs list) will trigger an animation where the content kinda swoops in from the left/the top/the top-left corner. This gets pretty annoying for a few reasons: - It takes around half a second before the content is visually parsable, and these half seconds accumulate pretty quick when doing lots of administration. - This makes visual diffing when refreshing or editing the URL impossible. If the animations weren't firing, it would be possible to refresh for instance the tree view of a complicated DAG and just see the treemap change, making the differences obvious. Currently you need to commit the state to memory and then recall it after the animation has finished to try and figure out what has changed. - I think it just makes no sense from a design point of view anyway to have all this data sliding around the screen. It's not like it passes off as a transition animation or anything. What does everyone else think?"
452,AIRFLOW-166,AIRFLOW,1464047386000,1543460022000,Bug,Closed,Major,3,4047,"Webserver times out using systemd script I just upgraded to 1.7.1.2 from 1.6 and I'm having a problem starting the webserver using the systemd script. This used to work fine. The issue is that it starts and then just hangs, no error is reported and it finally times out after about a minute. I tried starting it from the command line and it works fine without timing out. I also ran it in daemon mode with -D and again it seems to be fine. Any thoughts on how to debug this? Here's the log output: {code} May 23 16:27:50 ybendana-linux systemd: Starting Airflow webserver daemon... May 23 16:27:51 ybendana-linux airflow: [2016-05-23 16:27:51,444] {__init__.py:36} INFO - Using executor LocalExecutor May 23 16:27:53 ybendana-linux airflow: ____________ _____________ May 23 16:27:53 ybendana-linux airflow: ____ |__( )_________ __/__ /________ __ May 23 16:27:53 ybendana-linux airflow: ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / May 23 16:27:53 ybendana-linux airflow: ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / May 23 16:27:53 ybendana-linux airflow: _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ May 23 16:27:53 ybendana-linux airflow: [2016-05-23 16:27:53,446] {models.py:154} INFO - Filling up the DagBag from /opt/airflow/dags May 23 16:27:55 ybendana-linux airflow: [2016-05-23 16:27:55 +0000] [15960] [INFO] Starting gunicorn 19.3.0 May 23 16:27:55 ybendana-linux airflow: [2016-05-23 16:27:55 +0000] [15960] [INFO] Listening at: http://0.0.0.0:8080 (15960) May 23 16:27:55 ybendana-linux airflow: [2016-05-23 16:27:55 +0000] [15960] [INFO] Using worker: sync May 23 16:27:55 ybendana-linux airflow: [2016-05-23 16:27:55 +0000] [16067] [INFO] Booting worker with pid: 16067 May 23 16:27:55 ybendana-linux airflow: [2016-05-23 16:27:55 +0000] [16069] [INFO] Booting worker with pid: 16069 May 23 16:27:55 ybendana-linux airflow: [2016-05-23 16:27:55 +0000] [16070] [INFO] Booting worker with pid: 16070 May 23 16:27:55 ybendana-linux airflow: [2016-05-23 16:27:55 +0000] [16071] [INFO] Booting worker with pid: 16071 May 23 16:27:55 ybendana-linux airflow: [2016-05-23 16:27:55,876] {__init__.py:36} INFO - Using executor LocalExecutor May 23 16:27:55 ybendana-linux airflow: [2016-05-23 16:27:55,950] {__init__.py:36} INFO - Using executor LocalExecutor May 23 16:27:55 ybendana-linux airflow: [2016-05-23 16:27:55,972] {__init__.py:36} INFO - Using executor LocalExecutor May 23 16:27:55 ybendana-linux airflow: [2016-05-23 16:27:55,997] {__init__.py:36} INFO - Using executor LocalExecutor May 23 16:27:57 ybendana-linux airflow: [2016-05-23 16:27:57,885] {models.py:154} INFO - Filling up the DagBag from /opt/airflow/dags May 23 16:27:57 ybendana-linux airflow: [2016-05-23 16:27:57,951] {models.py:154} INFO - Filling up the DagBag from /opt/airflow/dags May 23 16:27:57 ybendana-linux airflow: [2016-05-23 16:27:57,983] {models.py:154} INFO - Filling up the DagBag from /opt/airflow/dags May 23 16:27:58 ybendana-linux airflow: [2016-05-23 16:27:58,014] {models.py:154} INFO - Filling up the DagBag from /opt/airflow/dags May 23 16:29:20 ybendana-linux systemd: airflow-webserver.service start operation timed out. Terminating. May 23 16:29:20 ybendana-linux airflow: [2016-05-23 16:29:20 +0000] [16070] [INFO] Worker exiting (pid: 16070) May 23 16:29:20 ybendana-linux airflow: [2016-05-23 16:29:20 +0000] [15960] [INFO] Handling signal: term May 23 16:29:20 ybendana-linux airflow: [2016-05-23 16:29:20 +0000] [16071] [INFO] Worker exiting (pid: 16071) May 23 16:29:20 ybendana-linux airflow: [2016-05-23 16:29:20 +0000] [16069] [INFO] Worker exiting (pid: 16069) May 23 16:29:20 ybendana-linux airflow: [2016-05-23 16:29:20 +0000] [16067] [INFO] Worker exiting (pid: 16067) May 23 16:29:21 ybendana-linux airflow: [2016-05-23 16:29:21 +0000] [15960] [INFO] Shutting down: Master May 23 16:29:21 ybendana-linux systemd: Failed to start Airflow webserver daemon. May 23 16:29:21 ybendana-linux systemd: Unit airflow-webserver.service entered failed state. May 23 16:29:21 ybendana-linux systemd: airflow-webserver.service failed. {code}"
453,AIRFLOW-167,AIRFLOW,1464104161000,1535980991000,Improvement,Closed,Major,3,387,"Get dag state for a given execution date. I was trying to get state for a particular dag-run programmatically, but couldn't find a way. If we could have a rest call like `/admin/dagrun?dag_id=<dag>&execution_date=<ts>` and get the output that would be best. Currently we've to do html parsing to get the same. Other (and easier) way is to add a cli support like we have for `task_state`."
454,AIRFLOW-168,AIRFLOW,1464106691000,1464438329000,Bug,Resolved,Major,3,418,"schedule_interval @once scheduling dag atleast twice I was looking at example_xcom example and found that it got scheduled twice. Ones at the start_time and ones at the current time. To be correct I tried multiple times (by reloading db) and its same. I am on airflow master, using sequential executor with sqlite3. Though it works as expected on a prod env which is running v1.7 with celery workers and mysql backend."
455,AIRFLOW-169,AIRFLOW,1464107270000,1536013285000,Wish,Resolved,Major,3,91,Hide expire dags in UI It would be great if we've option to hide expired schedules from UI.
456,AIRFLOW-171,AIRFLOW,1464139341000,1535980933000,Bug,Closed,Major,3,1174,"Email does not work in 1.7.1.2 Job failure emails was working in 1.7.0. They seem to have stopped working in 1.7.1. Error is {quote} [2016-05-25 00:48:02,334] {models.py:1311} ERROR - Failed to send email to: ['email@email.com'] [2016-05-25 00:48:02,334] {models.py:1312} ERROR - 'module' object has no attribute 'send_email_smtp' Traceback (most recent call last): File ""/usr/local/lib/python2.7/site-packages/airflow/models.py"", line 1308, in handle_failure self.email_alert(error, is_retry=False) File ""/usr/local/lib/python2.7/site-packages/airflow/models.py"", line 1425, in email_alert send_email(task.email, title, body) File ""/usr/local/lib/python2.7/site-packages/airflow/utils/email.py"", line 42, in send_email backend = getattr(module, attr) AttributeError: 'module' object has no attribute 'send_email_smtp' {quote} File exists and method exists. Seems to work fine when called in python directly. Maybe it's loading the wrong email module. Tried to set PYTHONPATH to have /usr/local/lib/python2.7/site-packages/airflow earlier in the path, but that didn't seem to work either. Could this be related to the utils refactoring that happened between 1.7.0 and 1.7.1?"
457,AIRFLOW-172,AIRFLOW,1464153738000,1486323426000,Bug,Closed,Major,3,2963,"All example DAGs report ""Only works with the CeleryExecutor, sorry"" After installing airflow and trying to run some example DAGs I was faced with {{Only works with the CeleryExecutor, sorry}} on every DAG I tried to run. {code}$ pip list airflow (1.7.1.2) alembic (0.8.6) Babel (1.3) bitarray (0.8.1) cffi (1.6.0) chartkick (0.4.2) croniter (0.3.12) cryptography (1.3.2) dill (0.2.5) docutils (0.12) Flask (0.10.1) Flask-Admin (1.4.0) Flask-Cache (0.13.1) Flask-Login (0.2.11) Flask-WTF (0.12) funcsigs (0.4) future (0.15.2) google-apputils (0.4.2) gunicorn (19.3.0) hive-thrift-py (0.0.1) idna (2.1) impyla (0.13.7) itsdangerous (0.24) Jinja2 (2.8) lockfile (0.12.2) Mako (1.0.4) Markdown (2.6.6) MarkupSafe (0.23) mysqlclient (1.3.7) numpy (1.11.0) pandas (0.18.1) pip (8.1.2) ply (3.8) protobuf (2.6.1) pyasn1 (0.1.9) pycparser (2.14) Pygments (2.1.3) PyHive (0.1.8) pykerberos (1.1.10) python-daemon (2.1.1) python-dateutil (2.5.3) python-editor (1.0) python-gflags (3.0.5) pytz (2016.4) requests (2.10.0) setproctitle (1.1.10) setuptools (21.2.1) six (1.10.0) snakebite (2.9.0) SQLAlchemy (1.0.13) thrift (0.9.3) thriftpy (0.3.8) unicodecsv (0.14.1) Werkzeug (0.11.10) WTForms (2.1) {code} {code} $ airflow webserver -p 8088 [2016-05-25 15:22:48,204] {__init__.py:36} INFO - Using executor LocalExecutor ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ [2016-05-25 15:22:49,066] {models.py:154} INFO - Filling up the DagBag from /opt/airflow/production/dags Running the Gunicorn server with 4 syncworkers on host 0.0.0.0 and port 8088 with a timeout of 120... [2016-05-25 15:22:49 +1000] [20191] [INFO] Starting gunicorn 19.3.0 [2016-05-25 15:22:49 +1000] [20191] [INFO] Listening at: http://0.0.0.0:8088 (20191) [2016-05-25 15:22:49 +1000] [20191] [INFO] Using worker: sync [2016-05-25 15:22:49 +1000] [20197] [INFO] Booting worker with pid: 20197 [2016-05-25 15:22:49 +1000] [20198] [INFO] Booting worker with pid: 20198 [2016-05-25 15:22:49 +1000] [20199] [INFO] Booting worker with pid: 20199 [2016-05-25 15:22:49 +1000] [20200] [INFO] Booting worker with pid: 20200 [2016-05-25 15:22:50,086] {__init__.py:36} INFO - Using executor LocalExecutor [2016-05-25 15:22:50,176] {__init__.py:36} INFO - Using executor LocalExecutor [2016-05-25 15:22:50,262] {__init__.py:36} INFO - Using executor LocalExecutor [2016-05-25 15:22:50,364] {__init__.py:36} INFO - Using executor LocalExecutor [2016-05-25 15:22:50,931] {models.py:154} INFO - Filling up the DagBag from /opt/airflow/production/dags [2016-05-25 15:22:51,000] {models.py:154} INFO - Filling up the DagBag from /opt/airflow/production/dags [2016-05-25 15:22:51,093] {models.py:154} INFO - Filling up the DagBag from /opt/airflow/production/dags [2016-05-25 15:22:51,191] {models.py:154} INFO - Filling up the DagBag from /opt/airflow/production/dags {code}"
458,AIRFLOW-175,AIRFLOW,1464186787000,1464213134000,Bug,Resolved,Major,3,327,"PR merge tool needs to reset environment after work_local finishes If you use the pr tool to work locally ({{airflow-pr work_local}}) and make changes to the files, then an error is raised when you try to exit the PR tool because git refuses to overwrite the changes. The tool needs to call {{git reset --hard}} before exiting."
459,AIRFLOW-176,AIRFLOW,1464191407000,1464277261000,Bug,Closed,Major,3,183,PR tool crashes with non-integer JIRA ids The PR tool crashes if a non-integer id is passed. This includes the default ID (AIRFLOW-XXX) so it affects folks who don't type in a new ID.
460,AIRFLOW-177,AIRFLOW,1464237233000,1464369288000,New Feature,Closed,Major,3,302,"Resume a failed dag Say I've a dag with 10 nodes and one of the dag run got failed at 5th node. Now if I want to resume that dag, I can go and run individual task one by one. Is there any way by which I can just tell dag_id and execution_date (or run_id) and it automatically retries only failed tasks?"
461,AIRFLOW-179,AIRFLOW,1464240576000,1478267803000,Bug,Resolved,Major,3,1377,"DbApiHook string serialization fails when string contains non-ASCII characters The DbApiHook.insert_rows(...) method tries to serialize all values to strings using the ASCII codec, this is problematic if the cell contains non-ASCII characters, i.e. >>> from airflow.hooks import DbApiHook >>> DbApiHook._serialize_cell('Nguyn Tn Dng') Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/local/lib/python2.7/dist-packages/airflow/hooks/dbapi_hook.py"", line 196, in _serialize_cell return ""'"" + str(cell).replace(""'"", ""''"") + ""'"" File ""/usr/local/lib/python2.7/dist-packages/future/types/newstr.py"", line 102, in __new__ return super(newstr, cls).__new__(cls, value) UnicodeDecodeError: 'ascii' codec can't decode byte 0xe1 in position 4: ordinal not in range(128) Rather than manually trying to serialize and escape values to an ASCII string one should try to serialize the value to string using the character set of the corresponding target database leveraging the connection to mutate the object to the SQL string literal. Additionally the escaping logic for single quotes (') within the _serialize_cell method seems wrong, i.e. str(cell).replace(""'"", ""''"") would escape the string ""you're"" to be ""'you''ve'"" as opposed to ""'you\'ve'"". Note an exception should still be thrown if the target encoding is not compatible with the source encoding."
462,AIRFLOW-180,AIRFLOW,1464281469000,1535980887000,Bug,Closed,Major,3,158,"Sensors do not test timeout correctly if timeout is more than a day/ Currently the sensors tests the timedelta seconds, instead of the timedelta total_seconds"
463,AIRFLOW-181,AIRFLOW,1464292477000,1520291972000,Bug,Closed,Major,3,157,Travis builds fail due to corrupt cache Corrupt cache is preventing from unpacking hadoop. It needs to redownload the distribution without checking the cache
464,AIRFLOW-184,AIRFLOW,1464369259000,1556724385000,Bug,Open,Major,3,312,"Add clear/mark success to CLI AIRFLOW-177 pointed out that the current CLI does not allow us to clear or mark success a task (including upstream, downstream, past, future, and recursive) the way that the UI widget does. Given a goal of keeping parity between the UI and CLI, it seems like we should support this."
465,AIRFLOW-186,AIRFLOW,1464373252000,1464379948000,Bug,Resolved,Major,3,81,"conn.literal is specific to MySQLdb, and should be factored out of the dbapi_hook"
466,AIRFLOW-188,AIRFLOW,1464492419000,1467355470000,Bug,Closed,Major,3,262,Exception on /admin/airflow/clear When I try to clear the past of a task I get a Oops. Here is the output I get : http://pastebin.com/yTYsekyB The code uses 2 files : http://pastebin.com/Uyp41wEh (subdags) http://pastebin.com/wiGP964w (maindag that uses subdags)
467,AIRFLOW-190,AIRFLOW,1464604361000,1464712123000,Bug,Closed,Major,3,184,Add codecov and remove download count Codecov gives a better way of viewing coverage and works without tokens. Download count does not work anymore since pypi has updated its mirroring
468,AIRFLOW-191,AIRFLOW,1464610844000,1535980992000,Bug,Resolved,Major,3,1325,"Database connection leak on Postgresql backend I raised this issue on github several months ago and there was even a PR but it never maid it into mainline. Basically, workers tend to hang onto DB connections in Postgres for recording heartbeat. I'm running a cluster with 115 workers, each with 8 slots. My Postgres DB is configured to allow 1000 simultaneous connections. I should effectively be able to run 920 tasks at the same time, but am actually limited to only about 450-480 because of idle transactions from workers hanging on to DB connections. If I run the following query select count(*),state, client_hostname from pg_stat_activity group by state, client_hostname These are the results: count state client_hostname 1 active (null) 1 idle localhost 451 idle in transaction (null) 446 idle (null) 1 active localhost The idle connections are all trying to run COMMIT The ""idle in transaction"" connections are all trying to run SELECT job.id AS job_id, job.dag_id AS job_dag_id, job.state AS job_state, job.job_type AS job_job_type, job.start_date AS job_start_date, job.end_date AS job_end_date, job.latest_heartbeat AS job_latest_heartbeat, job.executor_class AS job_executor_class, job.hostname AS job_hostname, job.unixname AS job_unixname FROM job WHERE job.id = 213823 LIMIT 1 with differing job.ids of course."
469,AIRFLOW-192,AIRFLOW,1464611765000,1516288248000,Improvement,Resolved,Major,3,1726,"Implement priority_weight aggregation using ancestors (rather than successors) Currently tasks are being scheduled based on the priority_weight. The effective priority of a task is it's own priority plus the priorities of all tasks that follow it in a dag. This results in undesirable scheduling behaviour in my use case. My use case involves running scientific workflows where a number of operations are being carried out on a set of samples in a set. Each sample is handled by a separate dag run that is manually triggered. It is common for several thousand dag instances to be in flight at a given time. The dag reserves a sample, operates on it, and then releases it. I would like for each sample to be reserved for as short a time as possible, so that other programs can have an opportunity to operate on it and dag runs can complete as fast as possible. However, because of the current priority logic, if I were to schedule several thousand dags at a given time, they would first all execute their first state, then all execute their second state, etc. Thus, no dag can complete fully, until all dags complete their second last state. This results in unnecessarily long dag run times and simultaneous completion of all dags. Ideally, Airflow would support the reverse of the current logic used for priorities i.e. a task's priority is the sum of priorities of all its ancestors. This way, the further along a dag is in its processing the more likely its tasks will get scheduled (thus leading to a shorter completion time, and release of its resources). Also, a nominal priority mode would be useful, where a task's priority is exactly the number given to it by the author, in order to allow more scheduling flexibility."
470,AIRFLOW-193,AIRFLOW,1464613012000,1585343809000,New Feature,Closed,Major,3,909,"Allow a series of tasks to be executed on the same worker Currently the only way to limit the execution of a series of tasks to a single worker is via pools, however this is not a very convenient method when managing hundreds of workers. In the context of scientific workflows it is a common desire to be able to retrieve a (possibly large) sample from a data repository (or object store), then progressively elaborate it via a series of transformations, and finally deposit the result back. From a modelling perspective it makes sense to have the series of transformations each be encapsulated in a separate task. From practical considerations (performance, network bandwidth) it would be desirable to retrieve the sample to a single worker's local storage, where it would then be worked upon until completion. This, of course, requires the ability to have a slew of tasks to be bound to a particular worker."
471,AIRFLOW-194,AIRFLOW,1464615942000,1559220213000,Bug,Closed,Major,3,1560,"Task hangs in up_for_retry state for very long I can observe this problem on 2 separate Airflow installations. The symptoms are: - One (and only one) task stays in up_for_retry state even when the last of the retries finished with an OK stays. - It is yellow in the tree view. - The execution somehow resumes several hours later automatically - It seems (not a certitude) related to a mode when the task execution is ""lagging"" behind normal execution. Here is an example of a task that should run every hour ""0 * * * *"": Current date : 2016-05-30T15:31:00+0200 ----- Run 1 ------ Run ID: 2016-05-05T21:00:00 Task start: 2015-05-30T07:38:XX.XXX Task end: 2015-05-30T08:23:XX.XXX Marked as success ----- Run 2 ------ Run ID: 2016-05-05T22:00:00 Task start: 2015-05-30T11:10:XX.XXX Task end: 2015-05-30T11:56:XX.XXX Marked as success ----- Run 3 ------ Run ID: 2016-05-05T23:00:00 Task start: 2015-05-30T11:56:XX.XXX Task end: 2015-05-30T12:41:XX.XXX Marked as success ----- Run 4 ------ Run ID: 2016-05-06T00:00:00 Task start: 2015-05-30T15:12:XX.XXX Task end: (Still running now) Marked as running There are nearly 2 hours between Run-1 and Run-2, and nearly 2 hours as well between Run-3 and Run-4. Only Run-3 starts immediately after the end of Run-2 what is the expected behavior as the Runs are very late on schedule (Run ID is 2016-05-06 while we are on 2016-05-30) This is a high priority issue for our setup. I could try to dig more in depth into this problem but I have no idea where to look to debug this issue. Any pointers would be more than welcome."
472,AIRFLOW-195,AIRFLOW,1464672916000,1464741982000,Bug,Closed,Major,3,265,Support toggling of SubDag clearing in the CLI for parity with UI functionality Functionality was added in https://github.com/apache/incubator-airflow/pull/1478 to allow for toggling of subdag clearing via the UI. I'm adding similar toggle functionality to the CLI.
473,AIRFLOW-196,AIRFLOW,1464695762000,1535980905000,Bug,Closed,Major,3,193,"HttpSensor does not handle exception properly If exception happens in poke function in HttpSensor, it is not well handled that make the sensor finish successfully, which is incorrect obviously."
474,AIRFLOW-198,AIRFLOW,1464726038000,1475021424000,New Feature,Resolved,Major,3,480,"Create a latest_only_operator Taken from: https://cwiki.apache.org/confluence/display/AIRFLOW/Roadmap ""For cases where we need to only run the latest in a series of task instance runs and mark the others as skipped. For example, we may have job to execute a DB snapshot every day. If the DAG is paused for 5 days and then unpaused, we dont want to run all 5, just the latest. With this feature, we will provide cron functionality for task scheduling that is not related to ETL"""
475,AIRFLOW-199,AIRFLOW,1464756626000,1535981022000,New Feature,Closed,Major,3,264,"Restart a DAG run from broken point. If a huge DAG fails at a task in a run, it would be nice that it can be restart at this broken task (failed task) or some particular task that has been executed, since it might be takes a very long time to re-run the whole DAG."
476,AIRFLOW-201,AIRFLOW,1464771781000,1535911254000,Bug,Closed,Major,3,1411,"HiveMetastoreHook kerberos support is broken Currently, HiveMetastoreHook on 1.7.1 with kerberos enabled fails with errors like: {code} [2016-05-31 12:25:20,341] {models.py:1286} ERROR - 'sasl.saslwrapper.Client' object is not callable Traceback (most recent call last): File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/airflow-1.7.1-py2.7.egg/airflow/models.py"", line 1245, in run result = task_copy.execute(context=context) File ""/usr/local/opr/virtualenv_airflow/lib/python2.7/site-packages/airflow-1.7.1-py2.7.egg/airflow/operators/sensors.py"", line 56, in execute while not self.poke(context): File ""/usr/local/opr/virtualenv_airflow/lib/python2.7/site-packages/airflow-1.7.1-py2.7.egg/airflow/operators/sensors.py"", line 255, in poke self.schema, self.table, self.partition) File ""/usr/local/opr/virtualenv_airflow/lib/python2.7/site-packages/airflow-1.7.1-py2.7.egg/airflow/hooks/hive_hooks.py"", line 316, in check_for_partition self.metastore._oprot.trans.open() File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/thrift_sasl/__init__.py"", line 67, in open self.sasl = self.sasl_client_factory() File ""/usr/local/opr/virtualenv_airflow/lib/python2.7/site-packages/airflow-1.7.1-py2.7.egg/airflow/hooks/hive_hooks.py"", line 293, in sasl_factory sasl_client(""service"", kerberos_service_name) TypeError: 'sasl.saslwrapper.Client' object is not callable{code}"
477,AIRFLOW-202,AIRFLOW,1464772655000,1535980865000,Bug,Closed,Major,3,5206,"Stray 'print' line deep_copy leads to errors with non-ascii encodings Erros like: {code} May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: K: last_loaded V: 2016-05-31 14:34:36.313699 May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: K: concurrency V: 16 May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: K: is_subdag V: False May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: K: parent_dag V: None May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: K: default_args V: {'retries': 3, 'email_on_retry': False, 'email_on_failure': True, 'wait_for_downstream': False, 'owner': 'Alexey Ustyantsev', 'depends_on_past': False, 'start_date': datetime.datetime(2016, 5, 30, 0, 0), 'pool': 'main_pool', 'email': ['a.ustyantsev@corp.mail.ru']} May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: K: start_date V: None May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: K: dag_id V: calculate_events_aggregation May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: K: max_active_runs V: 16 May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: K: end_date V: None May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: [2016-05-31 14:36:37,046] {app.py:1423} ERROR - Exception on /admin/airflow/clear [GET] May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: Traceback (most recent call last): May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/flask/app.py"", line 1817, in wsgi_app May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: response = self.full_dispatch_request() May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/flask/app.py"", line 1477, in full_dispatch_request May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: rv = self.handle_user_exception(e) May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/flask/app.py"", line 1381, in handle_user_exception May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: reraise(exc_type, exc_value, tb) May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/flask/app.py"", line 1475, in full_dispatch_request May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: rv = self.dispatch_request() May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/flask/app.py"", line 1461, in dispatch_request May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: return self.view_functions[rule.endpoint](**req.view_args) May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/flask_admin/base.py"", line 68, in inner May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: return self._run_view(f, *args, **kwargs) May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/flask_admin/base.py"", line 367, in _run_view May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: return fn(self, *args, **kwargs) May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/flask_login.py"", line 755, in decorated_view May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: return func(*args, **kwargs) May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/airflow-1.7.1-py2.7.egg/airflow/www/utils.py"", line 118, in wrapper May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: return f(*args, **kwargs) May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/airflow-1.7.1-py2.7.egg/airflow/www/utils.py"", line 167, in wrapper May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: return f(*args, **kwargs) May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/airflow-1.7.1-py2.7.egg/airflow/www/views.py"", line 1017, in clear May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: include_upstream=upstream) May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/airflow-1.7.1-py2.7.egg/airflow/models.py"", line 2870, in sub_dag May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: dag = copy.deepcopy(self) May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/lib/python2.7/copy.py"", line 174, in deepcopy May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: y = copier(memo) May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: File ""/usr/local/opr/virtualenv_airflow/local/lib/python2.7/site-packages/airflow-1.7.1-py2.7.egg/airflow/models.py"", line 2855, in __deepcopy__ May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: print(""K: {} V: {}"".format(k, v)) May 31 14:36:37 stat-ko-oprstatdb02 airflow_webserver: UnicodeEncodeError: 'ascii' codec can't encode characters in position 14-20: ordinal not in range(128) {code}"
478,AIRFLOW-203,AIRFLOW,1464780780000,1576099612000,Bug,Open,Major,3,1544,"Scheduler fails to reliably schedule tasks when many dag runs are triggered Using Airflow with Celery, Rabbitmq, and Postgres backend. Running 1 master node and 115 worker nodes, each with 8 cores. The workflow consists of series of 27 tasks, some of which are nearly instantaneous and some take hours to complete. Dag runs are manually triggered, about 3000 at a time, resulting in roughly 75 000 tasks. My observations are that the scheduling behaviour is extremely inconsistent, i.e. about 1000 tasks get scheduled and executed and then no new tasks get scheduled after that. Sometimes it is enough to restart the scheduler for new tasks to get scheduled, sometimes the scheduler and worker services need to be restarted multiple times to get any progress. When I look at the scheduler output it seems to be chugging away at trying to schedule tasks with messages like: ""2016-06-01 11:28:25,908] {base_executor.py:34} INFO - Adding to queue: airflow run ..."" However, these tasks do not show up in queued status on the UI and don't actually get scheduled out to the workers (nor make it into the rabbitmq queue, or the task_instance table). It is unclear what may be causing this behaviour as no errors are produced anywhere. The impact is especially high when short-running tasks are concerned because the cluster should be able to blow through them within a couple of minutes, but instead it takes hours of manual restarts to get through them. I'm happy to share logs or any other useful debug output as desired. Thanks in advance. Sergei."
479,AIRFLOW-204,AIRFLOW,1464780909000,1585614151000,Bug,Closed,Major,3,316,"Tasks from manually triggered dag runs do not show up in Queued status on the UI When I manually trigger dag runs I see that the scheduler is trying to add the tasks to the work queue, but these tasks never appear in queued status on the webserver UI, thus there is a lack of visibility into these outstanding tasks."
480,AIRFLOW-208,AIRFLOW,1464901089000,1547067469000,Improvement,Resolved,Major,3,59,Adding badge to README.md to show supported Python versions
481,AIRFLOW-209,AIRFLOW,1464958660000,1535981025000,Bug,Closed,Major,3,110,"Add unit tests to scheduler The scheduler (schedule_dag, process_dag) have not enough unittests covering them."
482,AIRFLOW-212,AIRFLOW,1464998832000,1585614182000,Improvement,Closed,Major,3,62,Bad plugin code can prevent Airflow from loading - add timeout
483,AIRFLOW-214,AIRFLOW,1465241412000,1535980948000,Bug,Closed,Major,3,1692,"TaskInstance can get detached in process_dag Is some rare occasions the TaskInstance can get detached in process_dag. It is unclear why {code}[2016-06-06 17:47:37,048] {models.py:3444} INFO - Updating state for <DagRun example_skip_dag @ 2016-06-05 00:00:00: scheduled__2016-06-05T00:00:00, externally triggered: False> considering 8 task(s) [2016-06-06 17:47:37,059] {jobs.py:670} ERROR - Instance <TaskInstance at 0x10d3e9410> is not bound to a Session; attribute refresh operation cannot proceed Traceback (most recent call last): File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/airflow-1.7.1.2-py2.7.egg/airflow/jobs.py"", line 667, in _do_dags self.process_dag(dag, tis_out) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/airflow-1.7.1.2-py2.7.egg/airflow/jobs.py"", line 531, in process_dag task = dag.get_task(ti.task_id) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/sqlalchemy/orm/attributes.py"", line 237, in __get__ return self.impl.get(instance_state(instance), dict_) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/sqlalchemy/orm/attributes.py"", line 578, in get value = state._load_expired(state, passive) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/sqlalchemy/orm/state.py"", line 474, in _load_expired self.manager.deferred_scalar_loader(self, toload) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/sqlalchemy/orm/loading.py"", line 610, in load_scalar_attributes (state_str(state))) DetachedInstanceError: Instance <TaskInstance at 0x10d3e9410> is not bound to a Session; attribute refresh operation cannot proceed{code}"
484,AIRFLOW-215,AIRFLOW,1465242255000,1558131580000,Bug,Resolved,Major,3,511,"Airflow worker (CeleryExecutor) needs to be restarted to pick up tasks We have a main dag that dynamically creates subdags containing tasks using BashOperator. Using CeleryExecutor we see Celery tasks been created with *STARTED* status but they are not picked up by our worker. However, if we restart our worker, then tasks are picked up. Here you can find code if you want to try to reproduce it https://www.dropbox.com/s/8u7xf8jt55v8zio/dags.zip. We also tested using LocalExecutor and everything worked fine."
485,AIRFLOW-219,AIRFLOW,1465364435000,1536014925000,New Feature,Resolved,Major,3,54,Impersonation - Unix user impersonation task parameter
486,AIRFLOW-221,AIRFLOW,1465399063000,1585614199000,Bug,Closed,Major,3,60,Improve task instance timeout logic to work in LocalExecutor
487,AIRFLOW-224,AIRFLOW,1465493378000,1535980863000,Improvement,Closed,Major,3,341,"Collect orphaned tasks in case of unclean shutdown Tasks in a ""scheduled"" state can get orphaned if either the executor or the scheduler is shutdown uncleanly (kill -9). This should be addressed by making sure the scheduler confirms sending tasks to the executor and checking weather ""scheduled"" tasks are still in the queue of the executor."
488,AIRFLOW-225,AIRFLOW,1465494434000,1535980869000,Improvement,Closed,Major,3,238,"Better units for task duration graph Right now the job duration window defaults to hours, which for short lived tasks results in numbers out to five decimals. Instead, it should adjust the scale to hours, minutes, seconds, as appropriate."
489,AIRFLOW-227,AIRFLOW,1465501844000,1477290332000,Improvement,Resolved,Major,3,633,"Show environmentally defined configs in the config view Right now the config page (when set to display) just loads the airflow.cfg and lists that. This means that any configuration values that are set via an environment variable are not shown. On the one hand, this is nice for secure values (ie, db connect string), but becomes a pain if large amounts of configuration are done throgh env vars. This situation can occur when deploying airflow in production versus staging versus dev environments. We currently load lots of values through env vars (passed into the Docker containers). These values don't show up in the config screen."
490,AIRFLOW-228,AIRFLOW,1465507663000,1468335372000,Bug,Resolved,Major,3,1722,"PR tool should be friendly about bad JIRA versions When running the PR tool, I entered 1.8 as the version, and got: {noformat} Proceed with AIRFLOW-AIRFLOW-142? [y/N]: y Enter comma-separated fix version(s) []: 1.8 Resetting git to remove any changes >> Running command: git reset --hard Restoring head pointer to master >> Running command: git checkout master Already on 'master' >> Running command: git branch Traceback (most recent call last): File ""dev/airflow-pr"", line 892, in <module> cli() File ""/Users/chrisr/.virtual-env/airflow/lib/python2.7/site-packages/click/core.py"", line 716, in __call__ return self.main(*args, **kwargs) File ""/Users/chrisr/.virtual-env/airflow/lib/python2.7/site-packages/click/core.py"", line 696, in main rv = self.invoke(ctx) File ""/Users/chrisr/.virtual-env/airflow/lib/python2.7/site-packages/click/core.py"", line 1060, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) File ""/Users/chrisr/.virtual-env/airflow/lib/python2.7/site-packages/click/core.py"", line 889, in invoke return ctx.invoke(self.callback, **ctx.params) File ""/Users/chrisr/.virtual-env/airflow/lib/python2.7/site-packages/click/core.py"", line 534, in invoke return callback(*args, **kwargs) File ""dev/airflow-pr"", line 830, in merge main(pr_num, local=False) File ""dev/airflow-pr"", line 763, in main merge_branches=merged_refs) File ""dev/airflow-pr"", line 534, in resolve_jira_issue map(lambda v: get_version_json(v), fix_versions)) File ""dev/airflow-pr"", line 534, in <lambda> map(lambda v: get_version_json(v), fix_versions)) File ""dev/airflow-pr"", line 530, in get_version_json return list(filter(lambda v: v.name == version_str, versions))[0].raw IndexError: list index out of range {noformat}"
491,AIRFLOW-230,AIRFLOW,1465533489000,1520293082000,Improvement,Resolved,Major,3,49,[HiveServer2Hook] adding multi statements support
492,AIRFLOW-231,AIRFLOW,1465534740000,1535981013000,Improvement,Closed,Major,3,59,Remove security issue around `eval` statement in PrestoHook
493,AIRFLOW-232,AIRFLOW,1465551713000,1520293120000,Bug,Closed,Major,3,270,"Web UI shows inaccurate task counts on main dashboard Pstgres, celery, rabbitmq, 170 worker nodes, 1 master. select count(*), state from task_instance where dag_id = 'freebayes' group by state; upstream_failed 2134 up_for_retry 520 success 141421 running 542 failed 1165"
494,AIRFLOW-233,AIRFLOW,1465566360000,1537259384000,Bug,Resolved,Major,3,2049,"Detached DagRun error in scheduler loop Running Airflow master, every scheduler loop has at least one detached DagRun error. This is the output: {code} [2016-06-10 09:41:54,772] {jobs.py:669} ERROR - Instance <DagRun at 0x10ab80dd8> is not bound to a Session; attribute refresh operation cannot proceed Traceback (most recent call last): File ""/Users/jlowin/git/airflow/airflow/jobs.py"", line 666, in _do_dags self.process_dag(dag, tis_out) File ""/Users/jlowin/git/airflow/airflow/jobs.py"", line 524, in process_dag State.UP_FOR_RETRY)) File ""/Users/jlowin/git/airflow/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/Users/jlowin/git/airflow/airflow/models.py"", line 3387, in get_task_instances TI.dag_id == self.dag_id, File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/sqlalchemy/orm/attributes.py"", line 237, in __get__ return self.impl.get(instance_state(instance), dict_) File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/sqlalchemy/orm/attributes.py"", line 578, in get value = state._load_expired(state, passive) File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/sqlalchemy/orm/state.py"", line 474, in _load_expired self.manager.deferred_scalar_loader(self, toload) File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/sqlalchemy/orm/loading.py"", line 610, in load_scalar_attributes (state_str(state))) sqlalchemy.orm.exc.DetachedInstanceError: Instance <DagRun at 0x10ab80dd8> is not bound to a Session; attribute refresh operation cannot proceed {code} This is the test DAG in question: {code} from airflow import DAG from airflow.operators import PythonOperator from datetime import datetime import logging import time default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2016, 4, 24), } dag_name = 'dp_test' dag = DAG( dag_name, default_args=default_args, schedule_interval='*/2 * * * *') def cb(**kw): time.sleep(2) logging.info('Done %s' % kw['ds']) d = PythonOperator(task_id=""delay"", provide_context=True, python_callable=cb, dag=dag) {code}"
495,AIRFLOW-234,AIRFLOW,1465581904000,1535980932000,Bug,Closed,Major,3,46,make task that aren't `running` self-terminate
496,AIRFLOW-235,AIRFLOW,1465599367000,1585614228000,Improvement,Closed,Major,3,583,"Improve connectors interface Right now the connections interface has the same fields for all connectors, whether or not they apply. Per-connector values are stuffed into the extra field, which doesn't have any description or clarification. Connectors don't have any way of displaying what extra information they require. It would be better if connectors could define what fields they specified through the interface (a map of field name to type, description, validator, etc). The connector web page could then render these and pass them back to the connector when it is instantiated."
497,AIRFLOW-236,AIRFLOW,1465673152000,1520293200000,Improvement,Resolved,Major,3,566,"Support passing S3 credentials through environmental variables Right now we expect S3 configs to be passed through one of a variety of config files, or through extra parameters in the connection screen. It'd be nice to be able to pass these through env variables and note as such through the extra parameters. This would lessen the need to include credentials in the webapp itself. Alternatively, for logging (rather than as a connector), it might just be better for Airflow to use the profie defined as AWS_DEFAULT and avoid needed an explicit configuration at all."
498,AIRFLOW-238,AIRFLOW,1465856709000,1468245386000,Bug,Closed,Major,3,54,New flask-admin==1.4.1 breaks the Pypi release 1.7.1.2
499,AIRFLOW-239,AIRFLOW,1465930617000,1535980990000,Bug,Closed,Major,3,614,"test_hdfs_sensor, etc not collected by nosetests Due to the indentation at https://github.com/apache/incubator-airflow/blob/5963ac8b7b7c0b3470ce6171b1cd928485bc9884/tests/core.py#L1605-L1623 The methods test_hdfs_sensor and below are treated as internal functions of test_presto_to_mysql and not collected by nosetests ```  zodiac-airflow git:(master) export AIRFLOW_RUNALL_TESTS=1  zodiac-airflow git:(master) nosetests -v --collect-only 2>&1 | grep test_presto_to_mysql | wc -l 1  zodiac-airflow git:(master) nosetests -v --collect-only 2>&1 | grep test_hdfs_sensor | wc -l 0  zodiac-airflow git:(master) ```"
500,AIRFLOW-240,AIRFLOW,1465932544000,1465942452000,Bug,Closed,Major,3,1386,"TemplateNotFound Error when using PostgresOperator and SQL script When using the PostgresOperator, I've been unable to get templating to work. It's quite possible I'm doing something wrong, but I'm pretty lost as to what the issue might be. Here's an example to reproduce the TemplateNotFound error I've been getting: {code:title=/my_dag/csv_to_redshift.py} copy_s3_to_redshift = PostgresOperator( task_id='load_table', sql=this_dag_path + '/copy_to_redshift.sql', params=dict( AWS_ACCESS_KEY_ID=Variable.get('AWS_ACCESS_KEY_ID'), AWS_SECRET_ACCESS_KEY=Variable.get('AWS_SECRET_ACCESS_KEY') ), postgres_conn_id='postgres_redshift', autocommit=False, dag=dag ) {code} {code:title=/my_dag/copy_to_redshift.sql} COPY public.table_foobar FROM 's3://mybucket/test-data/import/foobar.csv' CREDENTIALS 'aws_access_key_id={{ AWS_ACCESS_KEY_ID }};aws_secret_access_key={{ AWS_SECRET_ACCESS_KEY }}' CSV NULL as 'null' IGNOREHEADER as 1; {code} {code} Traceback (most recent call last): File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 2038, in resolve_template_files setattr(self, attr, env.loader.get_source(env, content)[0]) File ""/usr/local/lib/python2.7/dist-packages/jinja2/loaders.py"", line 187, in get_source raise TemplateNotFound(template) TemplateNotFound: /home/gregreda/airflow/dags/my_dag/copy_to_redshift.sql {code} Any ideas towards a fix are much appreciated."
