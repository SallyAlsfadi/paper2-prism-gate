rank_priority_only,issue_key,project,created_ts,updated_ts,issue_type,status,priority,priority_ordinal,text_len,text
1,AIRFLOW-56,AIRFLOW,1462536460000,1462871357000,Bug,Closed,Blocker,1,531,"Airflow's scheduler can ""lose"" queued tasks Tasks that get into queued status can get ""lost"" on the scheduler/executor. They remain in the DB with state ""queued"", but the scheduler won't pick them up as it will not look at the DB currently. The executor does not see those tasks, although the scheduler has NOT been restarted (ie. to see this issue it is not required to have ""-n x"" as a parameter to the scheduler). This happens *also* to tasks scheduled by the scheduler and not just with backfills created by the SubDagOperator."
2,AIRFLOW-106,AIRFLOW,1462985047000,1463126376000,Bug,Closed,Blocker,1,2999,"Task Retries, on_failure callback, and email_on_failure Not Honored if First Task in a DAG Fails Hello. I created the following workflow : {code} from airflow import DAG from airflow.operators import PythonOperator from datetime import datetime, timedelta from airflow.models import Variable from time import sleep default_args = { 'depends_on_past': False, 'start_date': datetime(2016, 5, 11, 15, 20), 'email': <my email> 'email_on_failure': True, 'email_on_retry': False, 'retries': 1, 'retry_delay': timedelta(minutes=2), 'end_date': datetime(2016, 5, 11, 16, 00), } PARENT_DAG_NAME = 'test' dag = DAG(PARENT_DAG_NAME, default_args=default_args, schedule_interval=timedelta(minutes=10)) def sleep1_function(**kwargs): sleep(90) return Variable.get('test_var') sleep1 = PythonOperator( task_id='sleep1', python_callable=sleep1_function, dag=dag) {code} I forgot to declare test_var so when this DAG launched it failed quickly. However no failure email was ever sent. Clearing the failed task to make it rerun doesn't trigger any email. Here is the logs : {code} [2016-05-11 15:53:31,784] {models.py:157} INFO - Filling up the DagBag from /var/lib/airflow/airflow/dags/test.py [2016-05-11 15:53:32,272] {models.py:157} INFO - Filling up the DagBag from /var/lib/airflow/airflow/dags/test.py [2016-05-11 15:53:32,313] {models.py:1216} INFO - -------------------------------------------------------------------------------- Starting attempt 1 of 2 -------------------------------------------------------------------------------- [2016-05-11 15:53:32,333] {models.py:1239} INFO - Executing <Task(PythonOperator): sleep1> on 2016-05-11 15:20:00 [2016-05-11 15:55:03,450] {models.py:1306} ERROR - Variable test_var does not exist Traceback (most recent call last): File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.0-py2.7.egg/airflow/models.py"", line 1265, in run result = task_copy.execute(context=context) File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.0-py2.7.egg/airflow/operators/python_operator.py"", line 66, in execute return_value = self.python_callable(*self.op_args, **self.op_kwargs) File ""/var/lib/airflow/airflow/dags/test.py"", line 31, in sleep1_function return Variable.get('test_var') File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.0-py2.7.egg/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/usr/local/lib/python2.7/dist-packages/airflow-1.7.0-py2.7.egg/airflow/models.py"", line 3145, in get raise ValueError('Variable {} does not exist'.format(key)) ValueError: Variable test_var does not exist [2016-05-11 15:55:03,581] {models.py:1318} INFO - Marking task as UP_FOR_RETRY [2016-05-11 15:55:03,759] {models.py:1347} ERROR - Variable test_var does not exist {code} In the DAG Runs page, the workflow is set as failed. In hte taks instance page, it is set as up_for_retry but no new run is ever scheduled. I tried incrementing the retires parameter, but nothing different happens, Airflow never retries after the first run. dud"
3,AIRFLOW-434,AIRFLOW,1471385417000,1475022298000,Bug,Resolved,Blocker,1,2555,"max_dag_run_reached blocks dag state change and new task scheduling Using the following DAG: ``` from airflow import DAG from airflow.operators.bash_operator import BashOperator from datetime import datetime, timedelta default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2016, 1, 1, 1, 0), 'email': ['xuanji@gmail.com'], 'email_on_failure': True, 'email_on_retry': False, 'retries': 3, 'retry_delay': timedelta(minutes=1), } dag = DAG('bash_bash_bash', default_args=default_args, schedule_interval=timedelta(seconds=10)) # t1, t2 and t3 are examples of tasks created by instatiating operators t1 = BashOperator( task_id='print_date', bash_command='date', dag=dag) t2 = BashOperator( task_id='sleep', bash_command='sleep 120', retries=3, dag=dag) templated_command = """""" {% for i in range(5) %} echo ""{{ ds }}"" echo ""{{ macros.ds_add(ds, 7)}}"" echo ""{{ params.my_param }}"" {% endfor %} """""" t3 = BashOperator( task_id='templated', bash_command=templated_command, params={'my_param': 'Parameter I passed in'}, dag=dag) t2.set_upstream(t1) t3.set_upstream(t1) ``` and an `airflow.cfg` that contains this: ``` min_file_process_interval = 1 ``` The state eventually becomes this: http://imgur.com/a/5bRTe The scheduler should be marking the 14 leftmost dagruns as success, but does not. the scheduler should also be scheduling tasks for the last two dagruns. A look at the logs explains the probable cause: ``` [2016-08-16 15:12:10,257] {jobs.py:1446} DagFileProcessor174 INFO - Processing file /Users/xuanji_li/airflow/dags/bash_bash_bash.py for tasks to queue [2016-08-16 15:12:10,258] {models.py:162} DagFileProcessor174 INFO - Filling up the DagBag from /Users/xuanji_li/airflow/dags/bash_bash_bash.py [2016-08-16 15:12:10,267] {jobs.py:1460} DagFileProcessor174 INFO - DAG(s) ['bash_bash_bash'] retrieved from /Users/xuanji_li/airflow/dags/bash_bash_bash.py [2016-08-16 15:12:10,289] {jobs.py:1062} DagFileProcessor174 INFO - Not processing DAG bash_bash_bash since its max runs has been reached [2016-08-16 15:12:10,290] {models.py:313} DagFileProcessor174 INFO - Finding 'running' jobs without a recent heartbeat [2016-08-16 15:12:10,290] {models.py:319} DagFileProcessor174 INFO - Failing jobs without heartbeat after 2016-08-16 15:09:55.290479 ``` It seems that processing of the dagrun is skipped completely because there are already 16 running dagruns. Binary search tracked down this commit as the one that introduced the bug. The logic added looks wrong to me. https://github.com/apache/incubator-airflow/pull/1716"
4,AIRFLOW-447,AIRFLOW,1471590020000,1472737415000,Bug,Resolved,Blocker,1,683,"Python 3 map object cannot be json-serialized - need lists instead On this line, `source_uris` are generated with `map`: https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/operators/gcs_to_bq.py#L124 In Python 2 `map` would return a list, however in Python 3 `map` returns a `map` object / generator. The Python 3 `map` object cannot be json-serialized downstream when storing corresponding job objects in the database: https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/hooks/bigquery_hook.py#L441 Constructing `source_uris` with a list comprehension instead of `map` resolves the issue for Python 3 and retains compatibility with Python 2."
5,AIRFLOW-607,AIRFLOW,1477931169000,1490736678000,Bug,Resolved,Blocker,1,851,"Cannot initdb on Oracle because of String(5000) fields in model Unfortunately Oracle cannot be used to host the Airflow application DB due to the fact that Oracle limits varchar data types to 4000 bytes. If you try to run the airflow initdb command to install the DB onto Oracle you will end up with the following error: sqlalchemy.exc.DatabaseError: (cx_Oracle.DatabaseError) ORA-00910: specified length too long for its datatype [SQL: '\nCREATE TABLE connection (\n\tid INTEGER NOT NULL, \n\tconn_id VARCHAR2(250 CHAR), \n\tconn_type VARCHAR2(500 CHAR), \n\thost VARCHAR2(500 CHAR), \n\tschema VARCHAR2(500 CHAR), \n\tlogin VARCHAR2(500 CHAR), \n\tpassword VARCHAR2(500 CHAR), \n\tport INTEGER, \n\textra VARCHAR2(5000 CHAR), \n\tPRIMARY KEY (id)\n)\n\n'] Request that we change String(5000) types in the Chart and Connection models to 4000. Thanks!"
6,AIRFLOW-660,AIRFLOW,1480515716000,1558123380000,Bug,Resolved,Blocker,1,2682,"Impossible to record second task failure {code} /var/log/airflow/airflow_scheduler_err.log.10: [SQL: 'INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s)'] [parameters: {'task_id': 'test_task', 'end_date': datetime.datetime(2016, 11, 30, 14, 38, 39, 197485), 'execution_date': datetime.datetime(2016, 11, 30, 0, 0), 'duration': 331.723087, 'start_date': datetime.datetime(2016, 11, 30, 14, 33, 7, 474398), 'dag_id': 'test_dag'}] /var/log/airflow/airflow_scheduler_err.log.10-Process DagFileProcessor314-Process: /var/log/airflow/airflow_scheduler_err.log.10-Traceback (most recent call last): /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap /var/log/airflow/airflow_scheduler_err.log.10- self.run() /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/multiprocessing/process.py"", line 114, in run /var/log/airflow/airflow_scheduler_err.log.10- self._target(*self._args, **self._kwargs) /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/site-packages/airflow/jobs.py"", line 318, in helper /var/log/airflow/airflow_scheduler_err.log.10- pickle_dags) /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/site-packages/airflow/utils/db.py"", line 56, in wrapper /var/log/airflow/airflow_scheduler_err.log.10- session.commit() /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 813, in commit /var/log/airflow/airflow_scheduler_err.log.10- self.transaction.commit() /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 390, in commit /var/log/airflow/airflow_scheduler_err.log.10- self._assert_active(prepared_ok=True) /var/log/airflow/airflow_scheduler_err.log.10- File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 214, in _assert_active /var/log/airflow/airflow_scheduler_err.log.10- % self._rollback_exception /var/log/airflow/airflow_scheduler_err.log.10:InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.IntegrityError) duplicate key value violates unique constraint ""task_fail_pkey"" /var/log/airflow/airflow_scheduler_err.log.10-DETAIL: Key (task_id, dag_id, execution_date)=(test_dag, test_task, 2016-11-30 00:00:00) already exists. {code}"
7,AIRFLOW-695,AIRFLOW,1481580870000,1542279432000,Bug,Closed,Blocker,1,910,"Retries do not execute because dagrun is in FAILED state Currently on the latest master commit (15ff540ecd5e60e7ce080177ea3ea227582a4672), running on the LocalExecutor, retries on tasks do not execute because the state of the corresponding dagrun changes to FAILED. The task instance then gets blocked because ""Task instance's dagrun was not in the 'running' state but in the state 'failed',"" the error message produced by the following lines: https://github.com/apache/incubator-airflow/blob/master/airflow/ti_deps/deps/dagrun_exists_dep.py#L48-L50 This error can be reproduced with the following simple DAG: {code:title=DAG.py|borderStyle=solid} dag = models.DAG(dag_id='test_retry_handling') task = BashOperator( task_id='test_retry_handling_op', bash_command='exit 1', retries=1, retry_delay=datetime.timedelta(minutes=1), dag=dag, owner='airflow', start_date=datetime.datetime(2016, 2, 1, 0, 0, 0)) {code}"
8,AIRFLOW-719,AIRFLOW,1482857456000,1541102707000,Sub-task,Closed,Blocker,1,251,"Skipped operations make DAG finish prematurely Since revision 2630361ca24737c28f458825b20ab11c9c996b17 the SKIPPED and SUCCESS are treated the same, this causes the DAG to finish prematurely when using a Branching operator where on branch is finished."
9,AIRFLOW-727,AIRFLOW,1483390477000,1490374009000,Bug,Resolved,Blocker,1,210,"try_number is not increased A dag that has retries enabled will retry indefinitely as try_number gets reset to 0 in LocalTaskJob as task_instance is not fully populated, but nevertheless saved to the databases."
10,AIRFLOW-738,AIRFLOW,1483735453000,1535980834000,Bug,Resolved,Blocker,1,5821,"XCom: Deadlock found when trying to get lock; try restarting transaction When using the following dag: {code} from datetime import datetime, timedelta import logging import pprint import random # The DAG object; we'll need this to instantiate a DAG from airflow import DAG # Operators; we need this to operate! from airflow.operators.python_operator import PythonOperator start_time = datetime.now().replace(minute=0, second=0, microsecond=0) start_time += timedelta(hours=-1) # timedelta(days=-2) default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': start_time, 'email': ['alex.papanic@gmail.com'], 'email_on_failure': True, 'email_on_retry': True, 'retries': 1, 'retry_delay': timedelta(minutes=1) # 'queue': 'bash_queue', # 'pool': 'backfill', # 'priority_weight': 10, # 'end_date': datetime(2016, 1, 1), } dag = DAG( 'xcom_test', default_args=default_args, schedule_interval='@once') def upload_activity_status(pgconn_id, **context): upstream_task_ids = context['task'].upstream_task_ids logging.info( ""Getting status from upstream task {}"".format(upstream_task_ids)) status = context['ti'].xcom_pull(task_ids=upstream_task_ids) logging.info(""Xcom pull results:\n{}"".format(pprint.pformat(status))) logging.info(""Upload to DB here"") upload_ativity_status = PythonOperator( task_id='upload_activity_status', python_callable=upload_activity_status, op_kwargs={'pgconn_id': 'postgres_conn'}, provide_context=True, dag=dag) def poll_data(params, execution_date, **context): logging.info(""Test polling function for {data_stream}"".format(**params)) status = random.random() < 0.5 output = dict( data_stream=params['data_stream'], timeperiod=execution_date + timedelta(hours=-1), status=status ) return output def poll_data_factory(data_stream, dag): return PythonOperator( task_id='poll_{}'.format(data_stream), python_callable=poll_data, params={u'data_stream': data_stream}, provide_context=True, dag=dag ) poll_streams = [] streams = ['stream' + str(i) for i in range(30)] for data_stream in streams: poll = poll_data_factory(data_stream, dag) poll_streams.append(poll) upload_ativity_status.set_upstream(poll) {code} The following error is thrown: {code} 2017-01-06 21:41:35,824] {jobs.py:1433} INFO - Heartbeating the scheduler Traceback (most recent call last): File ""/Users/bolke/Documents/dev/airflow_env/bin/airflow"", line 4, in <module> __import__('pkg_resources').run_script('airflow==1.7.2.dev0', 'airflow') File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 739, in run_script self.require(requires)[0].run_script(script_name, ns) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 1494, in run_script exec(code, namespace, namespace) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/airflow-1.7.2.dev0-py2.7.egg/EGG-INFO/scripts/airflow"", line 28, in <module> args.func(args) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/airflow-1.7.2.dev0-py2.7.egg/airflow/bin/cli.py"", line 380, in run pool=args.pool, File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/airflow-1.7.2.dev0-py2.7.egg/airflow/utils/db.py"", line 54, in wrapper result = func(*args, **kwargs) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/airflow-1.7.2.dev0-py2.7.egg/airflow/models.py"", line 1334, in run self.handle_failure(e, test_mode, context) File ""/Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/airflow-1.7.2.dev0-py2.7.egg/airflow/models.py"", line 1407, in handle_failure session.merge(self) File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/session.py"", line 1815, in merge File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/session.py"", line 1861, in _merge File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 831, in get File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 864, in _get_impl File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/loading.py"", line 223, in load_on_ident File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 2756, in one File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 2726, in one_or_none File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 2797, in __iter__ File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 2818, in _execute_and_instances File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 2827, in _get_bind_args File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/query.py"", line 2809, in _connection_from_session File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/session.py"", line 966, in connection File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/session.py"", line 971, in _connection_for_bind File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/session.py"", line 382, in _connection_for_bind File ""build/bdist.macosx-10.12-x86_64/egg/sqlalchemy/orm/session.py"", line 276, in _assert_active sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (_mysql_exceptions.OperationalError) (1213, 'Deadlock found when trying to get lock; try restarting transaction') [SQL: u'INSERT INTO xcom (`key`, value, timestamp, execution_date, task_id, dag_id) VALUES (%s, %s, now(), %s, %s, %s)'] [parameters: (u'return_value', '\x80\x02}q\x00(U\x06statusq\x01\x89U\ntimeperiodq\x02cdatetime\ndatetime\nq\x03U\n\x07\xe1\x01\x06\x13\x00\x00\x00\x00\x00q\x04\x85q\x05Rq\x06U\x0bdata_streamq\x07U\x08stream26q\x08u.', datetime.datetime(2017, 1, 6, 20, 0), 'poll_stream26', 'xcom_test')] {code}"
11,AIRFLOW-747,AIRFLOW,1484155872000,1484688596000,Bug,Closed,Blocker,1,1001,"retry_delay not honored In Airflow 1.8 alpha 2, using LocalExecutor, DAGs do not seem to honor the retry_delay parameter, i.e. the retries happen immediately one after the other without waiting the specific retry_delay time. However, the number of retries is honored. I am testing with the following code: from airflow import DAG from airflow.operators.bash_operator import BashOperator from datetime import datetime, timedelta default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2016, 10, 5, 19), 'end_date': datetime(2016, 10, 6, 19), 'email': ['airflow@airflow.com'], 'email_on_failure': False, 'email_on_retry': False, 'retries': 10, 'retry_delay': timedelta(0, 500) } dag = DAG('test_retry_handling_job', default_args=default_args, schedule_interval='@once') task1 = BashOperator( task_id='test_retry_handling_op1', bash_command='exit 1', dag=dag) task2 = BashOperator( task_id='test_retry_handling_op2', bash_command='exit 1', dag=dag) task2.set_upstream(task1)"
12,AIRFLOW-778,AIRFLOW,1484943262000,1486061708000,Bug,Resolved,Blocker,1,267,Metastore Partition Sensor Broken MetastorePartitionSensor always throws an exception on initialization due to 72cc8b3006576153aa30d27643807b4ae5dfb593 . Looks like the tests for this are only run if an explicit flag is set which is how this got past CI. cc [~xuanji]
13,AIRFLOW-803,AIRFLOW,1485341396000,1489525140000,Bug,Resolved,Blocker,1,461,"Manual triggered dags are not running After cgroups+impersonation was added the task_instances for manually created dag_runs are not executed anymore. This is due to the fact the task_instance table is now joined against running dag_runs with a 'scheduled' run_id. This change is however not required, as task_instances will only be in 'scheduled' state when they are send to the executore. Tasks from dag_runs in failed state will not be scheduled by contract."
14,AIRFLOW-821,AIRFLOW,1485857944000,1487511153000,Bug,Resolved,Blocker,1,237,Scheduler dagbag importing not Py3 compatible Function {{update_import_errors}} in scheduler (https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L694) in not Py3 compatible (using {{iteritems}} instead of {{items}}).
15,AIRFLOW-844,AIRFLOW,1486421662000,1486500622000,Bug,Resolved,Blocker,1,159,"CgroupTaskRunner does not create cgroups correctly Right now os.mkdir is used to create cgroups which is incorrect, instead the cgroups library should be used."
16,AIRFLOW-846,AIRFLOW,1486467795000,1489524613000,Task,Closed,Blocker,1,536,"Release schedule, latest tag is too old To my understanding, there is no clear point about the release schedule of the project. The latest tag is 1.7.1.3 from June 2016, which is not well suited for production now days. For example, the latest available release is still affected by AIRFLOW-178 which means that we have to patch the sources on production to work with ZIP files. Could you please share your thoughts and position on the release planning of the project ? Would it be possible to get a newer tag sometimes soon ? Thank you"
17,AIRFLOW-847,AIRFLOW,1486475103000,1558131912000,Bug,Resolved,Blocker,1,220,Xcoms are not passed into SubDAG It's not possible to do a xcom_pull within a subdag None of the following seems to be working: * As templated var in SubDagoperator * As var in SubDagoperator * From within Subdag-factory
18,AIRFLOW-856,AIRFLOW,1486675763000,1486732738000,Bug,Resolved,Blocker,1,316,"Execution_date is always set to None in local_client API def trigger_dag(self, dag_id, run_id=None, conf=None, execution_date=None): dr = trigger_dag.trigger_dag(dag_id=dag_id, run_id=run_id, conf=conf, execution_date=None) return ""Created {}"".format(dr) is the code in local_client. This sets execution_date to None"
19,AIRFLOW-893,AIRFLOW,1487798258000,1487886741000,Bug,Resolved,Blocker,1,259,"Webservers crash when a DAG doesn't have a start date set This commit introduced a bug: [AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger Dag 7c94d81c390881643f94d5e3d7d6fb351a445b72 Where webservers will crash if a DAG does not have a start date set."
20,AIRFLOW-894,AIRFLOW,1487865446000,1516027157000,Bug,Closed,Blocker,1,1158,"Trigger Rules not functioning Code below fails to schedule the join task. This includes with trigger rules for all_done, and one_success. It seems to only occur when dynamically generating tasks. from airflow import DAG from airflow.operators import PythonOperator, BranchPythonOperator, DummyOperator from datetime import datetime, timedelta from datetime import datetime from slackclient import SlackClient default_args = { 'owner': 'analytics', 'depends_on_past': False, #'start_date': sixty_days_ago, 'start_date': datetime(2017, 2, 22), 'retries': 0 # 'retry_delay': timedelta(seconds=30), } dag = DAG( 'Valet_Data', default_args=default_args, schedule_interval='*/5 * * * *', dagrun_timeout=timedelta(seconds=60)) def valet_function(locdata, ds, **kwargs): if locdata == 'D': print(intentionalFail) join = DummyOperator( task_id='join', trigger_rule='all_done', dag=dag ) list = ['A','B','C','D','E','F','G','H','I','J','Z'] for l in list: task = PythonOperator( task_id='{0}_PANTS'.format(l), provide_context=True, python_callable=valet_function, op_kwargs={'locdata': l}, # on_failure_callback=on_failure, # on_success_callback=on_success, dag=dag, )"
21,AIRFLOW-897,AIRFLOW,1487885310000,1487975374000,Bug,Closed,Blocker,1,320,"Dagruns get marked as failed as soon as one root task fails In 1.7.1, all root tasks must have failed in order for a dagrun to fail, in 1.8.0 a single root task failure will fail a dagrun and prevent all other tasks in the DAG from running which is not logical behavior and a regression from 1.7.1. [~bolke][~criccomini]"
22,AIRFLOW-910,AIRFLOW,1488047085000,1569858390000,Sub-task,Resolved,Blocker,1,167,Parallelize dag runs in backfills Currently dag runs are executed sequentially while backfilling. This is a regression and slows down the processing off tasks. [~aoen]
23,AIRFLOW-921,AIRFLOW,1488223549000,1489380318000,Task,Resolved,Blocker,1,600,"1.8.0rc Issues These are the pending issues for the Airflow 1.8.0 release: Blockers: [~bolke] please merge into the next RC and then remove from the list the sub-tasks linked in this JIRA Other Issues: - High DB Load (~8x more than 1.7) - We are still investigating but it's probably not a blocker for the release - (Theories: Might need execution_date index on dag_run (based on slow process list) OR it might be this query which is long running SELECT union_ti.dag_id AS union_ti_dag_id, union_ti.state AS union_ti_state, count( *) AS count_1 FR)) - Front page loading time is a lot slower [~bolke]"
24,AIRFLOW-932,AIRFLOW,1488394470000,1489380308000,Sub-task,Resolved,Blocker,1,90,Backfills delete existing task instances and mark them as removed I'm still investigating.
25,AIRFLOW-934,AIRFLOW,1488425529000,1489519643000,Bug,Resolved,Blocker,1,2440,"airflow delayed the task to start we have a complex DAG which includes many tasks. but recently we found some tasks start delayed. for instance: start_task(which will start at 00:00) --> create_cluster(will finished at 00:11) --> wait_task(start at 00:16) , note: 1. wait_task only has one upstream that is create_cluster 2. the server that the airflow hosts has enough memory, and celeryd_concurrency is 20. below is the log of wait_task: [2017-03-02 00:16:39,602] {models.py:124} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:16:39,603] {models.py:197} INFO - Importing /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:16:39,613] {models.py:284} INFO - Loaded DAG <DAG: etl_prod> [2017-03-02 00:16:40,333] {models.py:124} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:16:40,333] {models.py:197} INFO - Importing /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:16:40,345] {models.py:284} INFO - Loaded DAG <DAG: etl_prod> [2017-03-02 00:16:40,373] {models.py:936} INFO - -------------------------------------------------------------------------------- New run starting @2017-03-02T00:16:40.369560 -------------------------------------------------------------------------------- [2017-03-02 00:16:40,402] {models.py:951} INFO - Queuing into pool None [2017-03-02 00:22:31,161] {models.py:124} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:22:31,162] {models.py:197} INFO - Importing /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:22:31,172] {models.py:284} INFO - Loaded DAG <DAG: etl_prod> [2017-03-02 00:22:31,863] {models.py:124} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:22:31,863] {models.py:197} INFO - Importing /home/ubuntu/airflow/dags/etl_prod/etl_prod.py [2017-03-02 00:22:31,874] {models.py:284} INFO - Loaded DAG <DAG: etl_prod> [2017-03-02 00:22:31,901] {models.py:936} INFO - -------------------------------------------------------------------------------- New run starting @2017-03-02T00:22:31.897547 -------------------------------------------------------------------------------- [2017-03-02 00:22:31,911] {models.py:974} INFO - Executing <Task(BashOperator): wait_mins> on 2017-03-01 00:00:00 [2017-03-02 00:22:31,922] {bash_operator.py:52} INFO - tmp dir root location: /tmp"
26,AIRFLOW-968,AIRFLOW,1489180243000,1556702180000,Bug,Closed,Blocker,1,298,"TravisCI builds in master are failing for Python 2.7 Last good build on master: https://travis-ci.org/apache/incubator-airflow/builds/204780659 First failing build on master: https://travis-ci.org/apache/incubator-airflow/builds/205138766 Python 3.4 builds seem fine, Python 2.7 builds are failing."
27,AIRFLOW-1000,AIRFLOW,1489779310000,1492445369000,Task,Closed,Blocker,1,478,"Rebrand to Apache Airflow instead of Airflow Apache requires branding in the form of ""Apache Airflow"" instead of just ""Airflow"". We should figure out a way to rebrand to ""Apache Airflow"" while having an upgrade path as smooth as possible for our users. This could mean that we first release side-by-side: airflow-1.8.1-apache.incubating apache-airflow-1.8.1-incubating It seems that pip does support some kind of packaging that helps with this (see for example Apache Libcloud)."
28,AIRFLOW-1001,AIRFLOW,1489782787000,1535984422000,Bug,Closed,Blocker,1,1990,"Landing Time shows ""unsupported operand type(s) for -: 'datetime.datetime' and 'NoneType'"" on example_subdag_operator Sample DAG example_subdag_operator shows below Oops page on ""Landing Times"" tab {noformat} ------------------------------------------------------------------------------- Traceback (most recent call last): File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask/app.py"", line 1988, in wsgi_app response = self.full_dispatch_request() File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask/app.py"", line 1641, in full_dispatch_request rv = self.handle_user_exception(e) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask/app.py"", line 1544, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask/app.py"", line 1639, in full_dispatch_request rv = self.dispatch_request() File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask/app.py"", line 1625, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask_admin/base.py"", line 69, in inner return self._run_view(f, *args, **kwargs) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask_admin/base.py"", line 368, in _run_view return fn(self, *args, **kwargs) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/flask_login.py"", line 755, in decorated_view return func(*args, **kwargs) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/www/utils.py"", line 125, in wrapper return f(*args, **kwargs) File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/www/views.py"", line 1560, in landing_times secs = (ti.end_date - ts).total_seconds() TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'NoneType' {noformat} http://somehost:18111/admin/airflow/landing_times?root=&days=30&dag_id=example_subdag_operator"
29,AIRFLOW-1011,AIRFLOW,1489933675000,1558131584000,Bug,Resolved,Blocker,1,426,"Fix bug in BackfillJob._execute() for SubDAGs The attached test SubDAG is not executed when the parent DAG is triggered manually. Attached is a simple test DAG that exhibits the issue along with screenshots showing the UI differences between v1.8 and v1.7.1.3. Note that if the DAG is run via backfill from command line (e.g. ""airflow backfill Test_SubDAG -s 2017-03-18 -e 2017-03-18"") the task instances show up successfully."
30,AIRFLOW-1018,AIRFLOW,1490049741000,1580461132000,Bug,Resolved,Blocker,1,409,"Scheduler DAG processes can not log to stdout Each DAG has its own log file for the scheduler and we can specify the directory with child_process_log_directory param. Unfortunately we can not change device / by specifying /dev/stdout for example. That is very useful when we execute Airflow in a container. When we specify /dev/stdout it raises: ""OSError: [Errno 20] Not a directory: '/dev/stdout/2017-03-19'"""
31,AIRFLOW-1033,AIRFLOW,1490286084000,1491480235000,Bug,Resolved,Blocker,1,2377,"TypeError: can't compare datetime.datetime to NoneType in prev_dagrun_dep.py Dear, When starting a specific new dag we get the following error: [2017-03-23 16:51:16,354] {jobs.py:354} DagFileProcessor908 ERROR - Got an exception! Propagating... Traceback (most recent call last): File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper pickle_dags) File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1581, in process_file self._process_dags(dagbag, dags, ti_keys_to_schedule) File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1174, in _process_dags self._process_task_instances(dag, tis_out) File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 905, in _process_task_instances session=session): File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper result = func(*args, **kwargs) File ""/usr/lib/python2.7/site-packages/airflow/models.py"", line 1116, in are_dependencies_met session=session): File ""/usr/lib/python2.7/site-packages/airflow/models.py"", line 1140, in get_failed_dep_statuses dep_context): File ""/usr/lib/python2.7/site-packages/airflow/ti_deps/deps/base_ti_dep.py"", line 94, in get_dep_statuses for dep_status in self._get_dep_statuses(ti, session, dep_context): File ""/usr/lib/python2.7/site-packages/airflow/ti_deps/deps/prev_dagrun_dep.py"", line 47, in _get_dep_statuses if dag.previous_schedule(ti.execution_date) < ti.task.start_date: TypeError: can't compare datetime.datetime to NoneType I have added some debug code to the file 'prev_dagrun_dep.py: dag = ti.task.dag print 'Start dates:' print 'previous_exection_date: %s'%(dag.previous_schedule(ti.execution_date)) print 'current start date: %s'%(ti.task.start_date) if dag.catchup: if dag.previous_schedule(ti.execution_date) < ti.task.start_date: And this is the output I get: Start dates: previous_exection_date: None current start date: 2017-03-19 00:00:00 I think it is normall that the previous_exection_date is null, since it is the first time this dag is being run. But why is the start_date of the dag important, and not the start date of the run? I have the feeling the cause is the 'schedule_interval', which is set to None. Please find an example and it's log file as an attachment to this mail. Bert"
32,AIRFLOW-1050,AIRFLOW,1490674660000,1491544846000,Bug,Resolved,Blocker,1,1336,"Retries ignored - regression SubDag fails when first operator fails, despite the fact it's configured for retries. Information in UI afterwards are also incorrect. From SubDag prospective it's still {{running}} with operator marked as {{up_for_retry}}, from main DAG prospective, whole run is marked as {{failed}} same as SubDag. See attached screenshots. Latest not affected version is RC4 (310fb58). I tested RC5, 1.8.0 with LocalExecutor and CeleryExecutor. Example code: {code} from datetime import datetime, timedelta from airflow.models import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators.python_operator import PythonOperator from airflow.operators.subdag_operator import SubDagOperator args = { ""start_date"": datetime.today(), } dag = DAG( dag_id=""main"", default_args=args, dagrun_timeout=timedelta(minutes=60), schedule_interval=None, max_active_runs=1 ) sub_dag = DAG( dag_id=""main.test"", default_args=args, schedule_interval=None, ) op = BashOperator( task_id=""first"", dag=sub_dag, bash_command=""echo 1"" ) def throw_error(): raise RuntimeError() op2 = PythonOperator( task_id=""second"", dag=sub_dag, python_callable=throw_error, retries=3, retry_delay=timedelta(0, 20) ) op >> op2 prepare_environment = SubDagOperator( task_id='test', subdag=sub_dag, default_args=args, dag=dag, ) {code}"
33,AIRFLOW-1103,AIRFLOW,1491970176000,1516027138000,Task,Closed,Blocker,1,406,How we can check our developer programmes are pick which environment variables Hi I have updated environment variable .bashrc .profile but those are not picking up from programmes which is running from airflow what is the command to check what are the environment variables are picking by airflow ps eww 22559 can you guid me on this command or any other tool to find which environment variables are taking
34,AIRFLOW-1121,AIRFLOW,1492551627000,1497401870000,Bug,Resolved,Blocker,1,1167,"airflow webserver --pid no longer write out pid file! I've run into a regression with the webserver. It looks like the --pid argument is no longer honored in 1.8.1. The pid file is not being written out! As a result, watchdog processes like monitd, which watch the processes mentioned in the pid file, keep trying to spawn webservers because the pid files are not being written. HISTTIMEFORMAT=""%d/%m/%y %T "" PYTHONPATH=/usr/local/agari/ep-pipeline/production/current/analysis/cluster/:/usr/local/agari/ep-pipeline/production/current/analysis/lookups/ TMP=/data/tmp AIRFLOW_HOME=/data/airflow PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin airflow webserver -p 8080 --pid /data/airflow/pids/airflow-webserver.pid The Patch that might have broken this is https://github.com/apache/incubator-airflow/commit/a9b20a04 If I include -D, things work: airflow webserver -D --pid ~/this.pid sid-as-mbp:~ siddharth$ ls -lrt *pid -rw-r--r-- 1 siddharth staff 5 Apr 18 14:37 this-monitor.pid -rw-r--r-- 1 siddharth staff 5 Apr 18 14:37 this.pid sid-as-mbp:~ siddharth$ cat *pid 8732 8735 However, this doesn't help me as I don't want to run with -D. cc [~sekikn] [~criccomini]"
35,AIRFLOW-1124,AIRFLOW,1492599475000,1503954859000,Bug,Resolved,Blocker,1,166,"Do not set all task instances to scheduled on backfill Backfills are supposed to fill in the gaps, but in 1.8.0 we set all tasks to scheduled, so they will run again."
36,AIRFLOW-1127,AIRFLOW,1492631470000,1494351336000,Improvement,Closed,Blocker,1,497,"Move license notices to LICENSE instead of NOTICE For all the bundled files with different licenses (MIT, BSD, etc), the full texts of these licenses should be in the source tarball preferably at the end of the LICENSE file. webgl-2d needs to be called out as MIT license. Are all the entries in the NOTICE file required or do they just need to be in the LICENSE file? Any additions to the NOTICE have downstream repercussions as they need to be propagated down by any other project using airflow."
37,AIRFLOW-1132,AIRFLOW,1492709647000,1492709880000,Bug,Closed,Blocker,1,1570,"Clear DAG via UI causes exception When I try to clear some of my failed tasks via the UI, I get: {noformat} Traceback (most recent call last): File ""/usr/lib64/python2.7/site-packages/flask/app.py"", line 1988, in wsgi_app response = self.full_dispatch_request() File ""/usr/lib64/python2.7/site-packages/flask/app.py"", line 1641, in full_dispatch_request rv = self.handle_user_exception(e) File ""/usr/lib64/python2.7/site-packages/flask/app.py"", line 1544, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/usr/lib64/python2.7/site-packages/flask/app.py"", line 1639, in full_dispatch_request rv = self.dispatch_request() File ""/usr/lib64/python2.7/site-packages/flask/app.py"", line 1625, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/usr/lib64/python2.7/site-packages/flask_admin/base.py"", line 69, in inner return self._run_view(f, *args, **kwargs) File ""/usr/lib64/python2.7/site-packages/flask_admin/base.py"", line 368, in _run_view return fn(self, *args, **kwargs) File ""/usr/lib64/python2.7/site-packages/flask_login.py"", line 758, in decorated_view return func(*args, **kwargs) File ""/usr/lib/python2.7/site-packages/airflow/www/utils.py"", line 125, in wrapper return f(*args, **kwargs) File ""/usr/lib/python2.7/site-packages/airflow/www/utils.py"", line 172, in wrapper return f(*args, **kwargs) File ""/usr/lib/python2.7/site-packages/airflow/www/views.py"", line 1013, in clear dag = dag.sub_dag( AttributeError: 'NoneType' object has no attribute 'sub_dag' {noformat} These are failed DAGs with a failed task."
38,AIRFLOW-1138,AIRFLOW,1492787582000,1562254362000,Task,Resolved,Blocker,1,152,Add licenses to files in scripts directory These two files need license headers: modified: scripts/ci/requirements.txt modified: scripts/systemd/airflow
39,AIRFLOW-1142,AIRFLOW,1493079256000,1558131588000,Bug,Resolved,Blocker,1,588,"SubDAG Tasks Not Executed Even Though All Dependencies Met Testing on 1.8.1rc1, we noticed that tasks in subdags were not getting executed even though all dependencies had been met. We were able to create a simple test DAG that re-creates the issue. Attached is a test DAG, the log file of the subdag operator that shows it fails to run even though dependencies are met, and screenshots of what the UI looks like. This is definitely a regression as we have many similarly constructed DAGs that have been running successfully on a pre-v1.8 version (a fork of 1.7.1.3+master) for some time."
40,AIRFLOW-1178,AIRFLOW,1494178685000,1537533752000,Bug,Closed,Blocker,1,1234,"@once may run more than one time My DAG is running second (2nd) time although it is declared as @once. Here's DAG definition : {noformat} main_dag = DAG( dag_id = 'Test-DAG-1', default_args = default_args, # dafeult operators' arguments - see above user_defined_macros = dag_macros, # I do not get different between ## params = dag_macros, # user_defined_macros and params # start_date = datetime.now(), # or e.g. datetime(2015, 6, 1) # 'end_date' = datetime(2016, 1, 1), catchup = True, # Perform scheduler catchup (or only run latest)? # - defaults to True schedule_interval = '@once', # '@once'=None? # doesn't create multiple dag runs automatically concurrency = 3, # task instances allowed to run concurrently max_active_runs = 1, # only one DAG run at a time dagrun_timeout = timedelta(days=4), # no way this dag should ran for 4 days orientation = 'TB', # default graph view ) {noformat} As a workaround for AIRFLOW-1013 I changed catchup from False to True. Suggested on dev list. It ""worked around"" AIRFLOW-1013 execution, but screwed @once logic - the DAG got scheduled twice (!) which is a no-go for us. The DAG actually has to run not more than 1 time. IMO, catchup=True should be explicitly disallowed for @once schedule."
41,AIRFLOW-1290,AIRFLOW,1496868784000,1496936058000,Bug,Resolved,Blocker,1,330,"Change docs author from ""Maxime Beauchemin"" to ""Apache Airflow"" In `docs/conf.py`, the author of the docs is set to ""Maxime Beauchemin"" who was the original author of the docs, but since then the whole community has taken over authoring the Airflow documentation. Change all references from ""Maxime Beauchemin"" to ""Apache Airflow"""
42,AIRFLOW-1291,AIRFLOW,1496875106000,1505332197000,Bug,Resolved,Blocker,1,49,Update NOTICE and LICENSE files to meet ASF specs
43,AIRFLOW-1294,AIRFLOW,1496906049000,1497367452000,Bug,Resolved,Blocker,1,302,"Backfills can loose tasks to execute due to tasks setting themselves to NONE In the backfills we can loose tasks to execute due to a task setting its own state to NONE if concurrency limits are reached, this makes them fall outside of the scope the backfill is managing hence they will not be executed."
44,AIRFLOW-1296,AIRFLOW,1496967081000,1534855855000,Bug,Resolved,Blocker,1,1963,"DAGs using operators involving cascading skipped tasks fail prematurely So this is basically the same issue as AIRFLOW-872 and AIRFLOW-719. A workaround had fixed this (https://github.com/apache/incubator-airflow/pull/2125), but was later reverted (https://github.com/apache/incubator-airflow/pull/2195). I totally agree with the reason for reverting, but I still think this is an issue. The issue is related to any operators that involves cascading skipped tasks, like ShortCircuitOperator or LatestOnlyOperator. These operators mark only their *direct* downstream task as SKIPPED, but additional downstream tasks from that skipped task is left up to the scheduler to cascade the SKIPPED state (see latest only op docs about this expected behavior https://airflow.incubator.apache.org/concepts.html#latest-run-only). However, instead the scheduler marks the DAG run as FAILED prematurely before the DAG has a chance to skip all downstream tasks. This example DAG should reproduce the issue: https://gist.github.com/dhuang/61d38fb001c3a917edf4817bb0c915f9. Expected result: DAG succeeds with tasks - latest_only (success) -> dummy1 (skipped) -> dummy2 (skipped) -> dummy3 (skipped) Actual result: DAG fails with tasks - latest_only (success) -> dummy1 (skipped) -> dummy2 (none) -> dummy3 (none) I believe the results I'm seeing are because of this deadlock prevention logic, https://github.com/apache/incubator-airflow/blob/1.8.1/airflow/models.py#L4182. While that actual result shown above _could_ mean a deadlock, in this case it shouldn't be. Since this {{update_state}} logic is reached first in each scheduler run, dummy2/dummy3 don't get a chance to cascade the SKIPPED state. Commenting out that block gives me the results I expect. [~bolke] I know you spent awhile trying to reproduce my issue and weren't able to, but I'm still hitting this on a fresh environment, default configs, sqlite/mysql dbs, local/sequential/celery executors, and 1.8.1/master."
45,AIRFLOW-1482,AIRFLOW,1501684834000,1564723828000,Bug,Open,Blocker,1,4596,"Error when try to backfill the example_trigger_controller_dag Hello, Running a backfill command for the {noformat}example_trigger_controller_dag{noformat} example dag, result in the failed task {noformat}test_trigger_dagrun{noformat} It seems to me that the problem comes from the TriggerDagRunOperator in the example_trigger_controller_dag ? Backfill command: {noformat}airflow backfill -s 2017-07-10 -e 2017-07-13 --pool backfill example_trigger_controller_dag{noformat} Tested in 1.8.1 and 1.8.2rc1 Here is the output log from the backfill command : {noformat} [2017-08-02 13:53:00,844] {__init__.py:57} INFO - Using executor CeleryExecutor [2017-08-02 13:53:00,888] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt [2017-08-02 13:53:00,902] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt /var/lib/airflow/local/lib/python2.7/site-packages/airflow/www/app.py:23: FlaskWTFDeprecationWarning: ""flask_wtf.CsrfProtect"" has been renamed to ""CSRFProtect"" and will be removed in 1.0. csrf = CsrfProtect() [2017-08-02 13:53:01,033] {models.py:168} INFO - Filling up the DagBag from /var/lib/airflow/dags [2017-08-02 13:53:01,332] {models.py:1128} INFO - Dependencies all met for <TaskInstance: example_trigger_controller_dag.test_trigger_dagrun 2017-07-10 00:00:00 [scheduled]> [2017-08-02 13:53:01,337] {base_executor.py:50} INFO - Adding to queue: airflow run example_trigger_controller_dag test_trigger_dagrun 2017-07-10T00:00:00 --pickle 1 --local --pool backfill [2017-08-02 13:53:06,267] {celery_executor.py:81} INFO - [celery] queuing (u'example_trigger_controller_dag', u'test_trigger_dagrun', datetime.datetime(2017, 7, 10, 0, 0)) through celery, queue=default [2017-08-02 13:53:06,330] {models.py:4164} INFO - Updating state for <DagRun example_trigger_controller_dag @ 2017-07-10 00:00:00: backfill_2017-07-10T00:00:00, externally triggered: False> considering 1 task(s) [2017-08-02 13:53:06,334] {jobs.py:2020} INFO - [backfill progress] | finished run 0 of 1 | tasks waiting: 0 | succeeded: 0 | kicked_off: 1 | failed: 0 | skipped: 0 | deadlocked: 0 | not ready: 0 [2017-08-02 13:53:11,273] {jobs.py:1743} ERROR - Executor reports task instance <TaskInstance: example_trigger_controller_dag.test_trigger_dagrun 2017-07-10 00:00:00 [queued]> finished (failed) although the task says its queued. Was the task killed externally? [2017-08-02 13:53:11,273] {models.py:1433} ERROR - Executor reports task instance <TaskInstance: example_trigger_controller_dag.test_trigger_dagrun 2017-07-10 00:00:00 [queued]> finished (failed) although the task says its queued. Was the task killed externally? None [2017-08-02 13:53:11,273] {models.py:1457} INFO - Marking task as FAILED. [2017-08-02 13:53:11,279] {models.py:1478} ERROR - Executor reports task instance <TaskInstance: example_trigger_controller_dag.test_trigger_dagrun 2017-07-10 00:00:00 [queued]> finished (failed) although the task says its queued. Was the task killed externally? [2017-08-02 13:53:11,281] {jobs.py:1694} ERROR - Task instance <TaskInstance: example_trigger_controller_dag.test_trigger_dagrun 2017-07-10 00:00:00 [failed]> failed [2017-08-02 13:53:11,283] {models.py:4164} INFO - Updating state for <DagRun example_trigger_controller_dag @ 2017-07-10 00:00:00: backfill_2017-07-10T00:00:00, externally triggered: False> considering 1 task(s) [2017-08-02 13:53:11,285] {models.py:4204} INFO - Marking run <DagRun example_trigger_controller_dag @ 2017-07-10 00:00:00: backfill_2017-07-10T00:00:00, externally triggered: False> failed [2017-08-02 13:53:11,298] {jobs.py:2020} INFO - [backfill progress] | finished run 1 of 1 | tasks waiting: 0 | succeeded: 0 | kicked_off: 0 | failed: 1 | skipped: 0 | deadlocked: 0 | not ready: 0 Traceback (most recent call last): File ""/var/lib/airflow/bin/airflow"", line 28, in <module> args.func(args) File ""/var/lib/airflow/local/lib/python2.7/site-packages/airflow/bin/cli.py"", line 167, in backfill pool=args.pool) File ""/var/lib/airflow/local/lib/python2.7/site-packages/airflow/models.py"", line 3373, in run job.run() File ""/var/lib/airflow/local/lib/python2.7/site-packages/airflow/jobs.py"", line 201, in run self._execute() File ""/var/lib/airflow/local/lib/python2.7/site-packages/airflow/jobs.py"", line 2063, in _execute raise AirflowException(err) airflow.exceptions.AirflowException: --------------------------------------------------- Some task instances failed: set([(u'example_trigger_controller_dag', u'test_trigger_dagrun', datetime.datetime(2017, 7, 10, 0, 0))]) {noformat}"
46,AIRFLOW-1494,AIRFLOW,1502177433000,1535980836000,Bug,Resolved,Blocker,1,12327,"backfill job failed because new retry comes out when a valid job is running I have a spark job, wrapped in a BASH command to run. From fail log, i found airflow try to rerun the job while the job is running. Then a series strange things happened. My job finally failed. Logs Below: [2017-08-07 23:24:35,903] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:35,903] {bash_operator.py:94} INFO - 17/08/07 23:24:35 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:36,904] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:36,904] {bash_operator.py:94} INFO - 17/08/07 23:24:36 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:37,905] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:37,904] {bash_operator.py:94} INFO - 17/08/07 23:24:37 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:38,906] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:38,906] {bash_operator.py:94} INFO - 17/08/07 23:24:38 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:38,947] {cli.py:391} INFO - Loading pickle id 148 [2017-08-07 23:24:39,020] {base_task_runner.py:112} INFO - Running: ['bash', '-c', u'airflow run generated_daily submit_operator 2017-08-05T00:00:00 --pickle 148 --job_id 15372 --raw'] [2017-08-07 23:24:39,426] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,426] {__init__.py:57} INFO - Using executor CeleryExecutor [2017-08-07 23:24:39,495] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,494] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt [2017-08-07 23:24:39,519] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,519] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt [2017-08-07 23:24:39,710] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,710] {cli.py:391} INFO - Loading pickle id 148 [2017-08-07 23:24:39,772] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,772] {models.py:1120} INFO - Dependencies not met for <TaskInstance: generated_daily.submit_operator 2017-08-05 00:00:00 [running]>, dependency 'Task Instance Not Already Running' FAILED: Task is already running, it started on 2017-08-07 20:55:12.727910. [2017-08-07 23:24:39,777] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,777] {models.py:1120} INFO - Dependencies not met for <TaskInstance: generated_daily.submit_operator 2017-08-05 00:00:00 [running]>, dependency 'Task Instance State' FAILED: Task is in the 'running' state which is not a valid state for execution. The task must be cleared in order to be run. [2017-08-07 23:24:39,907] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:39,907] {bash_operator.py:94} INFO - 17/08/07 23:24:39 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:40,908] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:40,908] {bash_operator.py:94} INFO - 17/08/07 23:24:40 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:41,909] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:41,909] {bash_operator.py:94} INFO - 17/08/07 23:24:41 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:42,911] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:42,910] {bash_operator.py:94} INFO - 17/08/07 23:24:42 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:43,912] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:43,912] {bash_operator.py:94} INFO - 17/08/07 23:24:43 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:44,120] {jobs.py:2148} WARNING - Recorded pid 116446 is not a descendant of the current pid 33416 [2017-08-07 23:24:44,914] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:44,913] {bash_operator.py:94} INFO - 17/08/07 23:24:44 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:45,914] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:45,914] {bash_operator.py:94} INFO - 17/08/07 23:24:45 INFO yarn.Client: Application report for application_1498090868254_911911 (state: RUNNING) [2017-08-07 23:24:46,032] {jobs.py:2155} WARNING - State of this instance has been externally set to up_for_retry. Taking the poison pill. So long. [2017-08-07 23:24:46,081] {helpers.py:220} WARNING - Terminating descendant processes of ['/usr/bin/python /usr/local/bin/airflow run generated_daily submit_operator 2017-08-05T00:00:00 --pickle 148 --job_id 15015 --raw'] PID: 116446 [2017-08-07 23:24:46,081] {helpers.py:224} WARNING - Terminating descendant process ['bash', '/tmp/airflowtmpCFeIBn/submit_operatorGbO01R'] PID: 116496 [2017-08-07 23:24:46,087] {helpers.py:224} WARNING - Terminating descendant process ['/bin/sh', '/mnt/data/firework-0.2.0/bin/spark-submit', '--spark-version', '1.6.2', '--master', 'yarn-cluster', '--num-executors', '128', '--class', 'com.hulu.reco.metrics.middlelayer.BatchApplication', '--conf', 'spark.date=20170805', '--conf', 'spark.yarn.maxAppAttempts=1', '--conf', 'spark.driver.memory=8g', '--conf', 'spark.executor.memory=36g', '--conf', 'spark.executor.cores=8', '--conf', 'spark.user.javahome.enabled=true', '--conf', 'spark.user.javahome.path=/usr/lib/jvm/hulu-oracle-jdk8', '--conf', 'spark.output=hdfs://warehousestore/user/pcdm/generated_prod/20170805', '--queue', 'spark', '/home/deploy/middlelayer-1.0-SNAPSHOT-jar-with-dependencies.jar'] PID: 116498 [2017-08-07 23:24:46,091] {helpers.py:224} WARNING - Terminating descendant process ['/usr/java/jdk8/jdk1.8.0_92-1//bin/java', '-cp', '/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/conf/:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/assembly/target/scala-2.10/spark-assembly-1.6.2-hadoop2.6.0-cdh5.7.3-201612201803.jar:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/lib_managed/jars/datanucleus-core-3.2.10.jar:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/hadoop-conf/', 'org.apache.spark.deploy.SparkSubmit', '--master', 'yarn-cluster', '--conf', 'spark.executor.memory=36g', '--conf', 'spark.driver.memory=8g', '--conf', 'spark.user.javahome.path=/usr/lib/jvm/hulu-oracle-jdk8', '--conf', 'spark.output=hdfs://warehousestore/user/pcdm/generated_prod/20170805', '--conf', 'spark.yarn.maxAppAttempts=1', '--conf', 'spark.executor.cores=8', '--conf', 'spark.date=20170805', '--conf', 'spark.user.javahome.enabled=true', '--class', 'com.hulu.reco.metrics.middlelayer.BatchApplication', '--num-executors', '128', '--queue', 'spark', '/home/deploy/middlelayer-1.0-SNAPSHOT-jar-with-dependencies.jar'] PID: 116761 [2017-08-07 23:24:46,096] {helpers.py:231} WARNING - Waiting up to 5s for processes to exit... [2017-08-07 23:24:46,099] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:46,099] {bash_operator.py:94} INFO - 17/08/07 23:24:46 INFO util.ShutdownHookManager: Shutdown hook called [2017-08-07 23:24:46,100] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:46,100] {bash_operator.py:94} INFO - 17/08/07 23:24:46 INFO util.ShutdownHookManager: Deleting directory /mnt/data/tmp/spark-b5f3c795-7ea2-4ef1-a47c-2507b0747580 [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:46,118] {bash_operator.py:97} INFO - Command exited with return code -15 [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:46,118] {models.py:1417} ERROR - Bash command failed [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: Traceback (most recent call last): [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1374, in run [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: result = task_copy.execute(context=context) [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/lib/python2.7/dist-packages/airflow/operators/bash_operator.py"", line 100, in execute [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: raise AirflowException(""Bash command failed"") [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: AirflowException: Bash command failed [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:46,120] {models.py:1433} INFO - Marking task as UP_FOR_RETRY [2017-08-07 23:24:46,123] {helpers.py:234} WARNING - Done waiting [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:24:46,130] {models.py:1462} ERROR - Bash command failed [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: Traceback (most recent call last): [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/bin/airflow"", line 28, in <module> [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: args.func(args) [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 422, in run [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask: pool=args.pool, [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/lib/python2.7/dist-packages/airflow/utils/db.py"", line 53, in wrapper [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask: result = func(*args, **kwargs) [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1374, in run [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask: result = task_copy.execute(context=context) [2017-08-07 23:24:46,133] {base_task_runner.py:95} INFO - Subtask: File ""/usr/local/lib/python2.7/dist-packages/airflow/operators/bash_operator.py"", line 100, in execute [2017-08-07 23:24:46,133] {base_task_runner.py:95} INFO - Subtask: raise AirflowException(""Bash command failed"") [2017-08-07 23:24:46,133] {base_task_runner.py:95} INFO - Subtask: airflow.exceptions.AirflowException: Bash command failed [2017-08-07 23:24:51,034] {jobs.py:2083} INFO - Task exited with return code 1 [2017-08-07 23:33:27,872] {cli.py:391} INFO - Loading pickle id 148 [2017-08-07 23:33:27,940] {base_task_runner.py:112} INFO - Running: ['bash', '-c', u'airflow run generated_daily submit_operator 2017-08-05T00:00:00 --pickle 148 --job_id 15376 --raw'] [2017-08-07 23:33:28,302] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:33:28,301] {__init__.py:57} INFO - Using executor CeleryExecutor [2017-08-07 23:33:28,359] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:33:28,358] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt [2017-08-07 23:33:28,378] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:33:28,378] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt [2017-08-07 23:33:28,543] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:33:28,543] {cli.py:391} INFO - Loading pickle id 148 [2017-08-07 23:33:28,601] {base_task_runner.py:95} INFO - Subtask: [2017-08-07 23:33:28,601] {models.py:1120} INFO - Dependencies not met for <TaskInstance: generated_daily.submit_operator 2017-08-05 00:00:00 [up_for_retry]>, dependency 'Not In Retry Period' FAILED: Task is not ready for retry yet but will be retried automatically. Current date is 2017-08-07T23:33:28.601339 and task will be retried at 2017-08-08T00:24:46.119758. [2017-08-07 23:33:32,954] {jobs.py:2083} INFO - Task exited with return code 0"
47,AIRFLOW-1539,AIRFLOW,1503958220000,1535980907000,Bug,Closed,Blocker,1,1925,"python3 error import Dag Seeing the following error when using python3 (fine with python2) {code} Traceback (most recent call last): File ""/opt/conda/lib/python3.6/site-packages/airflow/models.py"", line 263, in process_file m = imp.load_source(mod_name, filepath) File ""/opt/conda/lib/python3.6/imp.py"", line 172, in load_source module = _load(spec) File ""<frozen importlib._bootstrap>"", line 675, in _load File ""<frozen importlib._bootstrap>"", line 655, in _load_unlocked File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed File ""/root/airflow/dags/LoadS3ToHiveTest.py"", line 2, in <module> from LoadS3ToHive import default_args, download_s3_to_hdfs_templated_command, generate_download_s3_to_hdfs_task, \ File ""/root/airflow/dags/LoadS3ToHive.py"", line 74, in <module> globals()[task_name] = DAG(task_name, default_args=default_args, schedule_interval=None) File ""/opt/conda/lib/python3.6/site-packages/airflow/models.py"", line 2664, in __init__ self.fileloc = inspect.getsourcefile(inspect.stack()[1][0]) File ""/opt/conda/lib/python3.6/inspect.py"", line 1465, in stack return getouterframes(sys._getframe(1), context) File ""/opt/conda/lib/python3.6/inspect.py"", line 1442, in getouterframes frameinfo = (frame,) + getframeinfo(frame, context) File ""/opt/conda/lib/python3.6/inspect.py"", line 1411, in getframeinfo filename = getsourcefile(frame) or getfile(frame) File ""/opt/conda/lib/python3.6/inspect.py"", line 666, in getsourcefile if getattr(getmodule(object, filename), '__loader__', None) is not None: File ""/opt/conda/lib/python3.6/inspect.py"", line 703, in getmodule if ismodule(module) and hasattr(module, '__file__'): File ""/opt/conda/lib/python3.6/site-packages/airflow/utils/timeout.py"", line 38, in handle_timeout raise AirflowTaskTimeout(self.error_message) airflow.exceptions.AirflowTaskTimeout: Timeout {code}"
48,AIRFLOW-1641,AIRFLOW,1506332962000,1509112710000,Bug,Closed,Blocker,1,794,"Task gets stuck in queued state Hello, I have one dag with ~20 tasks. The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind. There are some interactions with AWS and a remote DB. I only use LocalExecutor. What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. The manual workaround is to restart the task manually by clearing it. Does anyone have ideas about the issue behind, and how to avoid it for the future? Thanks in advance for your help. PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]"
49,AIRFLOW-1693,AIRFLOW,1507533365000,1558123699000,Bug,Resolved,Blocker,1,1744,"airflow initdb throws errors I am having installation problems. The log for `airflow initdb` are Traceback (most recent call last): File ""/Users/gauripradeep/ENVS/scheduler/bin/airflow"", line 17, in <module> from airflow import configuration File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/airflow/__init__.py"", line 30, in <module> from airflow import settings File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/airflow/settings.py"", line 159, in <module> configure_orm() File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/airflow/settings.py"", line 147, in configure_orm engine = create_engine(SQL_ALCHEMY_CONN, **engine_args) File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/sqlalchemy/engine/__init__.py"", line 391, in create_engine return strategy.create(*args, **kwargs) File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py"", line 80, in create dbapi = dialect_cls.dbapi(**dbapi_args) File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/sqlalchemy/dialects/mysql/mysqldb.py"", line 110, in dbapi return __import__('MySQLdb') File ""/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/MySQLdb/__init__.py"", line 19, in <module> import _mysql ImportError: dlopen(/Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/_mysql.cpython-36m-darwin.so, 2): Library not loaded: /usr/local/opt/mysql/lib/libmysqlclient.20.dylib Referenced from: /Users/gauripradeep/ENVS/scheduler/lib/python3.6/site-packages/_mysql.cpython-36m-darwin.so Reason: image not found Other commands and dependencies pip3 install mysqlclient and pip3 install mysql-connector-python-rf pip3 install airflow[mysql]"
50,AIRFLOW-1711,AIRFLOW,1507839702000,1509388549000,Bug,Resolved,Blocker,1,460,"Ldap Attributes not always a ""list"" part 2 in the LDAP auth module `group_contains_user` checks for `resp['attributes'].get(user_name_attr)[0] == username` Some Ldaps apparently have this as a simple string `resp['attributes'].get(user_name_attr) == username` also should be checked. But really a test should be done to see if the return is a 'list' and perform the check differently. If its not a list, python will check both arguments and exit with an error."
